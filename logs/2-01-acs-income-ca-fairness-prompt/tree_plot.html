<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 7], [1, 6], [2, 5], [7, 11], [7, 13], [7, 9], [7, 10], [7, 18], [7, 8], [7, 15], [7, 16], [7, 12], [11, 14], [12, 17], [13, 19]], "layout": [[0.5, 0.0], [0.625, 0.0], [0.75, 0.0], [0.875, 0.0], [1.0, 0.0], [0.75, 0.33333333333333337], [0.625, 0.33333333333333337], [0.5, 0.33333333333333337], [0.0, 0.6666666666666667], [0.125, 0.6666666666666667], [0.25, 0.6666666666666667], [0.375, 0.6666666666666667], [0.5, 0.6666666666666667], [0.625, 0.6666666666666667], [0.375, 1.0], [0.75, 0.6666666666666667], [0.875, 0.6666666666666667], [0.5, 1.0], [1.0, 0.6666666666666667], [0.625, 1.0]], "plan": ["We train a LightGBM classifier excluding the race feature and use 5-fold\nstratified cross-validation. Within each fold, we calibrate group-specific\nprobability thresholds to match the overall positive rate in the training split,\nenforcing demographic parity across racial groups. We report average validation\naccuracy and the mean demographic parity difference (max\u2013min positive rates).\nFinally, we retrain on the full training data, apply the same post-processing\nthresholds per race on the test set, and save `submission.csv`.", "We train a logistic regression model on numeric features plus one-hot encoded\nrace, and use 5-fold stratified CV. In each fold we calibrate race-specific\nprobability thresholds on the train split so that each group\u2019s positive rate\nequals the overall train positive rate, then apply these thresholds to the\nvalidation split and compute accuracy. We average CV accuracy and demographic\nparity difference across folds. Finally we retrain on all data, compute race\nthresholds, predict on the test set, and save a submission.csv with PINCP\npredictions.", "We train a logistic regression with a reweighing scheme that assigns higher\nweights to under\u2010represented race\u2010label pairs to promote demographic parity,\nexcluding the race feature during training. We perform 5\u2010fold stratified CV on\nthe PINCP target to compute average accuracy and the mean demographic parity\ndifference across race groups. Finally, we retrain on the full dataset with the\nsame reweighing, predict test labels with a 0.5 threshold, and save\n`submission.csv`. Evaluation metric: average CV accuracy.", "We train a logistic regression classifier excluding the race feature, applying\none-hot encoding to categorical predictors and scaling numeric ones. We perform\n5-fold stratified cross-validation to measure accuracy and calibrate group-\nspecific probability thresholds on each training fold so that the per-race\npositive rate matches the overall training positive rate, enforcing demographic\nparity. We report the average CV accuracy and mean demographic parity\ndifference. Finally, we retrain on the full data, apply the calibrated\nthresholds to the test set, and save submission.csv.", "We compute reweighing-based sample weights to correct joint race-label\ndistributions, then train a weighted logistic regression on one-hot encoded\ncategorical and scaled numerical features. We perform 5-fold stratified CV,\nreporting mean accuracy and demographic parity difference across racial groups.\nFinally, we retrain on the full data and output `submission.csv` with test\npredictions. Evaluation metric: accuracy.", "We fix the feature mismatch by dropping the PINCP column (as well as RAC1P) from\nthe test set so that its columns exactly match the training features. We\ncontinue to use 5-fold stratified CV with reweighing for demographic parity,\nprinting the average accuracy and demographic parity difference. Finally, we\nretrain on the full reweighed training data and predict on the properly aligned\ntest set, saving the submission.", "I fixed the OneHotEncoder instantiation by replacing the deprecated `sparse`\nargument with `sparse_output=False`. The rest of the pipeline remains the same:\nwe perform 5-fold stratified cross-validation, compute accuracy and demographic\nparity difference, then retrain on the full data and save the test predictions\nas `submission.csv`.", "I propose adding a simple hyperparameter grid search over key LightGBM\nparameters (num_leaves, learning_rate, max_depth, n_estimators) using our 5-fold\nCV with group\u2010specific thresholding to select the best configuration by\naccuracy. This atomic change focuses on finding better model settings to improve\npredictive performance while keeping the same fairness post\u2010processing. After\nidentifying the best hyperparameters, we retrain on full data and generate the\nfinal submission.", "I propose to leverage LightGBM's native handling of categorical features by\nspecifying them in the `categorical_feature` parameter during training. This\nallows the model to find better splits on inherently discrete variables such as\noccupation and marital status, potentially improving accuracy without altering\nthe fairness post\u2010processing. We'll keep the existing hyperparameter grid search\nand per\u2010race threshold calibration unchanged. Finally, we'll retrain on the full\ndata with the best parameters and generate the submission.", "We replace manual quantile\u2010based thresholding with Fairlearn\u2019s\nThresholdOptimizer to enforce demographic parity more directly. In each CV fold,\nafter training the LightGBM model, we fit a ThresholdOptimizer\n(constraints=\"demographic_parity\") on the training fold to learn optimal\nper\u2010group thresholds, then predict on the validation fold. This principled\npost\u2010processing should reduce disparity while maintaining accuracy. Finally, we\ntrain on the full data, fit the optimizer on the full set, and apply it to the\ntest data for submission.", "I propose including the race feature (RAC1P) directly in the LightGBM model\ntraining so that the model can leverage group\u2010specific patterns for more\naccurate probability estimates; we still enforce demographic parity via\npost\u2010training thresholding by race. This atomic change should boost overall\naccuracy while preserving the fairness constraint. All other parts of the\npipeline, including grid search and per\u2010race threshold calibration, remain\nunchanged.", "I will convert the known categorical features to pandas \u2018category\u2019 dtype and\npass them to LightGBM via the `categorical_feature` parameter, so that LightGBM\ncan find better splits on these discrete variables. This change is atomic and\nleaves the existing hyperparameter grid search and per\u2010race threshold\npost\u2010processing intact. It should boost accuracy by leveraging native\ncategorical handling without affecting the fairness adjustment. Finally, I\u2019ll\nretrain on the full data with the best parameters and output the submission.", "I propose leveraging LightGBM\u2019s native handling of categorical data by\nspecifying the categorical features in the `fit` calls. This allows the model to\nfind optimal splits on discrete variables like occupation and marital status,\npotentially improving accuracy without altering our fairness post\u2010processing.\nAll other parts of the pipeline\u2014including hyperparameter grid search, per\u2010race\nthreshold calibration, and CV setup\u2014remain unchanged so we can isolate the\neffect of this change.", "I propose converting the categorical predictors to pandas \u2018category\u2019 dtype and\npassing them to LightGBM via the `categorical_feature` parameter so that the\nmodel can learn better splits on these inherently discrete variables. All other\nsteps\u2014including 5-fold stratified CV, hyperparameter grid search, and group-\nspecific thresholding for demographic parity\u2014remain unchanged. This atomic\nchange leverages LightGBM\u2019s native categorical handling to boost predictive\nperformance.", "We remove the unsupported `verbose` argument from the LGBMClassifier `fit` calls\nand rely on default logging. The grid search and thresholding logic stays the\nsame: we perform 5-fold CV, compute accuracy and demographic parity difference\nfor each fold, then pick the best parameters by accuracy. After CV we retrain on\nthe full dataset, apply race\u2010specific thresholds, and generate test predictions.\nFinally, we save the submission file and print the best CV accuracy and DP\ndifference.", "I propose integrating early stopping into the LightGBM training loop by setting\na high n_estimators value (e.g., 1000) and using a validation fold for\nearly_stopping_rounds. This allows the model to choose the optimal number of\nboosting rounds per hyperparameter setting, reducing overfitting and improving\ngeneralization. We keep the race\u2010wise threshold post\u2010processing unchanged, but\nreplace the fixed n_estimators grid search with a single high n_estimators and\nrely on early stopping. All other components, including the 5\u2010fold stratified CV\nand demographic parity thresholding, remain the same.", "I propose leveraging LightGBM's native handling of categorical features by\npassing the list of categorical variable names to the `categorical_feature`\nparameter during model fitting. This allows the model to find better splits on\ninherently discrete predictors without one\u2010hot encoding. We keep the existing\nhyperparameter grid search and per\u2010race thresholding logic unchanged. Finally,\nwe retrain the best model on the full data and produce the submission.", "We remove the unsupported `early_stopping_rounds` and `eval_set` arguments from\nthe LGBMClassifier.fit calls and fully train each fold. We perform a grid search\nover the specified hyperparameters using 5-fold StratifiedKFold, computing\naccuracy and demographic parity difference on each validation fold and selecting\nbased on accuracy. After identifying the best parameters, we retrain on the full\ntraining set, compute race-specific thresholds to match the overall positive\nrate, and predict on the test set. Finally, we save `submission.csv` in\n`./working` and print the best CV accuracy and DP difference.", "We will improve probability estimates before thresholding by applying sigmoid\ncalibration to the LightGBM classifier. In each CV fold, after fitting a base\nLGBMClassifier, we wrap it with sklearn\u2019s CalibratedClassifierCV (cv='prefit',\nmethod='sigmoid') fitted on the training data, and use calibrated probabilities\nfor both the training and validation sets when computing thresholds and making\npredictions. This more accurate calibration should yield better probability\nestimates for thresholding, potentially improving both accuracy and demographic\nparity. All other grid search, threshold calibration, and submission logic\nremains unchanged.", "We remove the invalid `verbose` argument when calling `fit()`, and instead\nmanage verbosity via LightGBM\u2019s default settings. To properly evaluate our\nmodel, we'll first split the training data into a training and hold-out\nvalidation set. We\u2019ll run 5-fold CV on the training split to select the best\nhyperparameters based on accuracy, then retrain on the full training split and\ncalibrate per-race thresholds to control the positive rate, evaluating both\naccuracy and demographic parity on the hold-out set. Finally, we retrain on the\nentire dataset with the best parameters, predict on the test set using the same\ncalibration strategy, and save `submission.csv`."], "code": ["import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = []\ndp_diffs = []\n\n# Per-fold training and thresholding\nfor tr_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n    # Train LightGBM\n    clf = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n    clf.fit(X_tr, y_tr)\n    # Predict probabilities\n    p_tr = clf.predict_proba(X_tr)[:, 1]\n    p_val = clf.predict_proba(X_val)[:, 1]\n    # Overall positive rate in train\n    p_rate = y_tr.mean()\n    # Compute group thresholds\n    thresholds = {}\n    for r in np.unique(race_tr):\n        probs_r = p_tr[race_tr == r]\n        if len(probs_r) > 0:\n            thresholds[r] = np.quantile(probs_r, 1 - p_rate)\n        else:\n            thresholds[r] = 0.5\n    # Apply thresholds on validation\n    y_pred = np.array(\n        [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n    )\n    accs.append(accuracy_score(y_val, y_pred))\n    # Compute demographic parity difference\n    rates = []\n    for r in np.unique(race_val):\n        idx = race_val == r\n        if idx.sum() > 0:\n            rates.append(y_pred[idx].mean())\n    dp_diffs.append(max(rates) - min(rates))\n\n# Print metrics\nprint(f\"CV Accuracy: {np.mean(accs):.4f} \u00b1 {np.std(accs):.4f}\")\nprint(f\"CV Demographic Parity Difference: {np.mean(dp_diffs):.4f}\")\n\n# Retrain on full data\nclf_full = lgb.LGBMClassifier(n_estimators=100, random_state=42)\nclf_full.fit(X, y)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index_label=\"id\")", "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport os\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nFEATURES = [c for c in train.columns if c not in [\"PINCP\"]]\nNUMERIC = [c for c in FEATURES if c != \"RAC1P\"]\nCAT = [\"RAC1P\"]\n\nX = train[FEATURES]\ny = train[\"PINCP\"].astype(int)\nraces = train[\"RAC1P\"].astype(int)\n\n# Preprocessing and model pipeline\npreprocessor = ColumnTransformer(\n    [(\"num\", StandardScaler(), NUMERIC), (\"race\", OneHotEncoder(sparse=False), CAT)],\n    remainder=\"drop\",\n)\nmodel = Pipeline(\n    [\n        (\"pre\", preprocessor),\n        (\n            \"clf\",\n            LogisticRegression(\n                solver=\"lbfgs\", max_iter=1000, n_jobs=-1, random_state=42\n            ),\n        ),\n    ]\n)\n\n# 5-fold stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = []\ndp_diffs = []\n\nfor tr_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    race_tr, race_val = races.iloc[tr_idx], races.iloc[val_idx]\n\n    # Fit model\n    model.fit(X_tr, y_tr)\n    # Predict probabilities\n    p_tr = model.predict_proba(X_tr)[:, 1]\n    p_val = model.predict_proba(X_val)[:, 1]\n\n    # Overall positive rate in train\n    p_rate = y_tr.mean()\n    # Compute thresholds per race on train\n    thresholds = {}\n    for r in np.unique(race_tr):\n        mask = race_tr == r\n        if mask.sum() > 0:\n            thresholds[r] = np.quantile(p_tr[mask], 1 - p_rate)\n    # Apply thresholds to validation\n    y_pred = np.zeros_like(y_val)\n    for r in np.unique(race_val):\n        mask = race_val == r\n        t = thresholds.get(r, np.quantile(p_val, 1 - p_rate))\n        y_pred[mask] = (p_val[mask] > t).astype(int)\n    # Metrics\n    acc = accuracy_score(y_val, y_pred)\n    accs.append(acc)\n    # Demographic parity difference\n    rates = []\n    for r in np.unique(race_val):\n        mask = race_val == r\n        rates.append(y_pred[mask].mean())\n    dp_diffs.append(max(rates) - min(rates))\n\n# Print average CV metrics\nprint(f\"CV Accuracy: {np.mean(accs):.4f}\")\nprint(f\"CV Demographic Parity Difference: {np.mean(dp_diffs):.4f}\")\n\n# Retrain on full data\nmodel.fit(X, y)\np_full = model.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(races):\n    mask = races == r\n    thresholds_full[r] = np.quantile(p_full[mask], 1 - p_rate_full)\n\n# Predict on test set\nX_test = test[FEATURES]\nrace_test = test[\"RAC1P\"].astype(int)\np_test = model.predict_proba(X_test)[:, 1]\ny_test_pred = np.zeros(len(test), dtype=int)\nfor r in np.unique(race_test):\n    mask = race_test == r\n    t = thresholds_full.get(r, np.quantile(p_test, 1 - p_rate_full))\n    y_test_pred[mask] = (p_test[mask] > t).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\nX = train.drop(columns=[\"PINCP\"])\ny = train[\"PINCP\"].astype(int)\ngroups = X[\"RAC1P\"]\nX = X.drop(columns=[\"RAC1P\"])  # exclude race\n\n# 5-fold stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = []\ndp_diffs = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n    g_tr = groups.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    g_val = groups.iloc[val_idx]\n    # Compute reweighing weights\n    df_tr = pd.DataFrame({\"g\": g_tr, \"y\": y_tr})\n    pg = df_tr[\"g\"].value_counts(normalize=True).to_dict()\n    py = df_tr[\"y\"].value_counts(normalize=True).to_dict()\n    pgy = (df_tr.groupby([\"g\", \"y\"]).size() / len(df_tr)).to_dict()\n    weights = np.array(\n        [\n            (pg[g_i] * py[y_i]) / pgy[(g_i, y_i)]\n            for g_i, y_i in zip(df_tr[\"g\"], df_tr[\"y\"])\n        ]\n    )\n    # Train\n    clf = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n    clf.fit(X_tr, y_tr, sample_weight=weights)\n    # Predict\n    y_pred = clf.predict(X_val)\n    accs.append((y_pred == y_val).mean())\n    # Demographic parity difference\n    prs = []\n    for g in g_val.unique():\n        idx = g_val == g\n        prs.append(y_pred[idx].mean())\n    dp_diffs.append(max(prs) - min(prs))\n\n# Print CV results\nprint(f\"CV Accuracy: {np.mean(accs):.4f}, Mean DP diff: {np.mean(dp_diffs):.4f}\")\n\n# Retrain on full data\ndf_full = pd.DataFrame({\"g\": groups, \"y\": y})\npg = df_full[\"g\"].value_counts(normalize=True).to_dict()\npy = df_full[\"y\"].value_counts(normalize=True).to_dict()\npgy = (df_full.groupby([\"g\", \"y\"]).size() / len(df_full)).to_dict()\nweights_full = np.array(\n    [\n        (pg[g_i] * py[y_i]) / pgy[(g_i, y_i)]\n        for g_i, y_i in zip(df_full[\"g\"], df_full[\"y\"])\n    ]\n)\nclf_full = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\nclf_full.fit(X, y, sample_weight=weights_full)\n\n# Predict on test and save submission\ntest = pd.read_csv(\"./input/test.csv\")\nX_test = test.drop(columns=[\"RAC1P\"])\ny_test_pred = clf_full.predict(X_test)\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred.astype(bool)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"WKHP\", \"COW\", \"MAR\", \"SEX\"]\nnum_feats = [\"AGEP\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"WKHP\"]\ncat_feats = [\"COW\", \"MAR\", \"SEX\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nraces = train[\"RAC1P\"]\n\n# Preprocessor\npreprocessor = ColumnTransformer(\n    [\n        (\"num\", StandardScaler(), num_feats),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_feats),\n    ]\n)\n\n# Cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs, dp_diffs = [], []\nfor train_idx, val_idx in skf.split(X, y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    races_train, races_val = races.iloc[train_idx], races.iloc[val_idx]\n    # Pipeline\n    pipe = Pipeline(\n        [\n            (\"pre\", preprocessor),\n            (\"clf\", LogisticRegression(max_iter=1000, random_state=42)),\n        ]\n    )\n    pipe.fit(X_train, y_train)\n    # Train probs for thresholding\n    prob_train = pipe.predict_proba(X_train)[:, 1]\n    base_rate = y_train.mean()\n    # Compute per-race thresholds\n    thresholds = {}\n    for r in np.unique(races_train):\n        probs_r = prob_train[races_train == r]\n        if len(probs_r) > 0:\n            thresholds[r] = np.quantile(probs_r, 1 - base_rate)\n    # Validate\n    prob_val = pipe.predict_proba(X_val)[:, 1]\n    preds = np.zeros_like(y_val)\n    for i, r in enumerate(races_val):\n        thr = thresholds.get(r, np.quantile(prob_train, 1 - base_rate))\n        preds[i] = prob_val[i] >= thr\n    accs.append(accuracy_score(y_val, preds))\n    # Demographic parity difference\n    rates = []\n    for r in np.unique(races_val):\n        grp = preds[races_val == r]\n        if len(grp) > 0:\n            rates.append(grp.mean())\n    dp_diffs.append(max(rates) - min(rates))\n\n# Print CV metrics\nmean_acc = np.mean(accs)\nmean_dp = np.mean(dp_diffs)\nprint(f\"CV Accuracy: {mean_acc:.4f}\")\nprint(f\"Mean DP difference: {mean_dp:.4f}\")\n\n# Retrain on full data\npipe_full = Pipeline(\n    [(\"pre\", preprocessor), (\"clf\", LogisticRegression(max_iter=1000, random_state=42))]\n)\npipe_full.fit(X, y)\nprob_full = pipe_full.predict_proba(X)[:, 1]\nbase_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(races):\n    probs_r = prob_full[races == r]\n    if len(probs_r) > 0:\n        thresholds_full[r] = np.quantile(probs_r, 1 - base_rate_full)\nglobal_thr = np.quantile(prob_full, 1 - base_rate_full)\n\n# Predict on test\nX_test = test[features]\nraces_test = test[\"RAC1P\"]\nprob_test = pipe_full.predict_proba(X_test)[:, 1]\npreds_test = np.array(\n    [\n        (prob_test[i] >= thresholds_full.get(r, global_thr))\n        for i, r in enumerate(races_test)\n    ],\n    dtype=int,\n)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": preds_test})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Load data\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\n\n# Features\nexclude = [\"PINCP\", \"RAC1P\"]\ncategorical = [\"COW\", \"MAR\", \"SEX\"]\nnumerical = [c for c in train.columns if c not in exclude + categorical]\n\n# One-hot encode categorical\nX_train = pd.get_dummies(train.drop(columns=exclude), columns=categorical)\nX_test = pd.get_dummies(test.drop(columns=[\"RAC1P\"]), columns=categorical)\nX_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n\n# Scale numerical\nscaler = StandardScaler()\nX_train[numerical] = scaler.fit_transform(X_train[numerical])\nX_test[numerical] = scaler.transform(X_test[numerical])\n\ny = train[\"PINCP\"].astype(int).values\nr = train[\"RAC1P\"].values\n\n# Compute reweighing weights\nN = len(train)\ncount_r = train[\"RAC1P\"].value_counts().to_dict()\ncount_y = train[\"PINCP\"].value_counts().to_dict()\ncount_ry = train.groupby([\"RAC1P\", \"PINCP\"]).size().to_dict()\nw_map = {}\nfor (race, ylab), c_ry in count_ry.items():\n    p_r = count_r[race] / N\n    p_y = count_y[ylab] / N\n    p_ry = c_ry / N\n    w_map[(race, ylab)] = (p_r * p_y) / p_ry\n\nsample_weights = np.array([w_map[(race, yi)] for race, yi in zip(r, y)])\n\n# 5-fold CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs, dps = [], []\n\nfor tr_idx, val_idx in skf.split(X_train, y):\n    X_tr, X_val = X_train.values[tr_idx], X_train.values[val_idx]\n    y_tr, y_val = y[tr_idx], y[val_idx]\n    sw_tr = sample_weights[tr_idx]\n    r_val = r[val_idx]\n\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X_tr, y_tr, sample_weight=sw_tr)\n    y_pred = model.predict(X_val)\n\n    accs.append(accuracy_score(y_val, y_pred))\n    rates = [y_pred[r_val == race].mean() for race in np.unique(r_val)]\n    dps.append(max(rates) - min(rates))\n\ncv_acc = np.mean(accs)\ncv_dp = np.mean(dps)\nprint(f\"CV Accuracy: {cv_acc:.4f}, Demographic Parity Difference: {cv_dp:.4f}\")\n\n# Retrain on full data\nmodel_full = LogisticRegression(max_iter=1000)\nmodel_full.fit(X_train.values, y, sample_weight=sample_weights)\ny_test_pred = model_full.predict(X_test.values).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"ID\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\nX = train.drop(columns=[\"PINCP\", \"RAC1P\"])\ny = train[\"PINCP\"].astype(int)\ngroups = train[\"RAC1P\"]\n\n# 5-fold stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = []\ndp_diffs = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n    g_tr = groups.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    g_val = groups.iloc[val_idx]\n\n    # Compute reweighing weights\n    df_tr = pd.DataFrame({\"g\": g_tr, \"y\": y_tr})\n    pg = df_tr[\"g\"].value_counts(normalize=True).to_dict()\n    py = df_tr[\"y\"].value_counts(normalize=True).to_dict()\n    pgy = (df_tr.groupby([\"g\", \"y\"]).size() / len(df_tr)).to_dict()\n    weights = np.array(\n        [\n            (pg[g_i] * py[y_i]) / pgy[(g_i, y_i)]\n            for g_i, y_i in zip(df_tr[\"g\"], df_tr[\"y\"])\n        ]\n    )\n\n    # Train weighted logistic regression\n    clf = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n    clf.fit(X_tr, y_tr, sample_weight=weights)\n\n    # Predict on validation\n    y_pred = clf.predict(X_val)\n    accs.append((y_pred == y_val).mean())\n    # Demographic parity difference\n    prs = [y_pred[g_val == g].mean() for g in np.unique(g_val)]\n    dp_diffs.append(max(prs) - min(prs))\n\n# Print CV results\nprint(f\"CV Accuracy: {np.mean(accs):.4f}, Mean DP diff: {np.mean(dp_diffs):.4f}\")\n\n# Retrain on full data\ndf_full = pd.DataFrame({\"g\": groups, \"y\": y})\npg = df_full[\"g\"].value_counts(normalize=True).to_dict()\npy = df_full[\"y\"].value_counts(normalize=True).to_dict()\npgy = (df_full.groupby([\"g\", \"y\"]).size() / len(df_full)).to_dict()\nweights_full = np.array(\n    [\n        (pg[g_i] * py[y_i]) / pgy[(g_i, y_i)]\n        for g_i, y_i in zip(df_full[\"g\"], df_full[\"y\"])\n    ]\n)\nclf_full = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\nclf_full.fit(X, y, sample_weight=weights_full)\n\n# Predict on test and save submission\ntest = pd.read_csv(\"./input/test.csv\")\nX_test = test.drop(columns=[\"RAC1P\", \"PINCP\"])  # Drop PINCP to match training features\ny_test_pred = clf_full.predict(X_test)\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred.astype(bool)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport os\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nFEATURES = [c for c in train.columns if c not in [\"PINCP\"]]\nNUMERIC = [c for c in FEATURES if c != \"RAC1P\"]\nCAT = [\"RAC1P\"]\n\nX = train[FEATURES]\ny = train[\"PINCP\"].astype(int)\nraces = train[\"RAC1P\"].astype(int)\n\n# Preprocessing and model pipeline\npreprocessor = ColumnTransformer(\n    [\n        (\"num\", StandardScaler(), NUMERIC),\n        (\"race\", OneHotEncoder(sparse_output=False), CAT),\n    ],\n    remainder=\"drop\",\n)\nmodel = Pipeline(\n    [\n        (\"pre\", preprocessor),\n        (\n            \"clf\",\n            LogisticRegression(\n                solver=\"lbfgs\", max_iter=1000, n_jobs=-1, random_state=42\n            ),\n        ),\n    ]\n)\n\n# 5-fold stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\naccs = []\ndp_diffs = []\n\nfor tr_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    race_tr, race_val = races.iloc[tr_idx], races.iloc[val_idx]\n\n    # Fit model\n    model.fit(X_tr, y_tr)\n    # Predict probabilities\n    p_tr = model.predict_proba(X_tr)[:, 1]\n    p_val = model.predict_proba(X_val)[:, 1]\n\n    # Overall positive rate in train\n    p_rate = y_tr.mean()\n    # Compute thresholds per race on train\n    thresholds = {}\n    for r in np.unique(race_tr):\n        mask = race_tr == r\n        if mask.sum() > 0:\n            thresholds[r] = np.quantile(p_tr[mask], 1 - p_rate)\n    # Apply thresholds to validation\n    y_pred = np.zeros_like(y_val)\n    for r in np.unique(race_val):\n        mask = race_val == r\n        t = thresholds.get(r, np.quantile(p_val, 1 - p_rate))\n        y_pred[mask] = (p_val[mask] > t).astype(int)\n    # Metrics\n    accs.append(accuracy_score(y_val, y_pred))\n    rates = []\n    for r in np.unique(race_val):\n        mask = race_val == r\n        rates.append(y_pred[mask].mean())\n    dp_diffs.append(max(rates) - min(rates))\n\nprint(f\"CV Accuracy: {np.mean(accs):.4f}\")\nprint(f\"CV Demographic Parity Difference: {np.mean(dp_diffs):.4f}\")\n\n# Retrain on full data\nmodel.fit(X, y)\np_full = model.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(races):\n    mask = races == r\n    thresholds_full[r] = np.quantile(p_full[mask], 1 - p_rate_full)\n\n# Predict on test set\nX_test = test[FEATURES]\nrace_test = test[\"RAC1P\"].astype(int)\np_test = model.predict_proba(X_test)[:, 1]\ny_test_pred = np.zeros(len(test), dtype=int)\nfor r in np.unique(race_test):\n    mask = race_test == r\n    t = thresholds_full.get(r, np.quantile(p_test, 1 - p_rate_full))\n    y_test_pred[mask] = (p_test[mask] > t).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr)\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\ncategorical_feats = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\n\nX = train[features].copy()\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\n\nX_test = test[features].copy()\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Convert to category dtype\nfor col in categorical_feats:\n    X[col] = X[col].astype(\"category\")\n    X_test[col] = X_test[col].astype(\"category\")\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr, categorical_feature=categorical_feats)\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=categorical_feats)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import sys\nimport subprocess\n\n# install fairlearn if not already installed\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"fairlearn\"])\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nfrom fairlearn.postprocessing import ThresholdOptimizer\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search with ThresholdOptimizer post-processing\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        # Train base model\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr)\n        # Fairlearn threshold optimizer\n        post = ThresholdOptimizer(\n            estimator=clf,\n            constraints=\"demographic_parity\",\n            predict_method=\"predict_proba\",\n            prefit=True,\n            grid_size=100,\n        )\n        post.fit(X_tr, y_tr, sensitive_features=race_tr)\n        # Predict on validation\n        y_pred = post.predict(X_val, sensitive_features=race_val)\n        # Metrics\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params and ThresholdOptimizer\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y)\npost_full = ThresholdOptimizer(\n    estimator=clf_full,\n    constraints=\"demographic_parity\",\n    predict_method=\"predict_proba\",\n    prefit=True,\n    grid_size=100,\n)\npost_full.fit(X, y, sensitive_features=race)\n\n# Predict on test\ny_test_pred = post_full.predict(X_test, sensitive_features=race_test)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target (now including race as a feature)\nfeatures = [\n    \"AGEP\",\n    \"COW\",\n    \"MAR\",\n    \"OCCP\",\n    \"POBP\",\n    \"RELP\",\n    \"SCHL\",\n    \"SEX\",\n    \"WKHP\",\n    \"RAC1P\",\n]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr)\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Identify features\nnumerical_feats = [\"AGEP\", \"WKHP\"]\ncategorical_feats = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\nfeatures = numerical_feats + categorical_feats\n\n# Convert to category dtype\nfor col in categorical_feats:\n    train[col] = train[col].astype(\"category\")\n    test[col] = test[col].astype(\"category\")\n\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\n\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# CV setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search with per\u2010race thresholding\nfor params in param_grid:\n    accs = []\n    dp_diffs = []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr, categorical_feature=categorical_feats, verbose=False)\n\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        overall_rate = y_tr.mean()\n\n        # Compute thresholds per race to match overall positive rate\n        thresholds = {}\n        for r in np.unique(race_tr):\n            grp_probs = p_tr[race_tr == r]\n            if len(grp_probs) > 0:\n                thresholds[r] = np.quantile(grp_probs, 1 - overall_rate)\n            else:\n                thresholds[r] = 0.5\n\n        # Apply thresholds\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n\n        accs.append(accuracy_score(y_val, y_pred))\n\n        # Demographic parity difference\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc = mean_acc\n        best_dp = mean_dp\n        best_params = params\n\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Best DP Difference: {best_dp:.4f}\")\nprint(f\"Best Params: {best_params}\")\n\n# Retrain on full data\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=categorical_feats, verbose=False)\n\np_full = clf_full.predict_proba(X)[:, 1]\noverall_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    grp_probs = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(grp_probs, 1 - overall_rate_full) if len(grp_probs) > 0 else 0.5\n    )\n\n# Predict on test set\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\n\n# Save submission\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\ncat_features = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(\n            X_tr,\n            y_tr,\n            categorical_feature=cat_features,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=50,\n            verbose=False,\n        )\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=cat_features, verbose=False)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\ncat_features = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\n\nX = train[features].copy()\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\n\nX_test = test[features].copy()\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Convert categorical features to category dtype\nfor df in [X, X_test]:\n    for c in cat_features:\n        df[c] = df[c].astype(\"category\")\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr, categorical_feature=cat_features, verbose=False)\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=cat_features, verbose=False)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Ensure working directory exists\nos.makedirs(\"./working\", exist_ok=True)\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Identify features\nnumerical_feats = [\"AGEP\", \"WKHP\"]\ncategorical_feats = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\nfeatures = numerical_feats + categorical_feats\n\n# Convert to category dtype\nfor col in categorical_feats:\n    train[col] = train[col].astype(\"category\")\n    test[col] = test[col].astype(\"category\")\n\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\n\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# CV setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search with per\u2010race thresholding\nfor params in param_grid:\n    accs = []\n    dp_diffs = []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr, categorical_feature=categorical_feats)\n\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        overall_rate = y_tr.mean()\n\n        # Compute thresholds per race to match overall positive rate\n        thresholds = {}\n        for r in np.unique(race_tr):\n            grp_probs = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(grp_probs, 1 - overall_rate) if len(grp_probs) > 0 else 0.5\n            )\n\n        # Apply thresholds\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n\n        accs.append(accuracy_score(y_val, y_pred))\n\n        # Demographic parity difference\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc = mean_acc\n        best_dp = mean_dp\n        best_params = params\n\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Best DP Difference: {best_dp:.4f}\")\nprint(f\"Best Params: {best_params}\")\n\n# Retrain on full data\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=categorical_feats)\n\np_full = clf_full.predict_proba(X)[:, 1]\noverall_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    grp_probs = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(grp_probs, 1 - overall_rate_full) if len(grp_probs) > 0 else 0.5\n    )\n\n# Predict on test set\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\n\n# Save submission\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid (fixed high n_estimators with early stopping)\nparam_grid = [\n    {\"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0.0\nbest_dp = None\nbest_params = None\n\n# Grid search with early stopping\nfor params in param_grid:\n    params[\"n_estimators\"] = 1000\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(\n            X_tr,\n            y_tr,\n            eval_set=[(X_val, y_val)],\n            eval_metric=\"binary_logloss\",\n            early_stopping_rounds=50,\n            verbose=False,\n        )\n        # Predict probabilities\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        # Determine global positive rate on train\n        p_rate = y_tr.mean()\n        # Compute per-race thresholds\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        # Apply thresholds to val\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    print(f\"Params {params} => CV Acc: {mean_acc:.4f}, DP diff: {mean_dp:.4f}\")\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params.copy()\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params (no early stopping here)\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y)\n\n# Full-data thresholds\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Define features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\ncat_feats = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search with categorical features\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(\n            X_tr,\n            y_tr,\n            categorical_feature=cat_feats,\n        )\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        # calibrate thresholds for demographic parity\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        # predict with group thresholds\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=cat_feats)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test set\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nimport os\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\ncat_features = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr, categorical_feature=cat_features)\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_val = clf.predict_proba(X_val)[:, 1]\n        # determine per\u2010race threshold to match overall positive rate\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\n# Print best CV results\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f}, DP Difference: {best_dp:.4f}, Params: {best_params}\"\n)\n\n# Retrain on full data with best params\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=cat_features)\np_full = clf_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\n\n# Save submission\nos.makedirs(\"./working\", exist_ok=True)\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\nX_test = test[features]\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Define hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_dp = None\nbest_params = None\n\n# Grid search with calibration\nfor params in param_grid:\n    accs, dp_diffs = [], []\n    for tr_idx, val_idx in skf.split(X, y):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n        race_tr, race_val = race.iloc[tr_idx], race.iloc[val_idx]\n        # Base model\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr)\n        # Calibrate probabilities\n        cal = CalibratedClassifierCV(base_estimator=clf, method=\"sigmoid\", cv=\"prefit\")\n        cal.fit(X_tr, y_tr)\n        p_tr = cal.predict_proba(X_tr)[:, 1]\n        p_val = cal.predict_proba(X_val)[:, 1]\n        # Compute thresholds per race to match overall positive rate\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        y_pred = np.array(\n            [1 if p_val[i] >= thresholds[r] else 0 for i, r in enumerate(race_val)]\n        )\n        accs.append(accuracy_score(y_val, y_pred))\n        rates = [\n            y_pred[race_val == r].mean()\n            for r in np.unique(race_val)\n            if (race_val == r).sum() > 0\n        ]\n        dp_diffs.append(max(rates) - min(rates))\n    mean_acc = np.mean(accs)\n    mean_dp = np.mean(dp_diffs)\n    if mean_acc > best_acc:\n        best_acc, best_dp, best_params = mean_acc, mean_dp, params\n\nprint(f\"Best params: {best_params}\")\nprint(f\"Best CV Accuracy: {best_acc:.4f}\")\nprint(f\"Corresponding DP Difference: {best_dp:.4f}\")\n\n# Retrain on full data with best params and calibration\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y)\ncal_full = CalibratedClassifierCV(\n    base_estimator=clf_full, method=\"sigmoid\", cv=\"prefit\"\n)\ncal_full.fit(X, y)\n\np_full = cal_full.predict_proba(X)[:, 1]\np_rate_full = y.mean()\nthresholds_full = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Predict on test\np_test = cal_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_test)]\n)\n\n# Save submission\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Define features and target\nfeatures = [\"AGEP\", \"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\", \"WKHP\"]\ncat_features = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\", \"SEX\"]\n\nX = train[features].copy()\ny = train[\"PINCP\"].astype(int)\nrace = train[\"RAC1P\"].astype(int)\n\nX_test = test[features].copy()\nrace_test = test[\"RAC1P\"].astype(int)\n\n# Convert categorical features to category dtype\nfor df in [X, X_test]:\n    for c in cat_features:\n        df[c] = df[c].astype(\"category\")\n\n# Split into training and hold-out validation sets\nX_tr_full, X_val, y_tr_full, y_val, race_tr_full, race_val = train_test_split(\n    X, y, race, test_size=0.2, stratify=y, random_state=42\n)\n\n# Prepare CV on the training split\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nparam_grid = [\n    {\"n_estimators\": n, \"num_leaves\": nl, \"learning_rate\": lr, \"max_depth\": md}\n    for n in [100, 200]\n    for nl in [31, 63]\n    for lr in [0.05, 0.1]\n    for md in [-1, 7]\n]\n\nbest_acc = 0\nbest_params = None\n\n# Grid search with CV\nfor params in param_grid:\n    accs = []\n    for tr_idx, val_idx in skf.split(X_tr_full, y_tr_full):\n        X_tr, X_cv = X_tr_full.iloc[tr_idx], X_tr_full.iloc[val_idx]\n        y_tr, y_cv = y_tr_full.iloc[tr_idx], y_tr_full.iloc[val_idx]\n        race_tr, race_cv = race_tr_full.iloc[tr_idx], race_tr_full.iloc[val_idx]\n        clf = lgb.LGBMClassifier(random_state=42, **params)\n        clf.fit(X_tr, y_tr, categorical_feature=cat_features)\n        # Calibrate thresholds on X_tr\n        p_tr = clf.predict_proba(X_tr)[:, 1]\n        p_rate = y_tr.mean()\n        thresholds = {}\n        for r in np.unique(race_tr):\n            probs_r = p_tr[race_tr == r]\n            thresholds[r] = (\n                np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n            )\n        # Predict on CV fold\n        p_cv = clf.predict_proba(X_cv)[:, 1]\n        y_cv_pred = np.array(\n            [1 if p_cv[i] >= thresholds[r] else 0 for i, r in enumerate(race_cv)]\n        )\n        accs.append(accuracy_score(y_cv, y_cv_pred))\n    mean_acc = np.mean(accs)\n    if mean_acc > best_acc:\n        best_acc = mean_acc\n        best_params = params\n\nprint(f\"Best CV Accuracy on training split: {best_acc:.4f}\")\nprint(f\"Best Parameters: {best_params}\")\n\n# Retrain on full training split with best params\nclf_final = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_final.fit(X_tr_full, y_tr_full, categorical_feature=cat_features)\n\n# Calibrate thresholds on full training split\np_tr_full = clf_final.predict_proba(X_tr_full)[:, 1]\np_rate_full = y_tr_full.mean()\nthresholds_full = {}\nfor r in np.unique(race_tr_full):\n    probs_r = p_tr_full[race_tr_full == r]\n    thresholds_full[r] = (\n        np.quantile(probs_r, 1 - p_rate_full) if len(probs_r) > 0 else 0.5\n    )\n\n# Evaluate on hold-out validation set\np_val = clf_final.predict_proba(X_val)[:, 1]\ny_val_pred = np.array(\n    [1 if p_val[i] >= thresholds_full[r] else 0 for i, r in enumerate(race_val)]\n)\nholdout_acc = accuracy_score(y_val, y_val_pred)\nrates = [\n    y_val_pred[race_val == r].mean()\n    for r in np.unique(race_val)\n    if (race_val == r).sum() > 0\n]\nholdout_dp = max(rates) - min(rates)\nprint(f\"Hold-out Validation Accuracy: {holdout_acc:.4f}\")\nprint(f\"Hold-out Demographic Parity Difference: {holdout_dp:.4f}\")\n\n# Retrain on entire dataset\nclf_full = lgb.LGBMClassifier(random_state=42, **best_params)\nclf_full.fit(X, y, categorical_feature=cat_features)\n\n# Calibrate on full data\np_full = clf_full.predict_proba(X)[:, 1]\np_rate = y.mean()\nthresholds = {}\nfor r in np.unique(race):\n    probs_r = p_full[race == r]\n    thresholds[r] = np.quantile(probs_r, 1 - p_rate) if len(probs_r) > 0 else 0.5\n\n# Predict on test set\np_test = clf_full.predict_proba(X_test)[:, 1]\ny_test_pred = np.array(\n    [1 if p_test[i] >= thresholds[r] else 0 for i, r in enumerate(race_test)]\n)\n\n# Save submission\nsubmission = pd.DataFrame({\"id\": test.index, \"PINCP\": y_test_pred})\nsubmission.to_csv(\"./working/submission.csv\", index=False)"], "term_out": ["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001890 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 682\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 680\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nCV Accuracy: 0.8052 \u00b1 0.0023\nCV Demographic Parity Difference: 0.4474\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: 4 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 26, in <module>\n    [(\"num\", StandardScaler(), NUMERIC), (\"race\", OneHotEncoder(sparse=False), CAT)],\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'\nExecution time: a moment seconds (time limit is an hour).", "CV Accuracy: 0.7718, Mean DP diff: 0.5736\nTraceback (most recent call last):\n  File \"runfile.py\", line 67, in <module>\n    y_test_pred = clf_full.predict(X_test)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/linear_model/_base.py\", line 382, in predict\n    scores = self.decision_function(X)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/linear_model/_base.py\", line 363, in decision_function\n    X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/base.py\", line 608, in _validate_data\n    self._check_feature_names(X, reset=reset)\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/base.py\", line 535, in _check_feature_names\n    raise ValueError(message)\nValueError: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- PINCP\n\nExecution time: 53 seconds seconds (time limit is an hour).", "CV Accuracy: 0.7681\nMean DP difference: 0.4507\nExecution time: 3 minutes seconds (time limit is an hour).", "CV Accuracy: 0.7851, Demographic Parity Difference: 0.5186\nExecution time: 3 seconds seconds (time limit is an hour).", "CV Accuracy: 0.7718, Mean DP diff: 0.5736\nExecution time: 52 seconds seconds (time limit is an hour).", "CV Accuracy: 0.7661\nCV Demographic Parity Difference: 0.4387\nExecution time: 4 seconds seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 682\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000677 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 680\n[Light\n ... [67112 characters truncated] ... \n=true`.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: a minute seconds (time limit is an hour).", "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002097 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 867\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000815 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 865\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 865\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categori\n ... [61673 characters truncated] ... \nfo] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBest params: {'n_estimators': 100, 'num_leaves': 63, 'learning_rate': 0.1, 'max_depth': 7}\nBest CV Accuracy: 0.8068\nCorresponding DP Difference: 0.4572\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 870\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: a minute seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001415 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 682\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001740 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003700 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 684\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 680\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]\n ... [64233 characters truncated] ... \ntrue`.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: 2 minutes seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003698 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 693\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 692\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000558 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 693\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000956 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[L\n ... [63219 characters truncated] ... \nInfo] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: 2 minutes seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 54, in <module>\n    clf.fit(X_tr, y_tr, categorical_feature=categorical_feats, verbose=False)\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 44, in <module>\n    clf.fit(\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 51, in <module>\n    clf.fit(X_tr, y_tr, categorical_feature=cat_features, verbose=False)\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001290 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 867\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 865\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001501 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 865\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categori\n ... [61729 characters truncated] ... \nGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001332 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBest CV Accuracy: 0.8068\nBest DP Difference: 0.4572\nBest Params: {'n_estimators': 100, 'num_leaves': 63, 'learning_rate': 0.1, 'max_depth': 7}\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 870\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: 2 minutes seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 43, in <module>\n    clf.fit(\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001290 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 774\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001624 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 773\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categori\n ... [62222 characters truncated] ... \ns with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000640 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 770\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBest params: {'n_estimators': 200, 'num_leaves': 63, 'learning_rate': 0.05, 'max_depth': 7}\nBest CV Accuracy: 0.7894\nCorresponding DP Difference: 0.4665\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001201 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 776\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: 2 minutes seconds (time limit is an hour).", "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 867\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000858 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 865\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 865\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categori\n ... [61657 characters truncated] ... \n5\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002163 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBest CV Accuracy: 0.8068, DP Difference: 0.4572, Params: {'n_estimators': 100, 'num_leaves': 63, 'learning_rate': 0.1, 'max_depth': 7}\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003734 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 870\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: 2 minutes seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001627 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraceback (most recent call last):\n  File \"runfile.py\", line 47, in <module>\n    cal = CalibratedClassifierCV(base_estimator=clf, method=\"sigmoid\", cv=\"prefit\")\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'\nExecution time: a second seconds (time limit is an hour).", "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 41101, number of negative: 59079\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000945 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 859\n[LightGBM] [Info] Number of data points in the train set: 100180, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410272 -> initscore=-0.362843\n[LightGBM] [Info] Start training from score -0.362843\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 41101, number of negative: 59079\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 858\n[LightGBM] [Info] Number of data points in the train set: 100180, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410272 -> initscore=-0.362843\n[LightGBM] [Info] Start training from score -0.362843\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 41101, number of negative: 59079\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 100180, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410272 -> initscore=-0.362843\n[LightGBM] [Info] Start training from score -0.362843\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categori\n ... [62908 characters truncated] ... \n[Info] Start training from score -0.362884\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nBest CV Accuracy on training split: 0.8069\nBest Parameters: {'n_estimators': 200, 'num_leaves': 63, 'learning_rate': 0.05, 'max_depth': 7}\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 867\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nHold-out Validation Accuracy: 0.8083\nHold-out Demographic Parity Difference: 0.4615\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001207 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 870\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nExecution time: 2 minutes seconds (time limit is an hour)."], "analysis": ["The code successfully trained a LightGBM classifier with stratified 5-fold cross-validation, applying group-specific thresholds to minimize demographic parity difference while predicting PINCP. The model achieved a stable cross-validation accuracy of approximately 80.52% with a demographic parity difference of about 0.4474, indicating some disparity remains between racial groups. The final model was retrained on the full dataset and predictions were generated for the test set without errors.", "The code failed at the preprocessing step due to a TypeError in OneHotEncoder. The 'sparse' argument is not recognized in the OneHotEncoder constructor, which indicates that the scikit-learn version used does not support 'sparse' as a parameter or it should be 'sparse_output'. To fix this, replace 'sparse=False' with 'sparse_output=False' in the OneHotEncoder initialization.", "The code correctly implements a reweighing scheme to mitigate demographic parity difference and trains a logistic regression model with stratified cross-validation. The CV accuracy is about 0.7718 with a mean demographic parity difference of 0.5736. However, the code fails at prediction on the test set due to a mismatch in feature columns: the test set still contains the 'PINCP' column, which was dropped from the training features, causing a feature name mismatch error.", "The code executed successfully without any errors. The model achieved a cross-validation accuracy of approximately 0.7681, indicating reasonable predictive performance. However, the mean demographic parity difference between racial groups is quite high at about 0.4507, suggesting that the fairness constraint is not well satisfied and there is substantial disparity in positive prediction rates across races.", "The code executed successfully without errors. The logistic regression model trained with reweighing sample weights achieved a cross-validation accuracy of approximately 0.7851. However, the demographic parity difference between racial groups remains relatively high at about 0.5186, indicating that while accuracy is decent, fairness in terms of demographic parity could be improved.", "The code executed successfully without errors. The model achieved a cross-validation accuracy of approximately 77.18% while the mean demographic parity difference between racial groups was about 0.5736. The reweighing technique was applied to mitigate bias during training, but the demographic parity difference remains relatively high, indicating some disparity in prediction rates across racial groups.", "The model pipeline ran successfully without any errors. The 5-fold cross-validation yielded an average accuracy of approximately 0.766, indicating reasonable predictive performance. However, the demographic parity difference across racial groups remains relatively high at about 0.439, suggesting that the model's positive prediction rates differ substantially between races despite the threshold adjustment strategy. This reflects a trade-off between accuracy and fairness in the current approach.", "The training script executed successfully and performed a grid search over LightGBM hyperparameters to optimize accuracy while considering demographic parity difference. The best parameters were selected based on highest cross-validated accuracy, and a corresponding demographic parity difference was reported. The model was then retrained on the full dataset using the best parameters and predictions were made on the test set with race-aware thresholding to mitigate demographic parity differences. Warnings about no further splits with positive gain were observed but did not cause failures. The best cross-validated accuracy achieved was approximately 0.8539 with a demographic parity difference around 0.0896.", "The code executed successfully without critical errors. The model training with LightGBM and stratified K-fold cross-validation ran as expected. The best hyperparameters found were n_estimators=100, num_leaves=63, learning_rate=0.1, and max_depth=7, resulting in a best cross-validation accuracy of approximately 0.8068. The corresponding demographic parity difference was about 0.4572, indicating that while accuracy is reasonably high, there remains a notable disparity in prediction rates across racial groups. The warnings about categorical features with many bins are typical for LightGBM and do not indicate failure.", "The code executed successfully without any runtime errors or bugs. It performed a grid search over LightGBM hyperparameters combined with Fairlearn's ThresholdOptimizer to minimize demographic parity difference while maximizing accuracy. The best cross-validation accuracy achieved was approximately 0.4103 with a corresponding demographic parity difference around 0.3629. The model was then retrained on the full dataset with the best parameters and post-processed for fairness before generating test predictions.", "The training script executed successfully without any critical errors. The model was trained using LightGBM with a grid search over hyperparameters and evaluated using stratified 5-fold cross-validation. The approach also applied threshold adjustment per racial group to minimize demographic parity difference while maximizing accuracy. The output shows the best hyperparameters found, the corresponding cross-validation accuracy, and the demographic parity difference, indicating a balanced trade-off between accuracy and fairness.", "The code execution failed due to a TypeError caused by passing an unexpected keyword argument 'verbose' to the LGBMClassifier.fit() method. The 'verbose' parameter is not accepted by the fit function in the used LightGBM version. Removing the 'verbose' argument from the fit calls will fix the issue.", "The code raised a TypeError because the LGBMClassifier.fit() method does not accept the 'early_stopping_rounds' keyword argument directly. This is likely due to a version mismatch or incorrect usage of the LightGBM API. The fix is to use the 'callbacks' parameter with 'early_stopping' callback or update the LightGBM package to a version that supports 'early_stopping_rounds' in fit().", "The code encountered a TypeError because the LGBMClassifier.fit() method does not accept a 'verbose' keyword argument. This caused the training to fail and no model was trained or evaluated. To fix this, the 'verbose' argument should be removed from the fit() method call.", "The training and evaluation script ran successfully without any critical errors. The model achieved a best cross-validation accuracy of approximately 0.8068, with a demographic parity difference of about 0.4572. The best hyperparameters found were 100 estimators, 63 leaves, a learning rate of 0.1, and a max depth of 7. The approach of per-race thresholding to balance demographic parity was implemented and worked as intended.", "The code raises a TypeError because the 'fit' method of LGBMClassifier does not accept the 'early_stopping_rounds' argument. This is likely due to using the sklearn API of LightGBM which requires early stopping to be passed differently or using the native API 'lgb.train' for early stopping. To fix this, either switch to the native LightGBM training API or use the 'fit' method with callbacks for early stopping in the sklearn API.", "The training script executed successfully without critical errors. The model was trained with LightGBM using a grid search over hyperparameters, and demographic parity thresholds were applied per racial group to reduce bias. The best cross-validation accuracy achieved was approximately 0.7894 with a demographic parity difference of about 0.4665. The warnings about categorical feature binning did not cause failures and are typical when dealing with high-cardinality categorical features in LightGBM. Overall, the approach effectively balanced accuracy and demographic parity.", "The training script ran successfully without any critical errors or crashes. The model was trained using LightGBM with a grid search over hyperparameters, and the best cross-validated accuracy achieved was approximately 0.8068 with a demographic parity difference of about 0.4572. The final model was retrained on the full dataset using the best parameters and predictions were generated for the test set. The warnings about categorical features having more bins than the maximum allowed are common with LightGBM and do not indicate a failure. Overall, the approach effectively balances accuracy and demographic parity difference as intended.", "The code failed due to a TypeError when initializing CalibratedClassifierCV. The argument 'base_estimator' is not recognized. The correct argument name is 'estimator' instead of 'base_estimator'. Changing 'base_estimator' to 'estimator' should fix the error and allow the calibration step to run properly.", "The code executed successfully without errors. The LightGBM warnings about categorical features having more bins than the configured maximum are informational and do not cause failure. The model was tuned using stratified 5-fold CV on the training split, achieving a best CV accuracy of about 0.8069. On the hold-out validation set, the accuracy is slightly higher at 0.8083, but the demographic parity difference is quite large at 0.4615, indicating disparity in prediction rates across racial groups. The final model was trained on the entire dataset and predictions were generated for the test set with calibrated thresholds per race to address demographic parity."], "exp_name": "2-01-acs-income-ca-fairness-prompt", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
