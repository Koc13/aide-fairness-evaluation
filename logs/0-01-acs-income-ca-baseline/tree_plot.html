<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 6], [4, 5], [5, 7], [6, 9], [7, 8], [8, 10], [9, 11], [10, 14], [11, 12], [12, 15], [12, 13], [15, 17], [15, 19], [15, 16], [15, 18]], "layout": [[0.2, 0.0], [0.4, 0.0], [0.6, 0.0], [0.8, 0.0], [1.0, 0.0], [1.0, 0.16666666666666663], [0.2, 0.16666666666666663], [1.0, 0.33333333333333337], [1.0, 0.5], [0.2, 0.33333333333333337], [1.0, 0.6666666666666667], [0.2, 0.5], [0.2, 0.6666666666666667], [0.1, 0.8333333333333334], [1.0, 0.8333333333333334], [0.3, 0.8333333333333334], [0.0, 1.0], [0.2, 1.0], [0.4, 1.0], [0.6, 1.0]], "plan": ["We will use LightGBM classifier with ordinal encoding for categorical features.\nWe evaluate using 5-fold cross-validation reporting mean accuracy, then train on\nfull data, predict on test set, and save predictions to submission.csv.", "We will train a LightGBM classifier to predict PINCP, encoding categorical\nvariables via label encoding and treating AGEP and WKHP as numeric. We will\nperform 5-fold stratified cross-validation to estimate accuracy and print the\nmean CV accuracy. After CV, we will retrain on the full training data and\npredict on the test set. Finally, we will save the test predictions to\n`./working/submission.csv`.", "We build a simple scikit\u2010learn pipeline that one\u2010hot encodes categorical\nfeatures (COW, MAR, RAC1P, SEX) and standardizes numeric ones (AGEP, POBP, OCCP,\nRELP, SCHL, WKHP), then fits a logistic regression. We split the training data\ninto a stratified train/validation set to measure hold\u2010out accuracy. After\nprinting the validation accuracy, we retrain on the full training set and\npredict the test set, saving the results to `./working/submission.csv`.", "We\u2019ll one\u2010hot encode the categorical features (COW, MAR, RAC1P, SEX) and\nstandard\u2010scale the numerical ones (AGEP, POBP, OCCP, RELP, SCHL, WKHP), then\ntrain a RandomForestClassifier. We\u2019ll split the training data 80/20 stratified\nby PINCP to compute validation accuracy. After printing the validation accuracy,\nwe retrain on the full training set and predict on the test set. Finally, we\nsave the test predictions in ./working/submission.csv.", "We\u2019ll encode categorical features with one\u2010hot encoding and leave numeric\nfeatures as is, then train an XGBoost classifier with default settings. We\u2019ll\nevaluate performance using 5\u2010fold stratified cross\u2010validation and report the\nmean accuracy. After verifying the CV score, we\u2019ll retrain on the full training\nset and generate predictions on the test set. Finally, we\u2019ll write the test\npredictions to `./working/submission.csv`.", "We can replace XGBoost with scikit-learn\u2019s built-in\nHistGradientBoostingClassifier to avoid the missing xgboost dependency. The data\nloading, one-hot encoding, and StratifiedKFold evaluation remain the same, and\nwe\u2019ll compute and print the mean CV accuracy. Finally we\u2019ll retrain on the full\ndataset, generate test predictions, and save them to submission.csv in the\nworking directory.", "We fix the issue by treating POBP as a numeric feature rather than categorical,\nremoving it from the list passed to the OrdinalEncoder so we don\u2019t encounter\nunseen categories at test time. We split out a hold-out validation set to\ncompute and print accuracy, and then retrain on all data to generate our final\npredictions. Categorical encoding is done only on low-cardinality columns, and\ncontinuous columns (including POBP and OCCP) are left as-is.", "I suggest replacing the HistGradientBoosting model with LightGBM using native\ncategorical support. We\u2019ll label\u2010encode the four categorical variables, pass\ntheir names as `categorical_feature` to `LGBMClassifier`, and increase the\nnumber of trees with a smaller learning rate. This should capture categorical\npatterns more effectively and boost accuracy. We retain the 5\u2010fold stratified CV\nto measure performance, then retrain on the full data and write out the\nsubmission.", "I will remove the unsupported `verbose` argument from the `.fit()` calls and, if\ndesired, set verbosity in the `LGBMClassifier` constructor instead. I'll keep\nthe rest of the pipeline the same, use 5\u2010fold stratified CV to compute and print\nmean accuracy, then retrain on all data and generate `submission.csv`.", "We need to drop the PINCP column from the test set so that our test features\nmatch the training features in number and order. We define X_test by removing\nPINCP, fit an OrdinalEncoder on the training data only, and apply it to\nvalidation and test sets consistently. After printing the validation accuracy,\nwe refit the encoder on the full training data, retrain the model, transform the\ntest features, and save the predictions in submission.csv.", "We can fix the feature\u2010mismatch by explicitly defining our feature list (all\ncolumns except PINCP) and then selecting exactly those columns from test. This\ndrops any stray or extra columns in test (e.g. a residual PINCP). We then\nlabel\u2010encode using the union of categories from train and test on those\nfeatures, run 5\u2010fold stratified CV, print mean accuracy, retrain on the full\nset, and save the submission.", "We switch from a single train/validation split to 5\u2010fold stratified\ncross\u2010validation with early stopping in LightGBM to get a more robust estimate\nof accuracy and automatically find the optimal number of trees. In each fold we\nordinally encode categoricals, fit LGBMClassifier with a high n_estimators and\nearly_stopping_rounds on the fold\u2019s validation set, record the accuracy and best\niteration. We average the accuracies and best iterations across folds, print the\nmean accuracy, then retrain a final model on the full training data using the\naverage best iteration and predict on the test set. Finally, we save the\npredictions to submission.csv.", "I replaced the unsupported `early_stopping_rounds` argument with the LightGBM\ncallbacks API, using `lgb.early_stopping` for early stopping and\n`lgb.log_evaluation` to suppress logging. This ensures `best_iteration_` is\nproperly set without error. All other steps (ordinal encoding, 5-fold Stratified\nCV, accuracy calculation, retraining, and submission generation) remain the\nsame. The script prints the mean validation accuracy and writes out\n`submission.csv`.", "We can replace the ordinal encoding of categorical features with smoothed target\nmean encoding to provide continuous, informative predictors. For each fold, we\ncompute the smoothed mean target per category on the training split and map\nthese to both training and validation sets, filling unseen categories with the\nglobal mean. This should help LightGBM better capture the relationship between\ncategories and the target. We will then perform 5-fold CV with early stopping,\nretrain on full data, and generate test predictions as before.", "We need to treat all integer\u2010coded fields (occupation, place of birth,\nrelationship, education) as categoricals for LightGBM.  By adding OCCP, POBP,\nRELP and SCHL to our categorical list and label\u2010encoding them jointly on\ntrain+test (so unseen levels map correctly), we avoid misleading continuous\nsplits.  We then rerun 5\u2010fold stratified CV, print the mean accuracy, retrain on\nall data, predict on test, and save `submission.csv`.", "We will perform a small grid search over LightGBM\u2019s learning_rate and num_leaves\nusing 5-fold stratified CV with early stopping to find the hyperparameters that\nmaximize accuracy. For each parameter combination, we\u2019ll encode categoricals\nwith OrdinalEncoder, train with early stopping, and record mean CV accuracy and\nmean best iteration. After selecting the best configuration, we retrain on the\nfull dataset using the averaged best iteration count and output test predictions\nto `submission.csv`. This targeted hyperparameter tuning should yield a modest\ngain in predictive performance.", "I propose adding feature and data subsampling to the LightGBM models to reduce\noverfitting. Specifically, we will set feature_fraction=0.8,\nbagging_fraction=0.8, and bagging_freq=5 in both cross\u2010validation and final\ntraining while keeping the existing grid search over learning_rate and\nnum_leaves. This small change can improve generalization and boost CV accuracy\nslightly. We then retrain on the full dataset with the best parameters including\nthese subsampling settings and save the test predictions.", "We create a new categorical feature that encodes the interaction between\nrelationship status (RELP) and sex (SEX), then include it alongside the existing\ncategoricals. We rerun the same grid search over learning_rate and num_leaves\nwith 5-fold stratified CV and early stopping, which now can exploit this\ninteraction. Finally, we retrain the best model on the full training set and\nsave test predictions. This feature interaction may help the model capture\nsubgroup differences and boost accuracy.", "I will replace the ordinal encoding of categorical variables with frequency\nencoding, mapping each category to its prevalence in the training fold.  This\ngives the model continuous signals about category rarity and may improve\ngeneralization.  The rest of the grid search, early stopping, and retraining\npipeline remains unchanged so that we can directly measure the impact of this\nsingle change.", "We will extend the existing grid search by adding LightGBM\u2019s feature_fraction\nand bagging_fraction parameters (values 0.8 and 1.0) to perform feature and data\nsubsampling for better generalization. The grid search will loop over\nlearning_rate, num_leaves, feature_fraction, and bagging_fraction using 5\u2010fold\nstratified CV with early stopping and ordinal encoding of categorical features.\nWe will track mean CV accuracy and select the best parameter set, then retrain\non the full dataset with those parameters. Finally, we will save test set\npredictions to `submission.csv` in the working directory."], "code": ["import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"]\n\n# Identify categorical features\ncategorical_cols = [\"COW\", \"MAR\", \"POBP\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Encode categorical features\nencoder = OrdinalEncoder()\nX[categorical_cols] = encoder.fit_transform(X[categorical_cols])\ntest[categorical_cols] = encoder.transform(test[categorical_cols])\n\n# Initialize classifier\nclf = lgb.LGBMClassifier(random_state=42)\n\n# Evaluate with 5-fold cross-validation\nscores = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Train on full data and predict test set\nclf.fit(X, y)\npreds = clf.predict(test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\ntarget = \"PINCP\"\nfeatures = [c for c in train.columns if c != target]\n\n# Identify categorical features (including moderate-cardinality ones)\ncat_feats = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Combine for consistent encoding\nfor col in cat_feats:\n    le = LabelEncoder()\n    le.fit(pd.concat([train[col], test[col]], axis=0).astype(str))\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\nX = train[features]\ny = train[target].astype(int)\nX_test = test[features]\n\n# 5-fold stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data and predict test\nfinal_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Split features and target\nX = train.drop(columns=[\"PINCP\"])\ny = train[\"PINCP\"]\n\n# Identify feature types\ncategorical_features = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nnumeric_features = [\"AGEP\", \"POBP\", \"OCCP\", \"RELP\", \"SCHL\", \"WKHP\"]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\n\npipeline = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", LogisticRegression(max_iter=1000, random_state=42)),\n    ]\n)\n\n# Hold-out split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Train and evaluate\npipeline.fit(X_train, y_train)\ny_pred_val = pipeline.predict(X_val)\nval_acc = accuracy_score(y_val, y_pred_val)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n\n# Retrain on full data\npipeline.fit(X, y)\ny_test_pred = pipeline.predict(test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Prepare features and target\ntrain[\"PINCP\"] = train[\"PINCP\"].astype(int)\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"]\n\n# Define feature groups\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nnumerical = [\"AGEP\", \"POBP\", \"OCCP\", \"RELP\", \"SCHL\", \"WKHP\"]\n\n# Split into train/validation\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Build preprocessing + model pipeline\npreprocessor = ColumnTransformer(\n    [\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n        (\"num\", StandardScaler(), numerical),\n    ]\n)\npipeline = Pipeline(\n    [\n        (\"pre\", preprocessor),\n        (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n    ]\n)\n\n# Train and evaluate\npipeline.fit(X_train, y_train)\ny_pred_val = pipeline.predict(X_val)\nacc = accuracy_score(y_val, y_pred_val)\nprint(f\"Validation Accuracy: {acc:.4f}\")\n\n# Retrain on full data\npipeline.fit(X, y)\n\n# Prepare test features and predict\nX_test = test.copy()\nif \"PINCP\" in X_test.columns:\n    X_test = X_test.drop(\"PINCP\", axis=1)\ny_test_pred = pipeline.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred.astype(bool)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# One-hot encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nX = pd.get_dummies(X, columns=categorical, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=categorical, drop_first=True)\n\n# Align columns\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n# 5-fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = xgb.XGBClassifier(\n        use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n    )\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = xgb.XGBClassifier(\n    use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n)\nfinal_model.fit(X, y)\n\n# Predict on test set\ntest_preds = final_model.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# One-hot encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nX = pd.get_dummies(X, columns=categorical, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=categorical, drop_first=True)\n\n# Align columns\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n# 5-fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = HistGradientBoostingClassifier(random_state=42)\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = HistGradientBoostingClassifier(random_state=42)\nfinal_model.fit(X, y)\n\n# Predict on test set\ntest_preds = final_model.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\n\n# Define categorical columns (exclude high-cardinality POBP)\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Split into train/validation\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Fit encoder on training only\nencoder = OrdinalEncoder()\nX_train_enc = X_train.copy()\nX_val_enc = X_val.copy()\ntest_enc = test.copy()\n\nX_train_enc[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\nX_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\ntest_enc[categorical_cols] = encoder.transform(test[categorical_cols])\n\n# Train model on training split\nclf = lgb.LGBMClassifier(random_state=42)\nclf.fit(X_train_enc, y_train)\n\n# Evaluate on validation split\nval_preds = clf.predict(X_val_enc)\nval_acc = accuracy_score(y_val, val_preds)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n\n# Retrain on full data and predict test set\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder.fit_transform(X[categorical_cols])\nclf.fit(X_full_enc, y)\n\ntest_preds = clf.predict(test_enc)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# Label\u2010encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nfor col in categorical:\n    cats = X[col].unique().tolist()\n    X[col] = pd.Categorical(X[col], categories=cats).codes\n    X_test[col] = pd.Categorical(X_test[col], categories=cats).codes\n\n# 5\u2010fold stratified CV with LightGBM\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"multiclass\", random_state=42, learning_rate=0.05, n_estimators=300\n    )\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = LGBMClassifier(\n    objective=\"multiclass\", random_state=42, learning_rate=0.05, n_estimators=300\n)\nfinal_model.fit(X, y, categorical_feature=categorical, verbose=False)\n\n# Predict on test set and save submission\ntest_preds = final_model.predict(X_test).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# Label\u2010encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nfor col in categorical:\n    # Use the union of categories from train and test to avoid unseen codes\n    cats = pd.Series(X[col].tolist() + X_test[col].tolist()).unique().tolist()\n    X[col] = pd.Categorical(X[col], categories=cats).codes\n    X_test[col] = pd.Categorical(X_test[col], categories=cats).codes\n\n# 5\u2010fold stratified CV with LightGBM\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n    )\n    # Remove unsupported verbose= from fit\n    model.fit(X_tr, y_tr, categorical_feature=categorical)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = LGBMClassifier(\n    objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n)\nfinal_model.fit(X, y, categorical_feature=categorical)\n\n# Predict on test set and save submission\ntest_preds = final_model.predict(X_test).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Define categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Split into train/validation\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Encode categorical features\nencoder = OrdinalEncoder()\nX_train_enc = X_train.copy()\nX_val_enc = X_val.copy()\nX_test_enc = X_test.copy()\n\nX_train_enc[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\nX_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\nX_test_enc[categorical_cols] = encoder.transform(X_test[categorical_cols])\n\n# Train and evaluate\nclf = lgb.LGBMClassifier(random_state=42)\nclf.fit(X_train_enc, y_train)\nval_preds = clf.predict(X_val_enc)\nval_acc = accuracy_score(y_val, val_preds)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n\n# Retrain on full data with fresh encoder\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\n\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nclf.fit(X_full_enc, y)\ntest_preds = clf.predict(X_test_enc)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Define feature columns as all except the target\nfeatures = [col for col in train.columns if col != \"PINCP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\n# Select exactly those features in test (drops any extra columns)\nX_test = test[features]\n\n# Label\u2010encode categorical features using union of train+test categories\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nfor col in categorical:\n    cats = (\n        pd.Series(pd.concat([X[col], X_test[col]], ignore_index=True)).unique().tolist()\n    )\n    X[col] = pd.Categorical(X[col], categories=cats).codes\n    X_test[col] = pd.Categorical(X_test[col], categories=cats).codes\n\n# 5\u2010fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n    )\n    model.fit(X_tr, y_tr, categorical_feature=categorical)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data and predict on test\nfinal_model = LGBMClassifier(\n    objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n)\nfinal_model.fit(X, y, categorical_feature=categorical)\ntest_preds = final_model.predict(X_test).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Define categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\nbest_iters = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n        drop=True\n    )\n    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n        drop=True\n    )\n\n    # Encode categoricals\n    encoder = OrdinalEncoder()\n    X_tr_enc = X_tr.copy()\n    X_val_enc = X_val.copy()\n    X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n    X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n    # Train with early stopping\n    clf = lgb.LGBMClassifier(n_estimators=10000, random_state=42)\n    clf.fit(\n        X_tr_enc,\n        y_tr,\n        eval_set=[(X_val_enc, y_val)],\n        early_stopping_rounds=50,\n        categorical_feature=categorical_cols,\n        verbose=False,\n    )\n\n    # Predict and evaluate\n    preds = clf.predict(X_val_enc)\n    acc = accuracy_score(y_val, preds)\n    fold_accuracies.append(acc)\n    best_iters.append(clf.best_iteration_)\n\n# Compute mean accuracy and best iteration\nmean_acc = np.mean(fold_accuracies)\nmean_iter = int(np.mean(best_iters))\nprint(f\"Validation Accuracy: {mean_acc:.4f}\")\n\n# Retrain on full data\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\n\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(n_estimators=mean_iter, random_state=42)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Define categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\nbest_iters = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n        drop=True\n    )\n    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n        drop=True\n    )\n\n    # Encode categoricals\n    encoder = OrdinalEncoder()\n    X_tr_enc = X_tr.copy()\n    X_val_enc = X_val.copy()\n    X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n    X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n    # Train with early stopping via callbacks\n    clf = lgb.LGBMClassifier(n_estimators=10000, random_state=42)\n    clf.fit(\n        X_tr_enc,\n        y_tr,\n        eval_set=[(X_val_enc, y_val)],\n        eval_metric=\"binary_error\",\n        categorical_feature=categorical_cols,\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0),\n        ],\n    )\n\n    # Predict and evaluate\n    preds = clf.predict(X_val_enc)\n    acc = accuracy_score(y_val, preds)\n    fold_accuracies.append(acc)\n    best_iters.append(clf.best_iteration_)\n\n# Compute mean accuracy and best iteration\nmean_acc = np.mean(fold_accuracies)\nmean_iter = int(np.mean(best_iters))\nprint(f\"Validation Accuracy: {mean_acc:.4f}\")\n\n# Retrain on full data\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\n\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(n_estimators=mean_iter, random_state=42)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n\n# Mean target encoding function with smoothing\ndef mean_target_encoding(train_series, target_series, valid_series, k=5):\n    \"\"\"\n    Compute smoothed mean target encoding.\n    train_series: pd.Series of categorical feature in training split\n    target_series: pd.Series of target in training split\n    valid_series: pd.Series of categorical feature in validation/test split\n    k: smoothing parameter\n    Returns encoded_train, encoded_valid\n    \"\"\"\n    # Compute global mean\n    global_mean = target_series.mean()\n    # Compute counts and sums per category\n    stats = target_series.groupby(train_series).agg([\"count\", \"mean\"])\n    counts = stats[\"count\"]\n    means = stats[\"mean\"]\n    # Smoothed means\n    smooth = (counts * means + k * global_mean) / (counts + k)\n    # Map to series\n    train_encoded = train_series.map(smooth).fillna(global_mean)\n    valid_encoded = valid_series.map(smooth).fillna(global_mean)\n    return train_encoded, valid_encoded\n\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns to target-encode\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\nbest_iters = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n        drop=True\n    )\n    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n        drop=True\n    )\n\n    # Initialize encoded copies\n    X_tr_enc = X_tr.copy()\n    X_val_enc = X_val.copy()\n\n    # Apply mean target encoding per categorical column\n    for col in categorical_cols:\n        tr_enc, val_enc = mean_target_encoding(X_tr[col], y_tr, X_val[col], k=5)\n        X_tr_enc[col] = tr_enc\n        X_val_enc[col] = val_enc\n\n    # Train with early stopping\n    clf = lgb.LGBMClassifier(n_estimators=10000, learning_rate=0.05, random_state=42)\n    clf.fit(\n        X_tr_enc,\n        y_tr,\n        eval_set=[(X_val_enc, y_val)],\n        eval_metric=\"binary_error\",\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0),\n        ],\n    )\n\n    # Predict and evaluate\n    preds = clf.predict(X_val_enc)\n    acc = accuracy_score(y_val, preds)\n    fold_accuracies.append(acc)\n    best_iters.append(clf.best_iteration_)\n\n# Compute mean accuracy and best iteration\nmean_acc = np.mean(fold_accuracies)\nmean_iter = int(np.mean(best_iters))\nprint(f\"Validation Accuracy: {mean_acc:.4f}\")\n\n# Retrain on full data\nX_full_enc = X.copy()\nfor col in categorical_cols:\n    tr_enc_full, _ = mean_target_encoding(X[col], y, X[col], k=5)\n    X_full_enc[col] = tr_enc_full\n\nX_test_enc = X_test.copy()\nfor col in categorical_cols:\n    # Use full-data encoding to map test\n    tr_enc_full, test_enc = mean_target_encoding(X[col], y, X_test[col], k=5)\n    X_test_enc[col] = test_enc\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=mean_iter, learning_rate=0.05, random_state=42\n)\nfinal_clf.fit(X_full_enc, y)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Define features and target\nfeatures = [c for c in train.columns if c != \"PINCP\"]\nX = train[features].copy()\ny = train[\"PINCP\"].astype(int)\nX_test = test[features].copy()\n\n# Treat these integer\u2010coded columns as categorical\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\"]\n\n# Joint label\u2010encode categories train+test\nfor col in categorical:\n    unified = pd.Categorical(pd.concat([X[col], X_test[col]], ignore_index=True))\n    codes = unified.codes.reshape(-1)\n    X[col] = codes[: len(X)]\n    X_test[col] = codes[len(X) :]\n\n# 5\u2010fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor tr_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"binary\", learning_rate=0.05, n_estimators=300, random_state=42\n    )\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data and predict\nfinal_model = LGBMClassifier(\n    objective=\"binary\", learning_rate=0.05, n_estimators=300, random_state=42\n)\nfinal_model.fit(X, y, categorical_feature=categorical, verbose=False)\ntest_preds = final_model.predict(X_test).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross-validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accuracies = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                val_idx\n            ].reset_index(drop=True)\n            y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                val_idx\n            ].reset_index(drop=True)\n\n            # Ordinal encoding\n            encoder = OrdinalEncoder()\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n            X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n            # Train model with early stopping\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000, learning_rate=lr, num_leaves=nl, random_state=42\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                categorical_feature=categorical_cols,\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            # Evaluate\n            preds = clf.predict(X_val_enc)\n            acc = accuracy_score(y_val, preds)\n            fold_accuracies.append(acc)\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accuracies)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"Params lr={lr}, num_leaves={nl} -> CV Accuracy = {mean_acc:.4f}, Mean Iter = {mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Print best CV performance\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross-validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\n# Grid search with subsampling parameters\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accuracies = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                val_idx\n            ].reset_index(drop=True)\n            y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                val_idx\n            ].reset_index(drop=True)\n\n            # Ordinal encoding\n            encoder = OrdinalEncoder()\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n            X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n            # Train model with early stopping and subsampling\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000,\n                learning_rate=lr,\n                num_leaves=nl,\n                feature_fraction=0.8,\n                bagging_fraction=0.8,\n                bagging_freq=5,\n                random_state=42,\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                categorical_feature=categorical_cols,\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            # Evaluate\n            preds = clf.predict(X_val_enc)\n            acc = accuracy_score(y_val, preds)\n            fold_accuracies.append(acc)\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accuracies)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"Params lr={lr}, num_leaves={nl} -> CV Accuracy = {mean_acc:.4f}, Mean Iter = {mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Print best CV performance\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params and subsampling\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    feature_fraction=0.8,\n    bagging_fraction=0.8,\n    bagging_freq=5,\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1).copy()\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1).copy()\n\n# Create interaction feature between RELP and SEX\nX[\"RELP_SEX\"] = X[\"RELP\"].astype(str) + \"_\" + X[\"SEX\"].astype(str)\nX_test[\"RELP_SEX\"] = X_test[\"RELP\"].astype(str) + \"_\" + X_test[\"SEX\"].astype(str)\n\n# Define categorical columns including the new interaction\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\", \"RELP_SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accs = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr = X.iloc[train_idx].reset_index(drop=True)\n            X_val = X.iloc[val_idx].reset_index(drop=True)\n            y_tr = y.iloc[train_idx].reset_index(drop=True)\n            y_val = y.iloc[val_idx].reset_index(drop=True)\n\n            # Ordinal encoding\n            encoder = OrdinalEncoder()\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n            X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n            # Train with early stopping\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000, learning_rate=lr, num_leaves=nl, random_state=42\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                categorical_feature=categorical_cols,\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            preds = clf.predict(X_val_enc)\n            fold_accs.append(accuracy_score(y_val, preds))\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accs)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"lr={lr}, num_leaves={nl} -> CV Accuracy={mean_acc:.4f}, iters={mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Report best CV result\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns to frequency\u2010encode\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross\u2010validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accuracies = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                val_idx\n            ].reset_index(drop=True)\n            y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                val_idx\n            ].reset_index(drop=True)\n\n            # Frequency encoding on training fold\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            for col in categorical_cols:\n                freq = X_tr_enc[col].value_counts(normalize=True)\n                X_tr_enc[col] = X_tr_enc[col].map(freq)\n                X_val_enc[col] = X_val_enc[col].map(freq).fillna(0.0)\n\n            # Train model with early stopping\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000, learning_rate=lr, num_leaves=nl, random_state=42\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            # Evaluate\n            preds = clf.predict(X_val_enc)\n            acc = accuracy_score(y_val, preds)\n            fold_accuracies.append(acc)\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accuracies)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"Params lr={lr}, num_leaves={nl} -> CV Accuracy = {mean_acc:.4f}, Mean Iter = {mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Print best CV performance\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params using frequency encoding\nX_full_enc = X.copy()\nX_test_enc = X_test.copy()\nfor col in categorical_cols:\n    freq_full = X_full_enc[col].value_counts(normalize=True)\n    X_full_enc[col] = X_full_enc[col].map(freq_full)\n    X_test_enc[col] = X_test_enc[col].map(freq_full).fillna(0.0)\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross-validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\nfeature_fractions = [0.8, 1.0]\nbagging_fractions = [0.8, 1.0]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        for ff in feature_fractions:\n            for bf in bagging_fractions:\n                fold_accuracies = []\n                fold_iters = []\n                for train_idx, val_idx in skf.split(X, y):\n                    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                        val_idx\n                    ].reset_index(drop=True)\n                    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                        val_idx\n                    ].reset_index(drop=True)\n\n                    # Ordinal encoding\n                    encoder = OrdinalEncoder()\n                    X_tr_enc = X_tr.copy()\n                    X_val_enc = X_val.copy()\n                    X_tr_enc[categorical_cols] = encoder.fit_transform(\n                        X_tr[categorical_cols]\n                    )\n                    X_val_enc[categorical_cols] = encoder.transform(\n                        X_val[categorical_cols]\n                    )\n\n                    # Train model with early stopping and subsampling\n                    clf = lgb.LGBMClassifier(\n                        n_estimators=10000,\n                        learning_rate=lr,\n                        num_leaves=nl,\n                        feature_fraction=ff,\n                        bagging_fraction=bf,\n                        bagging_freq=5,\n                        random_state=42,\n                    )\n                    clf.fit(\n                        X_tr_enc,\n                        y_tr,\n                        eval_set=[(X_val_enc, y_val)],\n                        eval_metric=\"binary_error\",\n                        categorical_feature=categorical_cols,\n                        callbacks=[\n                            lgb.early_stopping(stopping_rounds=50),\n                            lgb.log_evaluation(period=0),\n                        ],\n                    )\n\n                    preds = clf.predict(X_val_enc)\n                    fold_accuracies.append(accuracy_score(y_val, preds))\n                    fold_iters.append(clf.best_iteration_)\n\n                mean_acc = np.mean(fold_accuracies)\n                mean_iter = int(np.mean(fold_iters))\n                print(\n                    f\"Params lr={lr}, nl={nl}, ff={ff}, bf={bf} -> CV Acc = {mean_acc:.4f}, Iter = {mean_iter}\"\n                )\n                if mean_acc > best_acc:\n                    best_acc = mean_acc\n                    best_params = {\n                        \"learning_rate\": lr,\n                        \"num_leaves\": nl,\n                        \"feature_fraction\": ff,\n                        \"bagging_fraction\": bf,\n                    }\n                    best_iter = mean_iter\n\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    feature_fraction=best_params[\"feature_fraction\"],\n    bagging_fraction=best_params[\"bagging_fraction\"],\n    bagging_freq=5,\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)"], "term_out": ["Traceback (most recent call last):\n  File \"runfile.py\", line 21, in <module>\n    test[categorical_cols] = encoder.transform(test[categorical_cols])\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py\", line 1578, in transform\n    X_int, X_mask = self._transform(\n                    ^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py\", line 214, in _transform\n    raise ValueError(msg)\nValueError: Found unknown categories [338.0] in column 2 during transform\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003733 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 685\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002619 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003022 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 683\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002724 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 688\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003189 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 678\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nCV Accuracy: 0.8224\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: 4 seconds seconds (time limit is an hour).", "Validation Accuracy: 0.7872\nExecution time: a second seconds (time limit is an hour).", "Validation Accuracy: 0.8139\nExecution time: 3 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 5, in <module>\n    import xgboost as xgb\nModuleNotFoundError: No module named 'xgboost'\nExecution time: a moment seconds (time limit is an hour).", "Mean CV Accuracy: 0.8242\nExecution time: 8 seconds seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002940 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nValidation Accuracy: 0.8255\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 692\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nTraceback (most recent call last):\n  File \"runfile.py\", line 48, in <module>\n    test_preds = clf.predict(test_enc)\n                 ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1597, in predict\n    result = self.predict_proba(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1627, in predict_proba\n    result = super().predict(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1144, in predict\n    return self._Booster.predict(  # type: ignore[union-attr]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 4767, in predict\n    return predictor.predict(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1204, in predict\n    preds, nrow = self.__pred_for_np2d(\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1361, in __pred_for_np2d\n    return self.__inner_predict_np2d(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1307, in __inner_predict_np2d\n    _safe_call(\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: The number of features in data (11) is not the same as it was in training data (10).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 32, in <module>\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004654 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003239 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002496 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 692\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002252 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you ca\n ... [300 characters truncated] ... \ncuracy: 0.8260\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 695\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nTraceback (most recent call last):\n  File \"runfile.py\", line 47, in <module>\n    test_preds = final_model.predict(X_test).astype(int)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1597, in predict\n    result = self.predict_proba(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1627, in predict_proba\n    result = super().predict(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1144, in predict\n    return self._Booster.predict(  # type: ignore[union-attr]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 4767, in predict\n    return predictor.predict(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1204, in predict\n    preds, nrow = self.__pred_for_np2d(\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1361, in __pred_for_np2d\n    return self.__inner_predict_np2d(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1307, in __inner_predict_np2d\n    _safe_call(\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: The number of features in data (11) is not the same as it was in training data (10).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\nExecution time: 9 seconds seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003424 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nValidation Accuracy: 0.8255\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003586 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 692\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: a second seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002191 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003430 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 692\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002825 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 688\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nMean CV Accuracy: 0.8260\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002674 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 695\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: 9 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 43, in <module>\n    clf.fit(\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003630 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[179]\tvalid_0's binary_error: 0.174498\tvalid_0's binary_logloss: 0.380948\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002589 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[315]\tvalid_0's binary_error: 0.170473\tvalid_0's binary_logloss: 0.375164\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002653 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[256]\tvalid_0's binary_error: 0.177761\tvalid_0's binary_logloss: 0.381979\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004189 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[430]\tvalid_0's binary_error: 0.168402\tvalid_0's binary_logloss: 0.374522\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002828 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.174056\tvalid_0's binary_logloss: 0.38163\nValidation Accuracy: 0.8270\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002550 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: 10 seconds seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[375]\tvalid_0's binary_error: 0.173955\tvalid_0's binary_logloss: 0.380173\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 693\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[433]\tvalid_0's binary_error: 0.170601\tvalid_0's binary_logloss: 0.375409\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[358]\tvalid_0's binary_error: 0.17741\tvalid_0's binary_logloss: 0.382898\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 695\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[526]\tvalid_0's binary_error: 0.168849\tvalid_0's binary_logloss: 0.374257\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[161]\tvalid_0's binary_error: 0.176516\tvalid_0's binary_logloss: 0.385554\nValidation Accuracy: 0.8265\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 698\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: 12 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 36, in <module>\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004158 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[413]\tvalid_0's binary_error: 0.173859\tvalid_0's binary_logloss: 0.379836\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003470 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 689\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[357]\tvalid_0's binary_error: 0.171176\tvalid_0's binary_logloss: 0.376679\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002618 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[346]\tvalid_0's binary_error: 0.177378\tvalid_0's binary_logloss: 0.383217\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] T\n ... [17974 characters truncated] ... \navg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[81]\tvalid_0's binary_error: 0.176835\tvalid_0's binary_logloss: 0.382959\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003690 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[119]\tvalid_0's binary_error: 0.167923\tvalid_0's binary_logloss: 0.374062\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003698 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 687\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[71]\tvalid_0's binary_error: 0.173609\tvalid_0's binary_logloss: 0.380987\nParams lr=0.1, num_leaves=127 -> CV Accuracy = 0.8277, Mean Iter = 102\nBest CV Accuracy: 0.8277 with params {'learning_rate': 0.1, 'num_leaves': 127} and n_estimators=102\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003812 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: a minute seconds (time limit is an hour).", "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004282 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[368]\tvalid_0's binary_error: 0.174594\tvalid_0's binary_logloss: 0.379865\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] baggin\n ... [60901 characters truncated] ... \n_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[130]\tvalid_0's binary_error: 0.173098\tvalid_0's binary_logloss: 0.380262\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nParams lr=0.1, num_leaves=127 -> CV Accuracy = 0.8270, Mean Iter = 103\nBest CV Accuracy: 0.8275 with params {'learning_rate': 0.05, 'num_leaves': 63} and n_estimators=248\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005046 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\nExecution time: a minute seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003432 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 726\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[236]\tvalid_0's binary_error: 0.174945\tvalid_0's binary_logloss: 0.382309\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002796 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 725\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[513]\tvalid_0's binary_error: 0.169579\tvalid_0's binary_logloss: 0.375257\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002137 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 726\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[353]\tvalid_0's binary_error: 0.177314\tvalid_0's binary_logloss: 0.38346\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002856 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] To\n ... [17884 characters truncated] ... \nFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[107]\tvalid_0's binary_error: 0.179263\tvalid_0's binary_logloss: 0.383995\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 727\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[155]\tvalid_0's binary_error: 0.168402\tvalid_0's binary_logloss: 0.374341\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003335 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 723\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[78]\tvalid_0's binary_error: 0.175046\tvalid_0's binary_logloss: 0.381293\nlr=0.1, num_leaves=127 -> CV Accuracy=0.8266, iters=121\nBest CV Accuracy: 0.8277 with params {'learning_rate': 0.05, 'num_leaves': 127} and n_estimators=205\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004096 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 730\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: a minute seconds (time limit is an hour).", "[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[427]\tvalid_0's binary_error: 0.175839\tvalid_0's binary_logloss: 0.381358\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 693\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[409]\tvalid_0's binary_error: 0.171623\tvalid_0's binary_logloss: 0.377669\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000678 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[310]\tvalid_0's binary_error: 0.178177\tvalid_0's binary_logloss: 0.385938\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] T\n ... [17969 characters truncated] ... \navg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[139]\tvalid_0's binary_error: 0.177602\tvalid_0's binary_logloss: 0.38417\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000746 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 695\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[123]\tvalid_0's binary_error: 0.168977\tvalid_0's binary_logloss: 0.374361\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73850\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000854 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 691\n[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[105]\tvalid_0's binary_error: 0.174376\tvalid_0's binary_logloss: 0.381313\nParams lr=0.1, num_leaves=127 -> CV Accuracy = 0.8269, Mean Iter = 121\nBest CV Accuracy: 0.8275 with params {'learning_rate': 0.1, 'num_leaves': 63} and n_estimators=203\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 698\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\nExecution time: a minute seconds (time limit is an hour).", "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Number of positive: 51376, number of negative: 73849\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 690\n[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851\n[LightGBM] [Info] Start training from score -0.362851\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[368]\tvalid_0's binary_error: 0.174594\tvalid_0's binary_logloss: 0.379865\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] baggin\n ... [253317 characters truncated] ... \nmScore]: pavg=0.410266 -> initscore=-0.362865\n[LightGBM] [Info] Start training from score -0.362865\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[71]\tvalid_0's binary_error: 0.173609\tvalid_0's binary_logloss: 0.380987\n[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\nParams lr=0.1, nl=127, ff=1.0, bf=1.0 -> CV Acc = 0.8277, Iter = 102\nBest CV Accuracy: 0.8279 with params {'learning_rate': 0.1, 'num_leaves': 127, 'feature_fraction': 0.8, 'bagging_fraction': 1.0} and n_estimators=112\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n[LightGBM] [Info] Number of positive: 64220, number of negative: 92312\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 694\n[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859\n[LightGBM] [Info] Start training from score -0.362859\n[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\nExecution time: 6 minutes seconds (time limit is an hour)."], "analysis": ["The code fails during the transformation of the test set categorical features because the test set contains unseen categories (e.g., 338.0 in 'POBP') that were not present in the training set. This causes the OrdinalEncoder to raise a ValueError. To fix this, the encoding approach should handle unknown categories, for example by using an encoder that supports unseen categories or by ensuring the test set categories are consistent with the train set.", "The training script executed successfully without any errors. The LightGBM model was trained using 5-fold stratified cross-validation, achieving a consistent validation accuracy of approximately 0.8224. The final model was then retrained on the full training data and used to generate predictions for the test set. Overall, the model demonstrated good predictive performance on this classification task.", "The code executed successfully without any errors. The logistic regression model achieved a validation accuracy of approximately 0.7872, indicating a reasonably good performance on the hold-out validation set. The preprocessing pipeline with standard scaling for numeric features and one-hot encoding for categorical features was effective for this task.", "The training script executed successfully without any errors. The model achieved a validation accuracy of approximately 81.39%, indicating good predictive performance on the validation set. The pipeline used a combination of one-hot encoding for categorical features and standard scaling for numerical features, followed by a RandomForestClassifier, which appears to be effective for this task.", "The code execution failed because the 'xgboost' library is not installed in the environment. To fix this, install the 'xgboost' package using pip (e.g., 'pip install xgboost') before running the script. Once installed, the script should run successfully, performing stratified 5-fold cross-validation with XGBoost and generating predictions for the test set.", "The training and evaluation script ran successfully without any errors. The model achieved a mean cross-validation accuracy of 0.8242, which indicates good predictive performance on the PINCP classification task. The final model was retrained on the full training data and predictions were generated for the test set as expected.", "The code runs successfully until the final prediction on the test set, where it raises a LightGBM error due to a mismatch in the number of features between the training data (10 features) and the test data (11 features). This is likely caused by inconsistent preprocessing or an extra feature column in the test set that was not present or removed in training data. The model training and validation steps show a validation accuracy of about 0.8255, indicating decent performance before the error occurs. To fix, ensure the test data has the exact same feature columns as the training data after encoding, possibly by aligning columns explicitly after encoding or dropping extra columns in the test set.", "The code raised a TypeError because LGBMClassifier.fit() does not accept a 'verbose' argument. Removing 'verbose=False' from the fit() calls will fix this issue.", "The training and validation phases ran correctly with a mean cross-validation accuracy of about 0.8260, indicating a reasonably good model performance. However, the code failed during the test prediction step due to a feature mismatch error: the test data has 11 features while the model was trained with 10 features. This discrepancy caused a LightGBMError and prevented the generation of the final submission file. To fix this, ensure that the test data has the exact same columns as the training data before prediction, possibly by aligning columns or dropping/adding missing features.", "The training script executed successfully without errors. The model achieved a validation accuracy of approximately 0.8255, indicating good predictive performance on the validation set. The final model was retrained on the full training data and predictions were generated for the test set, with the submission file saved correctly.", "The code attempts to perform binary classification by setting the LightGBM objective to 'binary'. However, the target variable 'PINCP' likely has multiple classes (not binary), so using 'binary' objective is incorrect. This causes LightGBM to treat the problem as binary classification, which is not suitable for a multiclass target. The fix is to change the LightGBM objective to 'multiclass' and specify the number of classes with 'num_class'. This will allow proper multiclass classification and likely improve accuracy.", "The code raised a TypeError because the LGBMClassifier.fit() method does not accept the 'early_stopping_rounds' argument. This is likely due to an API usage mistake. To fix this, the early stopping parameters should be passed via the 'callbacks' parameter or use the 'fit' method with the 'eval_set' and 'early_stopping_round' arguments correctly according to the LightGBM version. Alternatively, the native LightGBM training API can be used for early stopping.", "The LightGBM model was trained using 5-fold stratified cross-validation with early stopping. The validation accuracy across folds averaged about 82.7%, indicating solid predictive performance for this binary classification task. The best iteration counts varied per fold, and the final model was retrained on the full dataset using the average best iteration. The code executed successfully without errors or bugs.", "The training script executed successfully without any errors or bugs. The model was trained using 5-fold stratified cross-validation with mean target encoding applied to categorical variables. The validation accuracy achieved was approximately 0.8265, indicating good predictive performance. Early stopping was used effectively to prevent overfitting, with best iterations varying between folds. The final model was retrained on the full dataset using the average best iteration and predictions were generated for the test set.", "The code raises a TypeError because LGBMClassifier.fit() does not accept the 'verbose' argument. Removing the 'verbose=False' argument from the fit() calls will fix this issue.", "The training script executed successfully without any errors. The model was trained using stratified 5-fold cross-validation with LightGBM, tuning learning rate and number of leaves. The best cross-validation accuracy achieved was approximately 0.8277 with learning rate 0.1 and 127 leaves, using about 102 boosting iterations on average. The final model was then retrained on the full dataset with the best parameters and used to generate predictions for the test set.", "The training script successfully completed a grid search over learning rates and number of leaves for a LightGBM classifier using stratified 5-fold cross-validation. The best cross-validation accuracy achieved was approximately 0.8275 with learning_rate=0.05 and num_leaves=63. Early stopping was used to select the optimal number of boosting iterations, which averaged 248 iterations for the best parameters. The script then retrained the model on the full training data with the best parameters and generated predictions on the test set for submission. The warnings about bagging parameters are informational and do not indicate errors.", "The code attempts to predict 'PINCP' using LightGBM with hyperparameter tuning and cross-validation. However, the execution output shows LightGBM binary classification logs, which is inconsistent with the task goal of predicting 'PINCP', which is a multi-class classification problem (PINCP is a personal income code, not binary). Also, the evaluation metric used is 'binary_error' which is for binary classification, not suitable for multi-class. This indicates a bug in the model configuration and evaluation metric. The code should use a multi-class objective and appropriate metric like 'multi_logloss' or 'multi_error' for accuracy. The output does not show final test predictions or submission file content, and the logs are truncated, so it is unclear if the code completed successfully. Overall, the code ran but with a fundamental bug in classification setup, making the results invalid.", "The training script successfully ran without any errors or bugs. It performed a grid search over learning rates and number of leaves for a LightGBM classifier using stratified 5-fold cross-validation. Frequency encoding was applied to categorical variables. The best cross-validation accuracy achieved was approximately 0.8275 with learning rate 0.1 and num_leaves 63. The model was then retrained on the full dataset with the best parameters and early stopping iterations, and predictions were generated for the test set. Overall, the approach and results are reasonable and consistent with the task goal of predicting PINCP with accuracy evaluation.", "The training script executed successfully without any errors or bugs. The LightGBM warnings about bagging_freq and feature_fraction parameter precedence are expected and do not affect the correctness. The cross-validation with hyperparameter grid search yielded a best validation accuracy of approximately 0.8279, which indicates a good predictive performance on the PINCP target. The model was then retrained on the full training data with the best hyperparameters and the best number of boosting iterations before generating the final test set predictions and saving the submission file."], "exp_name": "0-01-acs-income-ca-baseline", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
