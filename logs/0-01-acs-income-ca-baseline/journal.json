{"nodes":[{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"]\n\n# Identify categorical features\ncategorical_cols = [\"COW\", \"MAR\", \"POBP\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Encode categorical features\nencoder = OrdinalEncoder()\nX[categorical_cols] = encoder.fit_transform(X[categorical_cols])\ntest[categorical_cols] = encoder.transform(test[categorical_cols])\n\n# Initialize classifier\nclf = lgb.LGBMClassifier(random_state=42)\n\n# Evaluate with 5-fold cross-validation\nscores = cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Train on full data and predict test set\nclf.fit(X, y)\npreds = clf.predict(test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We will use LightGBM classifier with ordinal encoding for categorical features. We evaluate using 5-fold cross-validation reporting mean accuracy, then train on full data, predict on test set, and save predictions to submission.csv.","step":0,"id":"aa05560fd60a4173a76a8a9adefd275a","ctime":1764595018.7139955,"parent":null,"children":[],"_term_out":["Traceback (most recent call last):\n  File \"runfile.py\", line 21, in <module>\n    test[categorical_cols] = encoder.transform(test[categorical_cols])\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/utils/_set_output.py\", line 313, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py\", line 1578, in transform\n    X_int, X_mask = self._transform(\n                    ^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py\", line 214, in _transform\n    raise ValueError(msg)\nValueError: Found unknown categories [338.0] in column 2 during transform\n","Execution time: a moment seconds (time limit is an hour)."],"exec_time":0.5420901775360107,"exc_type":"ValueError","exc_info":{"args":["Found unknown categories [338.0] in column 2 during transform"]},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",21,"<module>","test[categorical_cols] = encoder.transform(test[categorical_cols])"],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/utils/_set_output.py",313,"wrapped","data_to_wrap = f(self, X, *args, **kwargs)"],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py",1578,"transform","X_int, X_mask = self._transform("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py",214,"_transform","raise ValueError(msg)"]],"analysis":"The code fails during the transformation of the test set categorical features because the test set contains unseen categories (e.g., 338.0 in 'POBP') that were not present in the training set. This causes the OrdinalEncoder to raise a ValueError. To fix this, the encoding approach should handle unknown categories, for example by using an encoder that supports unseen categories or by ensuring the test set categories are consistent with the train set.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\ntarget = \"PINCP\"\nfeatures = [c for c in train.columns if c != target]\n\n# Identify categorical features (including moderate-cardinality ones)\ncat_feats = [\"COW\", \"MAR\", \"OCCP\", \"POBP\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Combine for consistent encoding\nfor col in cat_feats:\n    le = LabelEncoder()\n    le.fit(pd.concat([train[col], test[col]], axis=0).astype(str))\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\nX = train[features]\ny = train[target].astype(int)\nX_test = test[features]\n\n# 5-fold stratified CV\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data and predict test\nfinal_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\nfinal_model.fit(X, y)\ntest_preds = final_model.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We will train a LightGBM classifier to predict PINCP, encoding categorical variables via label encoding and treating AGEP and WKHP as numeric. We will perform 5-fold stratified cross-validation to estimate accuracy and print the mean CV accuracy. After CV, we will retrain on the full training data and predict on the test set. Finally, we will save the test predictions to `./working/submission.csv`.","step":1,"id":"8cec8d16c5b74cff966af328c29ab383","ctime":1764595041.929342,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003733 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 685","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002619 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 683","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003022 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 683","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002724 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 688","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003189 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 678","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","CV Accuracy: 0.8224","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: 4 seconds seconds (time limit is an hour)."],"exec_time":4.381706237792969,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script executed successfully without any errors. The LightGBM model was trained using 5-fold stratified cross-validation, achieving a consistent validation accuracy of approximately 0.8224. The final model was then retrained on the full training data and used to generate predictions for the test set. Overall, the model demonstrated good predictive performance on this classification task.","metric":{"value":0.8224,"maximize":true},"is_buggy":false},{"code":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Split features and target\nX = train.drop(columns=[\"PINCP\"])\ny = train[\"PINCP\"]\n\n# Identify feature types\ncategorical_features = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nnumeric_features = [\"AGEP\", \"POBP\", \"OCCP\", \"RELP\", \"SCHL\", \"WKHP\"]\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), numeric_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\n\npipeline = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", LogisticRegression(max_iter=1000, random_state=42)),\n    ]\n)\n\n# Hold-out split\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Train and evaluate\npipeline.fit(X_train, y_train)\ny_pred_val = pipeline.predict(X_val)\nval_acc = accuracy_score(y_val, y_pred_val)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n\n# Retrain on full data\npipeline.fit(X, y)\ny_test_pred = pipeline.predict(test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We build a simple scikit\u2010learn pipeline that one\u2010hot encodes categorical features (COW, MAR, RAC1P, SEX) and standardizes numeric ones (AGEP, POBP, OCCP, RELP, SCHL, WKHP), then fits a logistic regression. We split the training data into a stratified train/validation set to measure hold\u2010out accuracy. After printing the validation accuracy, we retrain on the full training set and predict the test set, saving the results to `./working/submission.csv`.","step":2,"id":"89693979ecb948198d72829128f4d8a5","ctime":1764595071.6332078,"parent":null,"children":[],"_term_out":["Validation Accuracy: 0.7872","\n","Execution time: a second seconds (time limit is an hour)."],"exec_time":1.4745771884918213,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The code executed successfully without any errors. The logistic regression model achieved a validation accuracy of approximately 0.7872, indicating a reasonably good performance on the hold-out validation set. The preprocessing pipeline with standard scaling for numeric features and one-hot encoding for categorical features was effective for this task.","metric":{"value":0.7872,"maximize":true},"is_buggy":false},{"code":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Prepare features and target\ntrain[\"PINCP\"] = train[\"PINCP\"].astype(int)\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"]\n\n# Define feature groups\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nnumerical = [\"AGEP\", \"POBP\", \"OCCP\", \"RELP\", \"SCHL\", \"WKHP\"]\n\n# Split into train/validation\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Build preprocessing + model pipeline\npreprocessor = ColumnTransformer(\n    [\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n        (\"num\", StandardScaler(), numerical),\n    ]\n)\npipeline = Pipeline(\n    [\n        (\"pre\", preprocessor),\n        (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n    ]\n)\n\n# Train and evaluate\npipeline.fit(X_train, y_train)\ny_pred_val = pipeline.predict(X_val)\nacc = accuracy_score(y_val, y_pred_val)\nprint(f\"Validation Accuracy: {acc:.4f}\")\n\n# Retrain on full data\npipeline.fit(X, y)\n\n# Prepare test features and predict\nX_test = test.copy()\nif \"PINCP\" in X_test.columns:\n    X_test = X_test.drop(\"PINCP\", axis=1)\ny_test_pred = pipeline.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": y_test_pred.astype(bool)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We\u2019ll one\u2010hot encode the categorical features (COW, MAR, RAC1P, SEX) and standard\u2010scale the numerical ones (AGEP, POBP, OCCP, RELP, SCHL, WKHP), then train a RandomForestClassifier. We\u2019ll split the training data 80/20 stratified by PINCP to compute validation accuracy. After printing the validation accuracy, we retrain on the full training set and predict on the test set. Finally, we save the test predictions in ./working/submission.csv.","step":3,"id":"9fc730c4475d4483ac8c8065a75399e6","ctime":1764595101.0112314,"parent":null,"children":[],"_term_out":["Validation Accuracy: 0.8139","\n","Execution time: 3 seconds seconds (time limit is an hour)."],"exec_time":3.184990406036377,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script executed successfully without any errors. The model achieved a validation accuracy of approximately 81.39%, indicating good predictive performance on the validation set. The pipeline used a combination of one-hot encoding for categorical features and standard scaling for numerical features, followed by a RandomForestClassifier, which appears to be effective for this task.","metric":{"value":0.8139,"maximize":true},"is_buggy":false},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# One-hot encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nX = pd.get_dummies(X, columns=categorical, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=categorical, drop_first=True)\n\n# Align columns\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n# 5-fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = xgb.XGBClassifier(\n        use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n    )\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = xgb.XGBClassifier(\n    use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n)\nfinal_model.fit(X, y)\n\n# Predict on test set\ntest_preds = final_model.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We\u2019ll encode categorical features with one\u2010hot encoding and leave numeric features as is, then train an XGBoost classifier with default settings. We\u2019ll evaluate performance using 5\u2010fold stratified cross\u2010validation and report the mean accuracy. After verifying the CV score, we\u2019ll retrain on the full training set and generate predictions on the test set. Finally, we\u2019ll write the test predictions to `./working/submission.csv`.","step":4,"id":"3a0b247c4c9c48e5bc86f3cb7f3ba224","ctime":1764595126.183168,"parent":null,"children":[],"_term_out":["Traceback (most recent call last):\n  File \"runfile.py\", line 5, in <module>\n    import xgboost as xgb\nModuleNotFoundError: No module named 'xgboost'\n","Execution time: a moment seconds (time limit is an hour)."],"exec_time":0.3450136184692383,"exc_type":"ModuleNotFoundError","exc_info":{"args":["No module named 'xgboost'"],"name":"xgboost","msg":"No module named 'xgboost'"},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",5,"<module>","import xgboost as xgb"]],"analysis":"The code execution failed because the 'xgboost' library is not installed in the environment. To fix this, install the 'xgboost' package using pip (e.g., 'pip install xgboost') before running the script. Once installed, the script should run successfully, performing stratified 5-fold cross-validation with XGBoost and generating predictions for the test set.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# One-hot encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nX = pd.get_dummies(X, columns=categorical, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=categorical, drop_first=True)\n\n# Align columns\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n# 5-fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = HistGradientBoostingClassifier(random_state=42)\n    model.fit(X_tr, y_tr)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = HistGradientBoostingClassifier(random_state=42)\nfinal_model.fit(X, y)\n\n# Predict on test set\ntest_preds = final_model.predict(X_test)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We can replace XGBoost with scikit-learn\u2019s built-in HistGradientBoostingClassifier to avoid the missing xgboost dependency. The data loading, one-hot encoding, and StratifiedKFold evaluation remain the same, and we\u2019ll compute and print the mean CV accuracy. Finally we\u2019ll retrain on the full dataset, generate test predictions, and save them to submission.csv in the working directory.","step":5,"id":"e7081f52aa044341a204f2ea366ef5a2","ctime":1764595142.4784331,"parent":null,"children":[],"_term_out":["Mean CV Accuracy: 0.8242","\n","Execution time: 8 seconds seconds (time limit is an hour)."],"exec_time":8.306081771850586,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training and evaluation script ran successfully without any errors. The model achieved a mean cross-validation accuracy of 0.8242, which indicates good predictive performance on the PINCP classification task. The final model was retrained on the full training data and predictions were generated for the test set as expected.","metric":{"value":0.8242,"maximize":true},"is_buggy":false},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\n\n# Define categorical columns (exclude high-cardinality POBP)\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Split into train/validation\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Fit encoder on training only\nencoder = OrdinalEncoder()\nX_train_enc = X_train.copy()\nX_val_enc = X_val.copy()\ntest_enc = test.copy()\n\nX_train_enc[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\nX_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\ntest_enc[categorical_cols] = encoder.transform(test[categorical_cols])\n\n# Train model on training split\nclf = lgb.LGBMClassifier(random_state=42)\nclf.fit(X_train_enc, y_train)\n\n# Evaluate on validation split\nval_preds = clf.predict(X_val_enc)\nval_acc = accuracy_score(y_val, val_preds)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n\n# Retrain on full data and predict test set\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder.fit_transform(X[categorical_cols])\nclf.fit(X_full_enc, y)\n\ntest_preds = clf.predict(test_enc)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We fix the issue by treating POBP as a numeric feature rather than categorical, removing it from the list passed to the OrdinalEncoder so we don\u2019t encounter unseen categories at test time. We split out a hold-out validation set to compute and print accuracy, and then retrain on all data to generate our final predictions. Categorical encoding is done only on low-cardinality columns, and continuous columns (including POBP and OCCP) are left as-is.","step":6,"id":"6f541c5696184b178ad0df9125eac35d","ctime":1764595169.1036928,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002940 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Validation Accuracy: 0.8255","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 692","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Traceback (most recent call last):\n  File \"runfile.py\", line 48, in <module>\n    test_preds = clf.predict(test_enc)\n                 ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1597, in predict\n    result = self.predict_proba(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1627, in predict_proba\n    result = super().predict(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1144, in predict\n    return self._Booster.predict(  # type: ignore[union-attr]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 4767, in predict\n    return predictor.predict(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1204, in predict\n    preds, nrow = self.__pred_for_np2d(\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1361, in __pred_for_np2d\n    return self.__inner_predict_np2d(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1307, in __inner_predict_np2d\n    _safe_call(\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: The number of features in data (11) is not the same as it was in training data (10).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n","Execution time: a second seconds (time limit is an hour)."],"exec_time":1.6889116764068604,"exc_type":"LightGBMError","exc_info":{"args":["The number of features in data (11) is not the same as it was in training data (10).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."]},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",48,"<module>","test_preds = clf.predict(test_enc)"],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py",1597,"predict","result = self.predict_proba("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py",1627,"predict_proba","result = super().predict("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py",1144,"predict","return self._Booster.predict(  # type: ignore[union-attr]"],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",4767,"predict","return predictor.predict("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",1204,"predict","preds, nrow = self.__pred_for_np2d("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",1361,"__pred_for_np2d","return self.__inner_predict_np2d("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",1307,"__inner_predict_np2d","_safe_call("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",313,"_safe_call","raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))"]],"analysis":"The code runs successfully until the final prediction on the test set, where it raises a LightGBM error due to a mismatch in the number of features between the training data (10 features) and the test data (11 features). This is likely caused by inconsistent preprocessing or an extra feature column in the test set that was not present or removed in training data. The model training and validation steps show a validation accuracy of about 0.8255, indicating decent performance before the error occurs. To fix, ensure the test data has the exact same feature columns as the training data after encoding, possibly by aligning columns explicitly after encoding or dropping extra columns in the test set.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# Label\u2010encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nfor col in categorical:\n    cats = X[col].unique().tolist()\n    X[col] = pd.Categorical(X[col], categories=cats).codes\n    X_test[col] = pd.Categorical(X_test[col], categories=cats).codes\n\n# 5\u2010fold stratified CV with LightGBM\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"multiclass\", random_state=42, learning_rate=0.05, n_estimators=300\n    )\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = LGBMClassifier(\n    objective=\"multiclass\", random_state=42, learning_rate=0.05, n_estimators=300\n)\nfinal_model.fit(X, y, categorical_feature=categorical, verbose=False)\n\n# Predict on test set and save submission\ntest_preds = final_model.predict(X_test).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"I suggest replacing the HistGradientBoosting model with LightGBM using native categorical support. We\u2019ll label\u2010encode the four categorical variables, pass their names as `categorical_feature` to `LGBMClassifier`, and increase the number of trees with a smaller learning rate. This should capture categorical patterns more effectively and boost accuracy. We retain the 5\u2010fold stratified CV to measure performance, then retrain on the full data and write out the submission.","step":7,"id":"78426b50d24c425f97519380cabc4a76","ctime":1764595208.2222693,"parent":null,"children":[],"_term_out":["Traceback (most recent call last):\n  File \"runfile.py\", line 32, in <module>\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'\n","Execution time: a moment seconds (time limit is an hour)."],"exec_time":0.4779040813446045,"exc_type":"TypeError","exc_info":{"args":["LGBMClassifier.fit() got an unexpected keyword argument 'verbose'"]},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",32,"<module>","model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)"]],"analysis":"The code raised a TypeError because LGBMClassifier.fit() does not accept a 'verbose' argument. Removing 'verbose=False' from the fit() calls will fix this issue.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate target\ny = train[\"PINCP\"].astype(int)\nX = train.drop(columns=[\"PINCP\"])\nX_test = test.copy()\n\n# Label\u2010encode categorical features\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nfor col in categorical:\n    # Use the union of categories from train and test to avoid unseen codes\n    cats = pd.Series(X[col].tolist() + X_test[col].tolist()).unique().tolist()\n    X[col] = pd.Categorical(X[col], categories=cats).codes\n    X_test[col] = pd.Categorical(X_test[col], categories=cats).codes\n\n# 5\u2010fold stratified CV with LightGBM\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n    )\n    # Remove unsupported verbose= from fit\n    model.fit(X_tr, y_tr, categorical_feature=categorical)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data\nfinal_model = LGBMClassifier(\n    objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n)\nfinal_model.fit(X, y, categorical_feature=categorical)\n\n# Predict on test set and save submission\ntest_preds = final_model.predict(X_test).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"I will remove the unsupported `verbose` argument from the `.fit()` calls and, if desired, set verbosity in the `LGBMClassifier` constructor instead. I'll keep the rest of the pipeline the same, use 5\u2010fold stratified CV to compute and print mean accuracy, then retrain on all data and generate `submission.csv`.","step":8,"id":"791b861639df40b582822e59c23d2a77","ctime":1764595225.0186236,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004654 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003239 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002496 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 692","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002252 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 688","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Mean CV Accuracy: 0.8260","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Traceback (most recent call last):\n  File \"runfile.py\", line 47, in <module>\n    test_preds = final_model.predict(X_test).astype(int)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1597, in predict\n    result = self.predict_proba(\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1627, in predict_proba\n    result = super().predict(\n             ^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py\", line 1144, in predict\n    return self._Booster.predict(  # type: ignore[union-attr]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 4767, in predict\n    return predictor.predict(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1204, in predict\n    preds, nrow = self.__pred_for_np2d(\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1361, in __pred_for_np2d\n    return self.__inner_predict_np2d(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 1307, in __inner_predict_np2d\n    _safe_call(\n  File \"/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py\", line 313, in _safe_call\n    raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))\nlightgbm.basic.LightGBMError: The number of features in data (11) is not the same as it was in training data (10).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n","Execution time: 9 seconds seconds (time limit is an hour)."],"exec_time":9.176233291625977,"exc_type":"LightGBMError","exc_info":{"args":["The number of features in data (11) is not the same as it was in training data (10).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."]},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",47,"<module>","test_preds = final_model.predict(X_test).astype(int)"],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py",1597,"predict","result = self.predict_proba("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py",1627,"predict_proba","result = super().predict("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/sklearn.py",1144,"predict","return self._Booster.predict(  # type: ignore[union-attr]"],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",4767,"predict","return predictor.predict("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",1204,"predict","preds, nrow = self.__pred_for_np2d("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",1361,"__pred_for_np2d","return self.__inner_predict_np2d("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",1307,"__inner_predict_np2d","_safe_call("],["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/lightgbm/basic.py",313,"_safe_call","raise LightGBMError(_LIB.LGBM_GetLastError().decode(\"utf-8\"))"]],"analysis":"The training and validation phases ran correctly with a mean cross-validation accuracy of about 0.8260, indicating a reasonably good model performance. However, the code failed during the test prediction step due to a feature mismatch error: the test data has 11 features while the model was trained with 10 features. This discrepancy caused a LightGBMError and prevented the generation of the final submission file. To fix this, ensure that the test data has the exact same columns as the training data before prediction, possibly by aligning columns or dropping/adding missing features.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Define categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Split into train/validation\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Encode categorical features\nencoder = OrdinalEncoder()\nX_train_enc = X_train.copy()\nX_val_enc = X_val.copy()\nX_test_enc = X_test.copy()\n\nX_train_enc[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\nX_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\nX_test_enc[categorical_cols] = encoder.transform(X_test[categorical_cols])\n\n# Train and evaluate\nclf = lgb.LGBMClassifier(random_state=42)\nclf.fit(X_train_enc, y_train)\nval_preds = clf.predict(X_val_enc)\nval_acc = accuracy_score(y_val, val_preds)\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\n\n# Retrain on full data with fresh encoder\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\n\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nclf.fit(X_full_enc, y)\ntest_preds = clf.predict(X_test_enc)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds.astype(int)})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We need to drop the PINCP column from the test set so that our test features match the training features in number and order. We define X_test by removing PINCP, fit an OrdinalEncoder on the training data only, and apply it to validation and test sets consistently. After printing the validation accuracy, we refit the encoder on the full training data, retrain the model, transform the test features, and save the predictions in submission.csv.","step":9,"id":"8bc8438b48024e5fb0700cfc0799a96a","ctime":1764595261.1886027,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003424 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Validation Accuracy: 0.8255","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003586 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 692","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: a second seconds (time limit is an hour)."],"exec_time":1.715073585510254,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script executed successfully without errors. The model achieved a validation accuracy of approximately 0.8255, indicating good predictive performance on the validation set. The final model was retrained on the full training data and predictions were generated for the test set, with the submission file saved correctly.","metric":{"value":0.8255,"maximize":true},"is_buggy":false},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Define feature columns as all except the target\nfeatures = [col for col in train.columns if col != \"PINCP\"]\nX = train[features]\ny = train[\"PINCP\"].astype(int)\n# Select exactly those features in test (drops any extra columns)\nX_test = test[features]\n\n# Label\u2010encode categorical features using union of train+test categories\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\"]\nfor col in categorical:\n    cats = (\n        pd.Series(pd.concat([X[col], X_test[col]], ignore_index=True)).unique().tolist()\n    )\n    X[col] = pd.Categorical(X[col], categories=cats).codes\n    X_test[col] = pd.Categorical(X_test[col], categories=cats).codes\n\n# 5\u2010fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor train_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n    )\n    model.fit(X_tr, y_tr, categorical_feature=categorical)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data and predict on test\nfinal_model = LGBMClassifier(\n    objective=\"binary\", random_state=42, learning_rate=0.05, n_estimators=300\n)\nfinal_model.fit(X, y, categorical_feature=categorical)\ntest_preds = final_model.predict(X_test).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We can fix the feature\u2010mismatch by explicitly defining our feature list (all columns except PINCP) and then selecting exactly those columns from test. This drops any stray or extra columns in test (e.g. a residual PINCP). We then label\u2010encode using the union of categories from train and test on those features, run 5\u2010fold stratified CV, print mean accuracy, retrain on the full set, and save the submission.","step":10,"id":"3a2ed6fce57440b8ac0af49b12770c1f","ctime":1764595284.6195958,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002191 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003430 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 692","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002825 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 688","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Mean CV Accuracy: 0.8260","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002674 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: 9 seconds seconds (time limit is an hour)."],"exec_time":9.631561040878296,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The code attempts to perform binary classification by setting the LightGBM objective to 'binary'. However, the target variable 'PINCP' likely has multiple classes (not binary), so using 'binary' objective is incorrect. This causes LightGBM to treat the problem as binary classification, which is not suitable for a multiclass target. The fix is to change the LightGBM objective to 'multiclass' and specify the number of classes with 'num_class'. This will allow proper multiclass classification and likely improve accuracy.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Define categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\nbest_iters = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n        drop=True\n    )\n    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n        drop=True\n    )\n\n    # Encode categoricals\n    encoder = OrdinalEncoder()\n    X_tr_enc = X_tr.copy()\n    X_val_enc = X_val.copy()\n    X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n    X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n    # Train with early stopping\n    clf = lgb.LGBMClassifier(n_estimators=10000, random_state=42)\n    clf.fit(\n        X_tr_enc,\n        y_tr,\n        eval_set=[(X_val_enc, y_val)],\n        early_stopping_rounds=50,\n        categorical_feature=categorical_cols,\n        verbose=False,\n    )\n\n    # Predict and evaluate\n    preds = clf.predict(X_val_enc)\n    acc = accuracy_score(y_val, preds)\n    fold_accuracies.append(acc)\n    best_iters.append(clf.best_iteration_)\n\n# Compute mean accuracy and best iteration\nmean_acc = np.mean(fold_accuracies)\nmean_iter = int(np.mean(best_iters))\nprint(f\"Validation Accuracy: {mean_acc:.4f}\")\n\n# Retrain on full data\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\n\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(n_estimators=mean_iter, random_state=42)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We switch from a single train/validation split to 5\u2010fold stratified cross\u2010validation with early stopping in LightGBM to get a more robust estimate of accuracy and automatically find the optimal number of trees. In each fold we ordinally encode categoricals, fit LGBMClassifier with a high n_estimators and early_stopping_rounds on the fold\u2019s validation set, record the accuracy and best iteration. We average the accuracies and best iterations across folds, print the mean accuracy, then retrain a final model on the full training data using the average best iteration and predict on the test set. Finally, we save the predictions to submission.csv.","step":11,"id":"7ca43cfad07849bcab07d18202a99f54","ctime":1764595321.3428018,"parent":null,"children":[],"_term_out":["Traceback (most recent call last):\n  File \"runfile.py\", line 43, in <module>\n    clf.fit(\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n","Execution time: a moment seconds (time limit is an hour)."],"exec_time":0.5637011528015137,"exc_type":"TypeError","exc_info":{"args":["LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'"]},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",43,"<module>","clf.fit("]],"analysis":"The code raised a TypeError because the LGBMClassifier.fit() method does not accept the 'early_stopping_rounds' argument. This is likely due to an API usage mistake. To fix this, the early stopping parameters should be passed via the 'callbacks' parameter or use the 'fit' method with the 'eval_set' and 'early_stopping_round' arguments correctly according to the LightGBM version. Alternatively, the native LightGBM training API can be used for early stopping.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Define categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\nbest_iters = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n        drop=True\n    )\n    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n        drop=True\n    )\n\n    # Encode categoricals\n    encoder = OrdinalEncoder()\n    X_tr_enc = X_tr.copy()\n    X_val_enc = X_val.copy()\n    X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n    X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n    # Train with early stopping via callbacks\n    clf = lgb.LGBMClassifier(n_estimators=10000, random_state=42)\n    clf.fit(\n        X_tr_enc,\n        y_tr,\n        eval_set=[(X_val_enc, y_val)],\n        eval_metric=\"binary_error\",\n        categorical_feature=categorical_cols,\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0),\n        ],\n    )\n\n    # Predict and evaluate\n    preds = clf.predict(X_val_enc)\n    acc = accuracy_score(y_val, preds)\n    fold_accuracies.append(acc)\n    best_iters.append(clf.best_iteration_)\n\n# Compute mean accuracy and best iteration\nmean_acc = np.mean(fold_accuracies)\nmean_iter = int(np.mean(best_iters))\nprint(f\"Validation Accuracy: {mean_acc:.4f}\")\n\n# Retrain on full data\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\n\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(n_estimators=mean_iter, random_state=42)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"I replaced the unsupported `early_stopping_rounds` argument with the LightGBM callbacks API, using `lgb.early_stopping` for early stopping and `lgb.log_evaluation` to suppress logging. This ensures `best_iteration_` is properly set without error. All other steps (ordinal encoding, 5-fold Stratified CV, accuracy calculation, retraining, and submission generation) remain the same. The script prints the mean validation accuracy and writes out `submission.csv`.","step":12,"id":"80976f1e8ee046adb459f03244db4022","ctime":1764595340.9390233,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003630 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[179]\tvalid_0's binary_error: 0.174498\tvalid_0's binary_logloss: 0.380948","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002589 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[315]\tvalid_0's binary_error: 0.170473\tvalid_0's binary_logloss: 0.375164","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002653 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[256]\tvalid_0's binary_error: 0.177761\tvalid_0's binary_logloss: 0.381979","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004189 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[430]\tvalid_0's binary_error: 0.168402\tvalid_0's binary_logloss: 0.374522","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002828 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.174056\tvalid_0's binary_logloss: 0.38163","\n","Validation Accuracy: 0.8270","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002550 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: 10 seconds seconds (time limit is an hour)."],"exec_time":10.304556369781494,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The LightGBM model was trained using 5-fold stratified cross-validation with early stopping. The validation accuracy across folds averaged about 82.7%, indicating solid predictive performance for this binary classification task. The best iteration counts varied per fold, and the final model was retrained on the full dataset using the average best iteration. The code executed successfully without errors or bugs.","metric":{"value":0.827,"maximize":true},"is_buggy":false},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n\n# Mean target encoding function with smoothing\ndef mean_target_encoding(train_series, target_series, valid_series, k=5):\n    \"\"\"\n    Compute smoothed mean target encoding.\n    train_series: pd.Series of categorical feature in training split\n    target_series: pd.Series of target in training split\n    valid_series: pd.Series of categorical feature in validation/test split\n    k: smoothing parameter\n    Returns encoded_train, encoded_valid\n    \"\"\"\n    # Compute global mean\n    global_mean = target_series.mean()\n    # Compute counts and sums per category\n    stats = target_series.groupby(train_series).agg([\"count\", \"mean\"])\n    counts = stats[\"count\"]\n    means = stats[\"mean\"]\n    # Smoothed means\n    smooth = (counts * means + k * global_mean) / (counts + k)\n    # Map to series\n    train_encoded = train_series.map(smooth).fillna(global_mean)\n    valid_encoded = valid_series.map(smooth).fillna(global_mean)\n    return train_encoded, valid_encoded\n\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns to target-encode\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfold_accuracies = []\nbest_iters = []\n\nfor train_idx, val_idx in skf.split(X, y):\n    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n        drop=True\n    )\n    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n        drop=True\n    )\n\n    # Initialize encoded copies\n    X_tr_enc = X_tr.copy()\n    X_val_enc = X_val.copy()\n\n    # Apply mean target encoding per categorical column\n    for col in categorical_cols:\n        tr_enc, val_enc = mean_target_encoding(X_tr[col], y_tr, X_val[col], k=5)\n        X_tr_enc[col] = tr_enc\n        X_val_enc[col] = val_enc\n\n    # Train with early stopping\n    clf = lgb.LGBMClassifier(n_estimators=10000, learning_rate=0.05, random_state=42)\n    clf.fit(\n        X_tr_enc,\n        y_tr,\n        eval_set=[(X_val_enc, y_val)],\n        eval_metric=\"binary_error\",\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0),\n        ],\n    )\n\n    # Predict and evaluate\n    preds = clf.predict(X_val_enc)\n    acc = accuracy_score(y_val, preds)\n    fold_accuracies.append(acc)\n    best_iters.append(clf.best_iteration_)\n\n# Compute mean accuracy and best iteration\nmean_acc = np.mean(fold_accuracies)\nmean_iter = int(np.mean(best_iters))\nprint(f\"Validation Accuracy: {mean_acc:.4f}\")\n\n# Retrain on full data\nX_full_enc = X.copy()\nfor col in categorical_cols:\n    tr_enc_full, _ = mean_target_encoding(X[col], y, X[col], k=5)\n    X_full_enc[col] = tr_enc_full\n\nX_test_enc = X_test.copy()\nfor col in categorical_cols:\n    # Use full-data encoding to map test\n    tr_enc_full, test_enc = mean_target_encoding(X[col], y, X_test[col], k=5)\n    X_test_enc[col] = test_enc\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=mean_iter, learning_rate=0.05, random_state=42\n)\nfinal_clf.fit(X_full_enc, y)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We can replace the ordinal encoding of categorical features with smoothed target mean encoding to provide continuous, informative predictors. For each fold, we compute the smoothed mean target per category on the training split and map these to both training and validation sets, filling unseen categories with the global mean. This should help LightGBM better capture the relationship between categories and the target. We will then perform 5-fold CV with early stopping, retrain on full data, and generate test predictions as before.","step":13,"id":"620f065ab4344e2f91930d9dda2fe531","ctime":1764595405.9701524,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[375]\tvalid_0's binary_error: 0.173955\tvalid_0's binary_logloss: 0.380173","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[433]\tvalid_0's binary_error: 0.170601\tvalid_0's binary_logloss: 0.375409","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[358]\tvalid_0's binary_error: 0.17741\tvalid_0's binary_logloss: 0.382898","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[526]\tvalid_0's binary_error: 0.168849\tvalid_0's binary_logloss: 0.374257","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[161]\tvalid_0's binary_error: 0.176516\tvalid_0's binary_logloss: 0.385554","\n","Validation Accuracy: 0.8265","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 698","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: 12 seconds seconds (time limit is an hour)."],"exec_time":12.628101587295532,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script executed successfully without any errors or bugs. The model was trained using 5-fold stratified cross-validation with mean target encoding applied to categorical variables. The validation accuracy achieved was approximately 0.8265, indicating good predictive performance. Early stopping was used effectively to prevent overfitting, with best iterations varying between folds. The final model was retrained on the full dataset using the average best iteration and predictions were generated for the test set.","metric":{"value":0.8265,"maximize":true},"is_buggy":false},{"code":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Define features and target\nfeatures = [c for c in train.columns if c != \"PINCP\"]\nX = train[features].copy()\ny = train[\"PINCP\"].astype(int)\nX_test = test[features].copy()\n\n# Treat these integer\u2010coded columns as categorical\ncategorical = [\"COW\", \"MAR\", \"RAC1P\", \"SEX\", \"OCCP\", \"POBP\", \"RELP\", \"SCHL\"]\n\n# Joint label\u2010encode categories train+test\nfor col in categorical:\n    unified = pd.Categorical(pd.concat([X[col], X_test[col]], ignore_index=True))\n    codes = unified.codes.reshape(-1)\n    X[col] = codes[: len(X)]\n    X_test[col] = codes[len(X) :]\n\n# 5\u2010fold stratified CV\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\nfor tr_idx, val_idx in kf.split(X, y):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    model = LGBMClassifier(\n        objective=\"binary\", learning_rate=0.05, n_estimators=300, random_state=42\n    )\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\n    preds = model.predict(X_val)\n    scores.append(accuracy_score(y_val, preds))\n\nprint(f\"Mean CV Accuracy: {np.mean(scores):.4f}\")\n\n# Retrain on full data and predict\nfinal_model = LGBMClassifier(\n    objective=\"binary\", learning_rate=0.05, n_estimators=300, random_state=42\n)\nfinal_model.fit(X, y, categorical_feature=categorical, verbose=False)\ntest_preds = final_model.predict(X_test).astype(int)\n\n# Save submission\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We need to treat all integer\u2010coded fields (occupation, place of birth, relationship, education) as categoricals for LightGBM.  By adding OCCP, POBP, RELP and SCHL to our categorical list and label\u2010encoding them jointly on train+test (so unseen levels map correctly), we avoid misleading continuous splits.  We then rerun 5\u2010fold stratified CV, print the mean accuracy, retrain on all data, predict on test, and save `submission.csv`.","step":14,"id":"eca61f5570704bf9866ded74ea943677","ctime":1764595454.8056188,"parent":null,"children":[],"_term_out":["Traceback (most recent call last):\n  File \"runfile.py\", line 36, in <module>\n    model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'verbose'\n","Execution time: a moment seconds (time limit is an hour)."],"exec_time":0.4018542766571045,"exc_type":"TypeError","exc_info":{"args":["LGBMClassifier.fit() got an unexpected keyword argument 'verbose'"]},"exc_stack":[["/home/koc13/TUB/RDSPRO/.venv/lib64/python3.11/site-packages/aide/interpreter.py",147,"_run_session","exec(compile(code, self.agent_file_name, \"exec\"), global_scope)"],["runfile.py",36,"<module>","model.fit(X_tr, y_tr, categorical_feature=categorical, verbose=False)"]],"analysis":"The code raises a TypeError because LGBMClassifier.fit() does not accept the 'verbose' argument. Removing the 'verbose=False' argument from the fit() calls will fix this issue.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross-validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accuracies = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                val_idx\n            ].reset_index(drop=True)\n            y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                val_idx\n            ].reset_index(drop=True)\n\n            # Ordinal encoding\n            encoder = OrdinalEncoder()\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n            X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n            # Train model with early stopping\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000, learning_rate=lr, num_leaves=nl, random_state=42\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                categorical_feature=categorical_cols,\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            # Evaluate\n            preds = clf.predict(X_val_enc)\n            acc = accuracy_score(y_val, preds)\n            fold_accuracies.append(acc)\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accuracies)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"Params lr={lr}, num_leaves={nl} -> CV Accuracy = {mean_acc:.4f}, Mean Iter = {mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Print best CV performance\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We will perform a small grid search over LightGBM\u2019s learning_rate and num_leaves using 5-fold stratified CV with early stopping to find the hyperparameters that maximize accuracy. For each parameter combination, we\u2019ll encode categoricals with OrdinalEncoder, train with early stopping, and record mean CV accuracy and mean best iteration. After selecting the best configuration, we retrain on the full dataset using the averaged best iteration count and output test predictions to `submission.csv`. This targeted hyperparameter tuning should yield a modest gain in predictive performance.","step":15,"id":"4dcdc9d4733247b9b6ba0c6d751fa724","ctime":1764595482.563971,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004158 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[413]\tvalid_0's binary_error: 0.173859\tvalid_0's binary_logloss: 0.379836","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003470 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[357]\tvalid_0's binary_error: 0.171176\tvalid_0's binary_logloss: 0.376679","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002618 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[346]\tvalid_0's binary_error: 0.177378\tvalid_0's binary_logloss: 0.383217","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[411]\tvalid_0's binary_error: 0.169712\tvalid_0's binary_logloss: 0.375684","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002423 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[366]\tvalid_0's binary_error: 0.174248\tvalid_0's binary_logloss: 0.381092","\n","Params lr=0.05, num_leaves=31 -> CV Accuracy = 0.8267, Mean Iter = 378","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002404 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[283]\tvalid_0's binary_error: 0.173188\tvalid_0's binary_logloss: 0.379084","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[237]\tvalid_0's binary_error: 0.170537\tvalid_0's binary_logloss: 0.375729","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[258]\tvalid_0's binary_error: 0.176835\tvalid_0's binary_logloss: 0.382011","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002303 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[188]\tvalid_0's binary_error: 0.170191\tvalid_0's binary_logloss: 0.375714","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002770 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[311]\tvalid_0's binary_error: 0.173226\tvalid_0's binary_logloss: 0.379514","\n","Params lr=0.05, num_leaves=63 -> CV Accuracy = 0.8272, Mean Iter = 255","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.173028\tvalid_0's binary_logloss: 0.378887","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003025 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[268]\tvalid_0's binary_error: 0.169866\tvalid_0's binary_logloss: 0.374785","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003094 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[204]\tvalid_0's binary_error: 0.177698\tvalid_0's binary_logloss: 0.381407","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003759 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[298]\tvalid_0's binary_error: 0.16952\tvalid_0's binary_logloss: 0.373513","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002638 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[143]\tvalid_0's binary_error: 0.173705\tvalid_0's binary_logloss: 0.38059","\n","Params lr=0.05, num_leaves=127 -> CV Accuracy = 0.8272, Mean Iter = 217","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002942 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[179]\tvalid_0's binary_error: 0.174498\tvalid_0's binary_logloss: 0.380948","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003633 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[315]\tvalid_0's binary_error: 0.170473\tvalid_0's binary_logloss: 0.375164","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003458 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[256]\tvalid_0's binary_error: 0.177761\tvalid_0's binary_logloss: 0.381979","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002499 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[430]\tvalid_0's binary_error: 0.168402\tvalid_0's binary_logloss: 0.374522","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002803 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.174056\tvalid_0's binary_logloss: 0.38163","\n","Params lr=0.1, num_leaves=31 -> CV Accuracy = 0.8270, Mean Iter = 270","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003567 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[139]\tvalid_0's binary_error: 0.17453\tvalid_0's binary_logloss: 0.379874","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002788 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[122]\tvalid_0's binary_error: 0.170665\tvalid_0's binary_logloss: 0.376078","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002845 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[138]\tvalid_0's binary_error: 0.176484\tvalid_0's binary_logloss: 0.382455","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002542 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[212]\tvalid_0's binary_error: 0.168562\tvalid_0's binary_logloss: 0.374304","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003722 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[177]\tvalid_0's binary_error: 0.173034\tvalid_0's binary_logloss: 0.379611","\n","Params lr=0.1, num_leaves=63 -> CV Accuracy = 0.8273, Mean Iter = 157","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002613 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[129]\tvalid_0's binary_error: 0.172422\tvalid_0's binary_logloss: 0.378547","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003434 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[112]\tvalid_0's binary_error: 0.170856\tvalid_0's binary_logloss: 0.375515","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004562 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[81]\tvalid_0's binary_error: 0.176835\tvalid_0's binary_logloss: 0.382959","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003690 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[119]\tvalid_0's binary_error: 0.167923\tvalid_0's binary_logloss: 0.374062","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003698 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[71]\tvalid_0's binary_error: 0.173609\tvalid_0's binary_logloss: 0.380987","\n","Params lr=0.1, num_leaves=127 -> CV Accuracy = 0.8277, Mean Iter = 102","\n","Best CV Accuracy: 0.8277 with params {'learning_rate': 0.1, 'num_leaves': 127} and n_estimators=102","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003812 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: a minute seconds (time limit is an hour)."],"exec_time":88.89368009567261,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script executed successfully without any errors. The model was trained using stratified 5-fold cross-validation with LightGBM, tuning learning rate and number of leaves. The best cross-validation accuracy achieved was approximately 0.8277 with learning rate 0.1 and 127 leaves, using about 102 boosting iterations on average. The final model was then retrained on the full dataset with the best parameters and used to generate predictions for the test set.","metric":{"value":0.8277,"maximize":true},"is_buggy":false},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross-validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\n# Grid search with subsampling parameters\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accuracies = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                val_idx\n            ].reset_index(drop=True)\n            y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                val_idx\n            ].reset_index(drop=True)\n\n            # Ordinal encoding\n            encoder = OrdinalEncoder()\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n            X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n            # Train model with early stopping and subsampling\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000,\n                learning_rate=lr,\n                num_leaves=nl,\n                feature_fraction=0.8,\n                bagging_fraction=0.8,\n                bagging_freq=5,\n                random_state=42,\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                categorical_feature=categorical_cols,\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            # Evaluate\n            preds = clf.predict(X_val_enc)\n            acc = accuracy_score(y_val, preds)\n            fold_accuracies.append(acc)\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accuracies)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"Params lr={lr}, num_leaves={nl} -> CV Accuracy = {mean_acc:.4f}, Mean Iter = {mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Print best CV performance\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params and subsampling\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    feature_fraction=0.8,\n    bagging_fraction=0.8,\n    bagging_freq=5,\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"I propose adding feature and data subsampling to the LightGBM models to reduce overfitting. Specifically, we will set feature_fraction=0.8, bagging_fraction=0.8, and bagging_freq=5 in both cross\u2010validation and final training while keeping the existing grid search over learning_rate and num_leaves. This small change can improve generalization and boost CV accuracy slightly. We then retrain on the full dataset with the best parameters including these subsampling settings and save the test predictions.","step":16,"id":"744500116d8b46e2862ea99e8a646d82","ctime":1764595602.6998072,"parent":null,"children":[],"_term_out":["[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004282 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[368]\tvalid_0's binary_error: 0.174594\tvalid_0's binary_logloss: 0.379865","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002075 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[239]\tvalid_0's binary_error: 0.171783\tvalid_0's binary_logloss: 0.378787","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[240]\tvalid_0's binary_error: 0.177857\tvalid_0's binary_logloss: 0.384551","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003350 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[348]\tvalid_0's binary_error: 0.170319\tvalid_0's binary_logloss: 0.375208","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003002 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[328]\tvalid_0's binary_error: 0.174088\tvalid_0's binary_logloss: 0.380186","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, num_leaves=31 -> CV Accuracy = 0.8263, Mean Iter = 304","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[237]\tvalid_0's binary_error: 0.174114\tvalid_0's binary_logloss: 0.378771","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003476 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[260]\tvalid_0's binary_error: 0.169898\tvalid_0's binary_logloss: 0.37499","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[254]\tvalid_0's binary_error: 0.176612\tvalid_0's binary_logloss: 0.381474","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003718 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[274]\tvalid_0's binary_error: 0.168051\tvalid_0's binary_logloss: 0.373533","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002580 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[215]\tvalid_0's binary_error: 0.173641\tvalid_0's binary_logloss: 0.379779","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, num_leaves=63 -> CV Accuracy = 0.8275, Mean Iter = 248","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002860 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[205]\tvalid_0's binary_error: 0.17421\tvalid_0's binary_logloss: 0.378155","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002787 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[189]\tvalid_0's binary_error: 0.169451\tvalid_0's binary_logloss: 0.375366","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[156]\tvalid_0's binary_error: 0.177921\tvalid_0's binary_logloss: 0.382268","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002337 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[128]\tvalid_0's binary_error: 0.169009\tvalid_0's binary_logloss: 0.374801","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003292 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[278]\tvalid_0's binary_error: 0.172331\tvalid_0's binary_logloss: 0.377748","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, num_leaves=127 -> CV Accuracy = 0.8274, Mean Iter = 191","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003530 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[197]\tvalid_0's binary_error: 0.174498\tvalid_0's binary_logloss: 0.38056","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002429 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[175]\tvalid_0's binary_error: 0.171463\tvalid_0's binary_logloss: 0.377798","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002982 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[283]\tvalid_0's binary_error: 0.176612\tvalid_0's binary_logloss: 0.382931","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003536 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[295]\tvalid_0's binary_error: 0.168626\tvalid_0's binary_logloss: 0.374054","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004168 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[154]\tvalid_0's binary_error: 0.173609\tvalid_0's binary_logloss: 0.380892","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, num_leaves=31 -> CV Accuracy = 0.8270, Mean Iter = 220","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003458 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[186]\tvalid_0's binary_error: 0.173539\tvalid_0's binary_logloss: 0.379428","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[158]\tvalid_0's binary_error: 0.171431\tvalid_0's binary_logloss: 0.375879","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003357 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[162]\tvalid_0's binary_error: 0.177123\tvalid_0's binary_logloss: 0.382254","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003744 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[200]\tvalid_0's binary_error: 0.169297\tvalid_0's binary_logloss: 0.373754","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003172 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[120]\tvalid_0's binary_error: 0.172874\tvalid_0's binary_logloss: 0.379814","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, num_leaves=63 -> CV Accuracy = 0.8271, Mean Iter = 165","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004318 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[141]\tvalid_0's binary_error: 0.174274\tvalid_0's binary_logloss: 0.379169","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003647 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[55]\tvalid_0's binary_error: 0.17124\tvalid_0's binary_logloss: 0.378479","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003553 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[72]\tvalid_0's binary_error: 0.177602\tvalid_0's binary_logloss: 0.383101","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003683 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[121]\tvalid_0's binary_error: 0.168786\tvalid_0's binary_logloss: 0.373018","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003795 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[130]\tvalid_0's binary_error: 0.173098\tvalid_0's binary_logloss: 0.380262","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, num_leaves=127 -> CV Accuracy = 0.8270, Mean Iter = 103","\n","Best CV Accuracy: 0.8275 with params {'learning_rate': 0.05, 'num_leaves': 63} and n_estimators=248","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005046 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Execution time: a minute seconds (time limit is an hour)."],"exec_time":94.21402716636658,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script successfully completed a grid search over learning rates and number of leaves for a LightGBM classifier using stratified 5-fold cross-validation. The best cross-validation accuracy achieved was approximately 0.8275 with learning_rate=0.05 and num_leaves=63. Early stopping was used to select the optimal number of boosting iterations, which averaged 248 iterations for the best parameters. The script then retrained the model on the full training data with the best parameters and generated predictions on the test set for submission. The warnings about bagging parameters are informational and do not indicate errors.","metric":{"value":0.8275,"maximize":true},"is_buggy":false},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Separate features and target\nX = train.drop(\"PINCP\", axis=1).copy()\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1).copy()\n\n# Create interaction feature between RELP and SEX\nX[\"RELP_SEX\"] = X[\"RELP\"].astype(str) + \"_\" + X[\"SEX\"].astype(str)\nX_test[\"RELP_SEX\"] = X_test[\"RELP\"].astype(str) + \"_\" + X_test[\"SEX\"].astype(str)\n\n# Define categorical columns including the new interaction\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\", \"RELP_SEX\"]\n\n# Prepare cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accs = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr = X.iloc[train_idx].reset_index(drop=True)\n            X_val = X.iloc[val_idx].reset_index(drop=True)\n            y_tr = y.iloc[train_idx].reset_index(drop=True)\n            y_val = y.iloc[val_idx].reset_index(drop=True)\n\n            # Ordinal encoding\n            encoder = OrdinalEncoder()\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            X_tr_enc[categorical_cols] = encoder.fit_transform(X_tr[categorical_cols])\n            X_val_enc[categorical_cols] = encoder.transform(X_val[categorical_cols])\n\n            # Train with early stopping\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000, learning_rate=lr, num_leaves=nl, random_state=42\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                categorical_feature=categorical_cols,\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            preds = clf.predict(X_val_enc)\n            fold_accs.append(accuracy_score(y_val, preds))\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accs)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"lr={lr}, num_leaves={nl} -> CV Accuracy={mean_acc:.4f}, iters={mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Report best CV result\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We create a new categorical feature that encodes the interaction between relationship status (RELP) and sex (SEX), then include it alongside the existing categoricals. We rerun the same grid search over learning_rate and num_leaves with 5-fold stratified CV and early stopping, which now can exploit this interaction. Finally, we retrain the best model on the full training set and save test predictions. This feature interaction may help the model capture subgroup differences and boost accuracy.","step":17,"id":"6902d2088afb4a9ca61068ade21ba5c8","ctime":1764595741.6444907,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003432 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[236]\tvalid_0's binary_error: 0.174945\tvalid_0's binary_logloss: 0.382309","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002796 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 725","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[513]\tvalid_0's binary_error: 0.169579\tvalid_0's binary_logloss: 0.375257","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002137 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[353]\tvalid_0's binary_error: 0.177314\tvalid_0's binary_logloss: 0.38346","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002856 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 727","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[354]\tvalid_0's binary_error: 0.170447\tvalid_0's binary_logloss: 0.376385","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 723","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[263]\tvalid_0's binary_error: 0.174312\tvalid_0's binary_logloss: 0.382483","\n","lr=0.05, num_leaves=31 -> CV Accuracy=0.8267, iters=343","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002896 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[272]\tvalid_0's binary_error: 0.172613\tvalid_0's binary_logloss: 0.378994","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003217 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 725","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[441]\tvalid_0's binary_error: 0.168876\tvalid_0's binary_logloss: 0.37454","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[213]\tvalid_0's binary_error: 0.177953\tvalid_0's binary_logloss: 0.383312","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 727","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[275]\tvalid_0's binary_error: 0.169935\tvalid_0's binary_logloss: 0.374593","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003637 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 723","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[326]\tvalid_0's binary_error: 0.172523\tvalid_0's binary_logloss: 0.379684","\n","lr=0.05, num_leaves=63 -> CV Accuracy=0.8276, iters=305","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[262]\tvalid_0's binary_error: 0.172485\tvalid_0's binary_logloss: 0.378075","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 725","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[208]\tvalid_0's binary_error: 0.169643\tvalid_0's binary_logloss: 0.375083","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002425 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[129]\tvalid_0's binary_error: 0.176867\tvalid_0's binary_logloss: 0.383314","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 727","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[284]\tvalid_0's binary_error: 0.167891\tvalid_0's binary_logloss: 0.37371","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003331 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 723","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[144]\tvalid_0's binary_error: 0.174439\tvalid_0's binary_logloss: 0.380674","\n","lr=0.05, num_leaves=127 -> CV Accuracy=0.8277, iters=205","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004070 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[287]\tvalid_0's binary_error: 0.17338\tvalid_0's binary_logloss: 0.379846","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003965 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 725","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[234]\tvalid_0's binary_error: 0.170058\tvalid_0's binary_logloss: 0.375974","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003307 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[267]\tvalid_0's binary_error: 0.177282\tvalid_0's binary_logloss: 0.383122","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002765 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 727","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[331]\tvalid_0's binary_error: 0.168913\tvalid_0's binary_logloss: 0.375125","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003862 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 723","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[271]\tvalid_0's binary_error: 0.172842\tvalid_0's binary_logloss: 0.379704","\n","lr=0.1, num_leaves=31 -> CV Accuracy=0.8275, iters=278","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003229 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[131]\tvalid_0's binary_error: 0.173316\tvalid_0's binary_logloss: 0.379493","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003286 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 725","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[195]\tvalid_0's binary_error: 0.169323\tvalid_0's binary_logloss: 0.375159","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004366 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[136]\tvalid_0's binary_error: 0.179167\tvalid_0's binary_logloss: 0.383314","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003150 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 727","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[198]\tvalid_0's binary_error: 0.168913\tvalid_0's binary_logloss: 0.37451","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005363 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 723","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[119]\tvalid_0's binary_error: 0.174535\tvalid_0's binary_logloss: 0.38121","\n","lr=0.1, num_leaves=63 -> CV Accuracy=0.8269, iters=155","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004946 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[113]\tvalid_0's binary_error: 0.173444\tvalid_0's binary_logloss: 0.378386","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003620 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 725","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[154]\tvalid_0's binary_error: 0.171016\tvalid_0's binary_logloss: 0.375737","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002756 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 726","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[107]\tvalid_0's binary_error: 0.179263\tvalid_0's binary_logloss: 0.383995","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 727","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[155]\tvalid_0's binary_error: 0.168402\tvalid_0's binary_logloss: 0.374341","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003335 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 723","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[78]\tvalid_0's binary_error: 0.175046\tvalid_0's binary_logloss: 0.381293","\n","lr=0.1, num_leaves=127 -> CV Accuracy=0.8266, iters=121","\n","Best CV Accuracy: 0.8277 with params {'learning_rate': 0.05, 'num_leaves': 127} and n_estimators=205","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004096 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 730","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 11","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: a minute seconds (time limit is an hour)."],"exec_time":97.49000072479248,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The code attempts to predict 'PINCP' using LightGBM with hyperparameter tuning and cross-validation. However, the execution output shows LightGBM binary classification logs, which is inconsistent with the task goal of predicting 'PINCP', which is a multi-class classification problem (PINCP is a personal income code, not binary). Also, the evaluation metric used is 'binary_error' which is for binary classification, not suitable for multi-class. This indicates a bug in the model configuration and evaluation metric. The code should use a multi-class objective and appropriate metric like 'multi_logloss' or 'multi_error' for accuracy. The output does not show final test predictions or submission file content, and the logs are truncated, so it is unclear if the code completed successfully. Overall, the code ran but with a fundamental bug in classification setup, making the results invalid.","metric":{"value":null,"maximize":null},"is_buggy":true},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns to frequency\u2010encode\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross\u2010validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        fold_accuracies = []\n        fold_iters = []\n        for train_idx, val_idx in skf.split(X, y):\n            X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                val_idx\n            ].reset_index(drop=True)\n            y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                val_idx\n            ].reset_index(drop=True)\n\n            # Frequency encoding on training fold\n            X_tr_enc = X_tr.copy()\n            X_val_enc = X_val.copy()\n            for col in categorical_cols:\n                freq = X_tr_enc[col].value_counts(normalize=True)\n                X_tr_enc[col] = X_tr_enc[col].map(freq)\n                X_val_enc[col] = X_val_enc[col].map(freq).fillna(0.0)\n\n            # Train model with early stopping\n            clf = lgb.LGBMClassifier(\n                n_estimators=10000, learning_rate=lr, num_leaves=nl, random_state=42\n            )\n            clf.fit(\n                X_tr_enc,\n                y_tr,\n                eval_set=[(X_val_enc, y_val)],\n                eval_metric=\"binary_error\",\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=50),\n                    lgb.log_evaluation(period=0),\n                ],\n            )\n\n            # Evaluate\n            preds = clf.predict(X_val_enc)\n            acc = accuracy_score(y_val, preds)\n            fold_accuracies.append(acc)\n            fold_iters.append(clf.best_iteration_)\n\n        mean_acc = np.mean(fold_accuracies)\n        mean_iter = int(np.mean(fold_iters))\n        print(\n            f\"Params lr={lr}, num_leaves={nl} -> CV Accuracy = {mean_acc:.4f}, Mean Iter = {mean_iter}\"\n        )\n        if mean_acc > best_acc:\n            best_acc = mean_acc\n            best_params = {\"learning_rate\": lr, \"num_leaves\": nl}\n            best_iter = mean_iter\n\n# Print best CV performance\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params using frequency encoding\nX_full_enc = X.copy()\nX_test_enc = X_test.copy()\nfor col in categorical_cols:\n    freq_full = X_full_enc[col].value_counts(normalize=True)\n    X_full_enc[col] = X_full_enc[col].map(freq_full)\n    X_test_enc[col] = X_test_enc[col].map(freq_full).fillna(0.0)\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"I will replace the ordinal encoding of categorical variables with frequency encoding, mapping each category to its prevalence in the training fold.  This gives the model continuous signals about category rarity and may improve generalization.  The rest of the grid search, early stopping, and retraining pipeline remains unchanged so that we can directly measure the impact of this single change.","step":18,"id":"04007ff6bce440b6bb52cb933a90ed0e","ctime":1764595875.6793694,"parent":null,"children":[],"_term_out":["[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[427]\tvalid_0's binary_error: 0.175839\tvalid_0's binary_logloss: 0.381358","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[409]\tvalid_0's binary_error: 0.171623\tvalid_0's binary_logloss: 0.377669","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000678 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[310]\tvalid_0's binary_error: 0.178177\tvalid_0's binary_logloss: 0.385938","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[240]\tvalid_0's binary_error: 0.171309\tvalid_0's binary_logloss: 0.379766","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[371]\tvalid_0's binary_error: 0.174312\tvalid_0's binary_logloss: 0.382073","\n","Params lr=0.05, num_leaves=31 -> CV Accuracy = 0.8257, Mean Iter = 351","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[264]\tvalid_0's binary_error: 0.174913\tvalid_0's binary_logloss: 0.381246","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000809 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[209]\tvalid_0's binary_error: 0.171272\tvalid_0's binary_logloss: 0.378631","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[346]\tvalid_0's binary_error: 0.177442\tvalid_0's binary_logloss: 0.383976","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000877 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[407]\tvalid_0's binary_error: 0.16837\tvalid_0's binary_logloss: 0.374734","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[264]\tvalid_0's binary_error: 0.174567\tvalid_0's binary_logloss: 0.381648","\n","Params lr=0.05, num_leaves=63 -> CV Accuracy = 0.8267, Mean Iter = 298","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000740 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[309]\tvalid_0's binary_error: 0.174082\tvalid_0's binary_logloss: 0.37963","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[137]\tvalid_0's binary_error: 0.170537\tvalid_0's binary_logloss: 0.37856","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000803 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[212]\tvalid_0's binary_error: 0.178496\tvalid_0's binary_logloss: 0.384356","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[276]\tvalid_0's binary_error: 0.168977\tvalid_0's binary_logloss: 0.37481","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000858 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[220]\tvalid_0's binary_error: 0.17412\tvalid_0's binary_logloss: 0.380266","\n","Params lr=0.05, num_leaves=127 -> CV Accuracy = 0.8268, Mean Iter = 230","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000894 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[253]\tvalid_0's binary_error: 0.176127\tvalid_0's binary_logloss: 0.381317","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001483 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[299]\tvalid_0's binary_error: 0.170665\tvalid_0's binary_logloss: 0.376432","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[186]\tvalid_0's binary_error: 0.177155\tvalid_0's binary_logloss: 0.385006","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000779 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.169616\tvalid_0's binary_logloss: 0.37717","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[255]\tvalid_0's binary_error: 0.174695\tvalid_0's binary_logloss: 0.380968","\n","Params lr=0.1, num_leaves=31 -> CV Accuracy = 0.8263, Mean Iter = 233","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001206 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[270]\tvalid_0's binary_error: 0.173284\tvalid_0's binary_logloss: 0.380022","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[183]\tvalid_0's binary_error: 0.169611\tvalid_0's binary_logloss: 0.376039","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[152]\tvalid_0's binary_error: 0.177282\tvalid_0's binary_logloss: 0.384098","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[205]\tvalid_0's binary_error: 0.168658\tvalid_0's binary_logloss: 0.375401","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[208]\tvalid_0's binary_error: 0.173545\tvalid_0's binary_logloss: 0.381788","\n","Params lr=0.1, num_leaves=63 -> CV Accuracy = 0.8275, Mean Iter = 203","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[181]\tvalid_0's binary_error: 0.174402\tvalid_0's binary_logloss: 0.37945","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000866 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 693","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[60]\tvalid_0's binary_error: 0.16993\tvalid_0's binary_logloss: 0.379066","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[139]\tvalid_0's binary_error: 0.177602\tvalid_0's binary_logloss: 0.38417","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000746 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 695","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[123]\tvalid_0's binary_error: 0.168977\tvalid_0's binary_logloss: 0.374361","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000854 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[105]\tvalid_0's binary_error: 0.174376\tvalid_0's binary_logloss: 0.381313","\n","Params lr=0.1, num_leaves=127 -> CV Accuracy = 0.8269, Mean Iter = 121","\n","Best CV Accuracy: 0.8275 with params {'learning_rate': 0.1, 'num_leaves': 63} and n_estimators=203","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000860 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 698","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","Execution time: a minute seconds (time limit is an hour)."],"exec_time":89.3725233078003,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script successfully ran without any errors or bugs. It performed a grid search over learning rates and number of leaves for a LightGBM classifier using stratified 5-fold cross-validation. Frequency encoding was applied to categorical variables. The best cross-validation accuracy achieved was approximately 0.8275 with learning rate 0.1 and num_leaves 63. The model was then retrained on the full dataset with the best parameters and early stopping iterations, and predictions were generated for the test set. Overall, the approach and results are reasonable and consistent with the task goal of predicting PINCP with accuracy evaluation.","metric":{"value":0.8275,"maximize":true},"is_buggy":false},{"code":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\n\n# Load data\ntrain = pd.read_csv(\"./input/train.csv\")\ntest = pd.read_csv(\"./input/test.csv\")\n\n# Features and target\nX = train.drop(\"PINCP\", axis=1)\ny = train[\"PINCP\"].astype(int)\nX_test = test.drop(\"PINCP\", axis=1)\n\n# Categorical columns\ncategorical_cols = [\"COW\", \"MAR\", \"RAC1P\", \"RELP\", \"SCHL\", \"SEX\"]\n\n# Cross-validation setup\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Hyperparameter grid\nlearning_rates = [0.05, 0.1]\nnum_leaves_list = [31, 63, 127]\nfeature_fractions = [0.8, 1.0]\nbagging_fractions = [0.8, 1.0]\n\nbest_acc = 0.0\nbest_params = {}\nbest_iter = 0\n\nfor lr in learning_rates:\n    for nl in num_leaves_list:\n        for ff in feature_fractions:\n            for bf in bagging_fractions:\n                fold_accuracies = []\n                fold_iters = []\n                for train_idx, val_idx in skf.split(X, y):\n                    X_tr, X_val = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n                        val_idx\n                    ].reset_index(drop=True)\n                    y_tr, y_val = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n                        val_idx\n                    ].reset_index(drop=True)\n\n                    # Ordinal encoding\n                    encoder = OrdinalEncoder()\n                    X_tr_enc = X_tr.copy()\n                    X_val_enc = X_val.copy()\n                    X_tr_enc[categorical_cols] = encoder.fit_transform(\n                        X_tr[categorical_cols]\n                    )\n                    X_val_enc[categorical_cols] = encoder.transform(\n                        X_val[categorical_cols]\n                    )\n\n                    # Train model with early stopping and subsampling\n                    clf = lgb.LGBMClassifier(\n                        n_estimators=10000,\n                        learning_rate=lr,\n                        num_leaves=nl,\n                        feature_fraction=ff,\n                        bagging_fraction=bf,\n                        bagging_freq=5,\n                        random_state=42,\n                    )\n                    clf.fit(\n                        X_tr_enc,\n                        y_tr,\n                        eval_set=[(X_val_enc, y_val)],\n                        eval_metric=\"binary_error\",\n                        categorical_feature=categorical_cols,\n                        callbacks=[\n                            lgb.early_stopping(stopping_rounds=50),\n                            lgb.log_evaluation(period=0),\n                        ],\n                    )\n\n                    preds = clf.predict(X_val_enc)\n                    fold_accuracies.append(accuracy_score(y_val, preds))\n                    fold_iters.append(clf.best_iteration_)\n\n                mean_acc = np.mean(fold_accuracies)\n                mean_iter = int(np.mean(fold_iters))\n                print(\n                    f\"Params lr={lr}, nl={nl}, ff={ff}, bf={bf} -> CV Acc = {mean_acc:.4f}, Iter = {mean_iter}\"\n                )\n                if mean_acc > best_acc:\n                    best_acc = mean_acc\n                    best_params = {\n                        \"learning_rate\": lr,\n                        \"num_leaves\": nl,\n                        \"feature_fraction\": ff,\n                        \"bagging_fraction\": bf,\n                    }\n                    best_iter = mean_iter\n\nprint(\n    f\"Best CV Accuracy: {best_acc:.4f} with params {best_params} and n_estimators={best_iter}\"\n)\n\n# Retrain on full data with best params\nencoder_full = OrdinalEncoder()\nX_full_enc = X.copy()\nX_full_enc[categorical_cols] = encoder_full.fit_transform(X_full_enc[categorical_cols])\nX_test_enc = X_test.copy()\nX_test_enc[categorical_cols] = encoder_full.transform(X_test_enc[categorical_cols])\n\nfinal_clf = lgb.LGBMClassifier(\n    n_estimators=best_iter,\n    learning_rate=best_params[\"learning_rate\"],\n    num_leaves=best_params[\"num_leaves\"],\n    feature_fraction=best_params[\"feature_fraction\"],\n    bagging_fraction=best_params[\"bagging_fraction\"],\n    bagging_freq=5,\n    random_state=42,\n)\nfinal_clf.fit(X_full_enc, y, categorical_feature=categorical_cols)\n\n# Predict on test set and save submission\ntest_preds = final_clf.predict(X_test_enc).astype(int)\nsubmission = pd.DataFrame({\"PINCP\": test_preds})\nos.makedirs(\"./working\", exist_ok=True)\nsubmission.to_csv(\"./working/submission.csv\", index=False)\n","plan":"We will extend the existing grid search by adding LightGBM\u2019s feature_fraction and bagging_fraction parameters (values 0.8 and 1.0) to perform feature and data subsampling for better generalization. The grid search will loop over learning_rate, num_leaves, feature_fraction, and bagging_fraction using 5\u2010fold stratified CV with early stopping and ordinal encoding of categorical features. We will track mean CV accuracy and select the best parameter set, then retrain on the full dataset with those parameters. Finally, we will save test set predictions to `submission.csv` in the working directory.","step":19,"id":"ea377e4ddf34477093081f1f786d32d9","ctime":1764596001.023308,"parent":null,"children":[],"_term_out":["[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[368]\tvalid_0's binary_error: 0.174594\tvalid_0's binary_logloss: 0.379865","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002161 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[239]\tvalid_0's binary_error: 0.171783\tvalid_0's binary_logloss: 0.378787","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002929 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[240]\tvalid_0's binary_error: 0.177857\tvalid_0's binary_logloss: 0.384551","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003242 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[348]\tvalid_0's binary_error: 0.170319\tvalid_0's binary_logloss: 0.375208","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002605 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[328]\tvalid_0's binary_error: 0.174088\tvalid_0's binary_logloss: 0.380186","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, nl=31, ff=0.8, bf=0.8 -> CV Acc = 0.8263, Iter = 304","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002806 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[393]\tvalid_0's binary_error: 0.173444\tvalid_0's binary_logloss: 0.37925","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002881 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[418]\tvalid_0's binary_error: 0.170537\tvalid_0's binary_logloss: 0.375632","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002946 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[312]\tvalid_0's binary_error: 0.178081\tvalid_0's binary_logloss: 0.383636","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002927 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[489]\tvalid_0's binary_error: 0.168913\tvalid_0's binary_logloss: 0.374119","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003491 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[352]\tvalid_0's binary_error: 0.174439\tvalid_0's binary_logloss: 0.380505","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.05, nl=31, ff=0.8, bf=1.0 -> CV Acc = 0.8269, Iter = 392","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003166 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[459]\tvalid_0's binary_error: 0.173476\tvalid_0's binary_logloss: 0.379458","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003463 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[193]\tvalid_0's binary_error: 0.172837\tvalid_0's binary_logloss: 0.380575","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[247]\tvalid_0's binary_error: 0.178113\tvalid_0's binary_logloss: 0.38475","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002368 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[286]\tvalid_0's binary_error: 0.171437\tvalid_0's binary_logloss: 0.376963","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003122 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[249]\tvalid_0's binary_error: 0.17428\tvalid_0's binary_logloss: 0.381605","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, nl=31, ff=1.0, bf=0.8 -> CV Acc = 0.8260, Iter = 286","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002507 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[413]\tvalid_0's binary_error: 0.173859\tvalid_0's binary_logloss: 0.379836","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002836 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[357]\tvalid_0's binary_error: 0.171176\tvalid_0's binary_logloss: 0.376679","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003017 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[346]\tvalid_0's binary_error: 0.177378\tvalid_0's binary_logloss: 0.383217","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002983 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[411]\tvalid_0's binary_error: 0.169712\tvalid_0's binary_logloss: 0.375684","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002478 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[366]\tvalid_0's binary_error: 0.174248\tvalid_0's binary_logloss: 0.381092","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.05, nl=31, ff=1.0, bf=1.0 -> CV Acc = 0.8267, Iter = 378","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003642 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[237]\tvalid_0's binary_error: 0.174114\tvalid_0's binary_logloss: 0.378771","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[260]\tvalid_0's binary_error: 0.169898\tvalid_0's binary_logloss: 0.37499","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[254]\tvalid_0's binary_error: 0.176612\tvalid_0's binary_logloss: 0.381474","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003167 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[274]\tvalid_0's binary_error: 0.168051\tvalid_0's binary_logloss: 0.373533","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[215]\tvalid_0's binary_error: 0.173641\tvalid_0's binary_logloss: 0.379779","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, nl=63, ff=0.8, bf=0.8 -> CV Acc = 0.8275, Iter = 248","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003151 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[373]\tvalid_0's binary_error: 0.172773\tvalid_0's binary_logloss: 0.377267","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002976 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[258]\tvalid_0's binary_error: 0.169994\tvalid_0's binary_logloss: 0.374734","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004074 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[276]\tvalid_0's binary_error: 0.177921\tvalid_0's binary_logloss: 0.381288","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003991 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[389]\tvalid_0's binary_error: 0.167668\tvalid_0's binary_logloss: 0.372714","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[221]\tvalid_0's binary_error: 0.173098\tvalid_0's binary_logloss: 0.380208","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.05, nl=63, ff=0.8, bf=1.0 -> CV Acc = 0.8277, Iter = 303","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004492 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[265]\tvalid_0's binary_error: 0.173603\tvalid_0's binary_logloss: 0.379573","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004049 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[189]\tvalid_0's binary_error: 0.170888\tvalid_0's binary_logloss: 0.37764","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003302 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[172]\tvalid_0's binary_error: 0.177378\tvalid_0's binary_logloss: 0.383536","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003080 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[459]\tvalid_0's binary_error: 0.168147\tvalid_0's binary_logloss: 0.372955","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004883 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[199]\tvalid_0's binary_error: 0.173449\tvalid_0's binary_logloss: 0.380492","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, nl=63, ff=1.0, bf=0.8 -> CV Acc = 0.8273, Iter = 256","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004055 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[283]\tvalid_0's binary_error: 0.173188\tvalid_0's binary_logloss: 0.379084","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004358 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[237]\tvalid_0's binary_error: 0.170537\tvalid_0's binary_logloss: 0.375729","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003188 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[258]\tvalid_0's binary_error: 0.176835\tvalid_0's binary_logloss: 0.382011","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002411 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[188]\tvalid_0's binary_error: 0.170191\tvalid_0's binary_logloss: 0.375714","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002158 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[311]\tvalid_0's binary_error: 0.173226\tvalid_0's binary_logloss: 0.379514","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.05, nl=63, ff=1.0, bf=1.0 -> CV Acc = 0.8272, Iter = 255","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002655 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[205]\tvalid_0's binary_error: 0.17421\tvalid_0's binary_logloss: 0.378155","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002953 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[189]\tvalid_0's binary_error: 0.169451\tvalid_0's binary_logloss: 0.375366","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003093 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[156]\tvalid_0's binary_error: 0.177921\tvalid_0's binary_logloss: 0.382268","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002464 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[128]\tvalid_0's binary_error: 0.169009\tvalid_0's binary_logloss: 0.374801","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[278]\tvalid_0's binary_error: 0.172331\tvalid_0's binary_logloss: 0.377748","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, nl=127, ff=0.8, bf=0.8 -> CV Acc = 0.8274, Iter = 191","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003081 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[180]\tvalid_0's binary_error: 0.172581\tvalid_0's binary_logloss: 0.378293","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[139]\tvalid_0's binary_error: 0.170218\tvalid_0's binary_logloss: 0.376087","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002526 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.176675\tvalid_0's binary_logloss: 0.381021","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003181 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[260]\tvalid_0's binary_error: 0.168274\tvalid_0's binary_logloss: 0.372011","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004128 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[128]\tvalid_0's binary_error: 0.173162\tvalid_0's binary_logloss: 0.380064","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.05, nl=127, ff=0.8, bf=1.0 -> CV Acc = 0.8278, Iter = 176","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004345 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[153]\tvalid_0's binary_error: 0.173923\tvalid_0's binary_logloss: 0.379423","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004258 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[164]\tvalid_0's binary_error: 0.170058\tvalid_0's binary_logloss: 0.376047","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003306 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[114]\tvalid_0's binary_error: 0.178752\tvalid_0's binary_logloss: 0.383959","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004631 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[169]\tvalid_0's binary_error: 0.169552\tvalid_0's binary_logloss: 0.374615","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004599 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[222]\tvalid_0's binary_error: 0.174152\tvalid_0's binary_logloss: 0.379404","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.05, nl=127, ff=1.0, bf=0.8 -> CV Acc = 0.8267, Iter = 164","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003797 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.173028\tvalid_0's binary_logloss: 0.378887","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004684 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[268]\tvalid_0's binary_error: 0.169866\tvalid_0's binary_logloss: 0.374785","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004238 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[204]\tvalid_0's binary_error: 0.177698\tvalid_0's binary_logloss: 0.381407","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003507 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[298]\tvalid_0's binary_error: 0.16952\tvalid_0's binary_logloss: 0.373513","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[143]\tvalid_0's binary_error: 0.173705\tvalid_0's binary_logloss: 0.38059","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.05, nl=127, ff=1.0, bf=1.0 -> CV Acc = 0.8272, Iter = 217","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003199 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[197]\tvalid_0's binary_error: 0.174498\tvalid_0's binary_logloss: 0.38056","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002538 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[175]\tvalid_0's binary_error: 0.171463\tvalid_0's binary_logloss: 0.377798","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003286 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[283]\tvalid_0's binary_error: 0.176612\tvalid_0's binary_logloss: 0.382931","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[295]\tvalid_0's binary_error: 0.168626\tvalid_0's binary_logloss: 0.374054","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002788 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[154]\tvalid_0's binary_error: 0.173609\tvalid_0's binary_logloss: 0.380892","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, nl=31, ff=0.8, bf=0.8 -> CV Acc = 0.8270, Iter = 220","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002198 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[223]\tvalid_0's binary_error: 0.174753\tvalid_0's binary_logloss: 0.379622","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002983 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[308]\tvalid_0's binary_error: 0.169643\tvalid_0's binary_logloss: 0.374659","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002086 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[160]\tvalid_0's binary_error: 0.176995\tvalid_0's binary_logloss: 0.383706","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[312]\tvalid_0's binary_error: 0.167668\tvalid_0's binary_logloss: 0.37354","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002854 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[201]\tvalid_0's binary_error: 0.17313\tvalid_0's binary_logloss: 0.379728","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.1, nl=31, ff=0.8, bf=1.0 -> CV Acc = 0.8276, Iter = 240","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[250]\tvalid_0's binary_error: 0.174689\tvalid_0's binary_logloss: 0.380624","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002527 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[176]\tvalid_0's binary_error: 0.169898\tvalid_0's binary_logloss: 0.377649","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003113 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[165]\tvalid_0's binary_error: 0.177825\tvalid_0's binary_logloss: 0.384619","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002850 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[286]\tvalid_0's binary_error: 0.169935\tvalid_0's binary_logloss: 0.375192","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003142 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[237]\tvalid_0's binary_error: 0.174184\tvalid_0's binary_logloss: 0.380094","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, nl=31, ff=1.0, bf=0.8 -> CV Acc = 0.8267, Iter = 222","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004467 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[179]\tvalid_0's binary_error: 0.174498\tvalid_0's binary_logloss: 0.380948","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[315]\tvalid_0's binary_error: 0.170473\tvalid_0's binary_logloss: 0.375164","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003417 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[256]\tvalid_0's binary_error: 0.177761\tvalid_0's binary_logloss: 0.381979","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002697 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[430]\tvalid_0's binary_error: 0.168402\tvalid_0's binary_logloss: 0.374522","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003489 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[174]\tvalid_0's binary_error: 0.174056\tvalid_0's binary_logloss: 0.38163","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.1, nl=31, ff=1.0, bf=1.0 -> CV Acc = 0.8270, Iter = 270","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002507 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[186]\tvalid_0's binary_error: 0.173539\tvalid_0's binary_logloss: 0.379428","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[158]\tvalid_0's binary_error: 0.171431\tvalid_0's binary_logloss: 0.375879","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004187 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[162]\tvalid_0's binary_error: 0.177123\tvalid_0's binary_logloss: 0.382254","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003495 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[200]\tvalid_0's binary_error: 0.169297\tvalid_0's binary_logloss: 0.373754","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003494 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[120]\tvalid_0's binary_error: 0.172874\tvalid_0's binary_logloss: 0.379814","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, nl=63, ff=0.8, bf=0.8 -> CV Acc = 0.8271, Iter = 165","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003969 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[241]\tvalid_0's binary_error: 0.173412\tvalid_0's binary_logloss: 0.377106","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[125]\tvalid_0's binary_error: 0.169643\tvalid_0's binary_logloss: 0.37567","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003207 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[217]\tvalid_0's binary_error: 0.175398\tvalid_0's binary_logloss: 0.381065","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[206]\tvalid_0's binary_error: 0.168658\tvalid_0's binary_logloss: 0.372978","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003172 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[97]\tvalid_0's binary_error: 0.173864\tvalid_0's binary_logloss: 0.380815","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.1, nl=63, ff=0.8, bf=1.0 -> CV Acc = 0.8278, Iter = 177","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[190]\tvalid_0's binary_error: 0.173603\tvalid_0's binary_logloss: 0.379126","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004600 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[194]\tvalid_0's binary_error: 0.170729\tvalid_0's binary_logloss: 0.376433","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004532 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[108]\tvalid_0's binary_error: 0.177218\tvalid_0's binary_logloss: 0.382666","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004097 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[176]\tvalid_0's binary_error: 0.169456\tvalid_0's binary_logloss: 0.375054","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002606 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[168]\tvalid_0's binary_error: 0.173896\tvalid_0's binary_logloss: 0.380106","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, nl=63, ff=1.0, bf=0.8 -> CV Acc = 0.8270, Iter = 167","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003055 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[139]\tvalid_0's binary_error: 0.17453\tvalid_0's binary_logloss: 0.379874","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004660 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[122]\tvalid_0's binary_error: 0.170665\tvalid_0's binary_logloss: 0.376078","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004135 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[138]\tvalid_0's binary_error: 0.176484\tvalid_0's binary_logloss: 0.382455","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003573 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[212]\tvalid_0's binary_error: 0.168562\tvalid_0's binary_logloss: 0.374304","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003670 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[177]\tvalid_0's binary_error: 0.173034\tvalid_0's binary_logloss: 0.379611","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.1, nl=63, ff=1.0, bf=1.0 -> CV Acc = 0.8273, Iter = 157","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002870 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[141]\tvalid_0's binary_error: 0.174274\tvalid_0's binary_logloss: 0.379169","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[55]\tvalid_0's binary_error: 0.17124\tvalid_0's binary_logloss: 0.378479","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003491 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[72]\tvalid_0's binary_error: 0.177602\tvalid_0's binary_logloss: 0.383101","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002439 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[121]\tvalid_0's binary_error: 0.168786\tvalid_0's binary_logloss: 0.373018","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003159 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[130]\tvalid_0's binary_error: 0.173098\tvalid_0's binary_logloss: 0.380262","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, nl=127, ff=0.8, bf=0.8 -> CV Acc = 0.8270, Iter = 103","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002334 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[137]\tvalid_0's binary_error: 0.172262\tvalid_0's binary_logloss: 0.377198","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003214 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[73]\tvalid_0's binary_error: 0.170345\tvalid_0's binary_logloss: 0.376079","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002644 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[51]\tvalid_0's binary_error: 0.177346\tvalid_0's binary_logloss: 0.38436","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002587 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[156]\tvalid_0's binary_error: 0.167636\tvalid_0's binary_logloss: 0.372797","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003592 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[147]\tvalid_0's binary_error: 0.173002\tvalid_0's binary_logloss: 0.378704","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.1, nl=127, ff=0.8, bf=1.0 -> CV Acc = 0.8279, Iter = 112","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003073 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[78]\tvalid_0's binary_error: 0.173891\tvalid_0's binary_logloss: 0.38027","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003708 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[127]\tvalid_0's binary_error: 0.170984\tvalid_0's binary_logloss: 0.37654","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[105]\tvalid_0's binary_error: 0.177953\tvalid_0's binary_logloss: 0.383039","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002840 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[74]\tvalid_0's binary_error: 0.170095\tvalid_0's binary_logloss: 0.376067","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003668 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[106]\tvalid_0's binary_error: 0.174567\tvalid_0's binary_logloss: 0.380525","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8","\n","Params lr=0.1, nl=127, ff=1.0, bf=0.8 -> CV Acc = 0.8265, Iter = 98","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003055 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[129]\tvalid_0's binary_error: 0.172422\tvalid_0's binary_logloss: 0.378547","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73849","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003779 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 689","\n","[LightGBM] [Info] Number of data points in the train set: 125225, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410270 -> initscore=-0.362851","\n","[LightGBM] [Info] Start training from score -0.362851","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[112]\tvalid_0's binary_error: 0.170856\tvalid_0's binary_logloss: 0.375515","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003559 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 690","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[81]\tvalid_0's binary_error: 0.176835\tvalid_0's binary_logloss: 0.382959","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004595 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 691","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[119]\tvalid_0's binary_error: 0.167923\tvalid_0's binary_logloss: 0.374062","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 51376, number of negative: 73850","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003882 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 687","\n","[LightGBM] [Info] Number of data points in the train set: 125226, number of used features: 10","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410266 -> initscore=-0.362865","\n","[LightGBM] [Info] Start training from score -0.362865","\n","Training until validation scores don't improve for 50 rounds","\n","Early stopping, best iteration is:\n[71]\tvalid_0's binary_error: 0.173609\tvalid_0's binary_logloss: 0.380987","\n","[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Params lr=0.1, nl=127, ff=1.0, bf=1.0 -> CV Acc = 0.8277, Iter = 102","\n","Best CV Accuracy: 0.8279 with params {'learning_rate': 0.1, 'num_leaves': 127, 'feature_fraction': 0.8, 'bagging_fraction': 1.0} and n_estimators=112","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","[LightGBM] [Info] Number of positive: 64220, number of negative: 92312","\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.","\n","[LightGBM] [Info] Total Bins 694","\n","[LightGBM] [Info] Number of data points in the train set: 156532, number of used features: 10","\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.410268 -> initscore=-0.362859","\n","[LightGBM] [Info] Start training from score -0.362859","\n","[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8","\n","[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5","\n","[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0","\n","Execution time: 6 minutes seconds (time limit is an hour)."],"exec_time":367.0339868068695,"exc_type":null,"exc_info":null,"exc_stack":null,"analysis":"The training script executed successfully without any errors or bugs. The LightGBM warnings about bagging_freq and feature_fraction parameter precedence are expected and do not affect the correctness. The cross-validation with hyperparameter grid search yielded a best validation accuracy of approximately 0.8279, which indicates a good predictive performance on the PINCP target. The model was then retrained on the full training data with the best hyperparameters and the best number of boosting iterations before generating the final test set predictions and saving the submission file.","metric":{"value":0.8279,"maximize":true},"is_buggy":false}],"node2parent":{"e7081f52aa044341a204f2ea366ef5a2":"3a0b247c4c9c48e5bc86f3cb7f3ba224","6f541c5696184b178ad0df9125eac35d":"aa05560fd60a4173a76a8a9adefd275a","78426b50d24c425f97519380cabc4a76":"e7081f52aa044341a204f2ea366ef5a2","791b861639df40b582822e59c23d2a77":"78426b50d24c425f97519380cabc4a76","8bc8438b48024e5fb0700cfc0799a96a":"6f541c5696184b178ad0df9125eac35d","3a2ed6fce57440b8ac0af49b12770c1f":"791b861639df40b582822e59c23d2a77","7ca43cfad07849bcab07d18202a99f54":"8bc8438b48024e5fb0700cfc0799a96a","80976f1e8ee046adb459f03244db4022":"7ca43cfad07849bcab07d18202a99f54","620f065ab4344e2f91930d9dda2fe531":"80976f1e8ee046adb459f03244db4022","eca61f5570704bf9866ded74ea943677":"3a2ed6fce57440b8ac0af49b12770c1f","4dcdc9d4733247b9b6ba0c6d751fa724":"80976f1e8ee046adb459f03244db4022","744500116d8b46e2862ea99e8a646d82":"4dcdc9d4733247b9b6ba0c6d751fa724","6902d2088afb4a9ca61068ade21ba5c8":"4dcdc9d4733247b9b6ba0c6d751fa724","04007ff6bce440b6bb52cb933a90ed0e":"4dcdc9d4733247b9b6ba0c6d751fa724","ea377e4ddf34477093081f1f786d32d9":"4dcdc9d4733247b9b6ba0c6d751fa724"},"__version":"2"}