{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa1c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  freq      iccs unit geo  time  value\n",
      "0    A  ICCS0101   NR  AL  2008   88.0\n",
      "1    A  ICCS0101   NR  AL  2009   82.0\n",
      "2    A  ICCS0101   NR  AL  2010  118.0\n",
      "3    A  ICCS0101   NR  AL  2011  124.0\n",
      "4    A  ICCS0101   NR  AL  2012  125.0\n",
      "Shape: (19582, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def eurostat_json_to_tidy(json_data):\n",
    "    \"\"\"\n",
    "    Convert a Eurostat JSON (statistics 1.0 API) dataset\n",
    "    into a tidy Pandas DataFrame.\n",
    "\n",
    "    Each row in the result is one observation.\n",
    "    Columns are the dimensions (e.g. freq, iccs, unit, geo, time)\n",
    "    plus one column \"value\".\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Read the list of dimension ids and their sizes\n",
    "    # Example: id = [\"freq\", \"iccs\", \"unit\", \"geo\", \"time\"]\n",
    "    dim_ids = json_data[\"id\"]           # list of dimension names\n",
    "    dim_sizes = json_data[\"size\"]       # list of sizes for each dimension\n",
    "    dims = json_data[\"dimension\"]       # details for each dimension\n",
    "\n",
    "    # 2. Read the \"value\" object (a dict: index -> number)\n",
    "    val_dict = json_data[\"value\"]\n",
    "    # keys are strings \"0\", \"1\", ..., convert to int\n",
    "    obs_idx = np.array(list(map(int, val_dict.keys())))\n",
    "    obs_values = np.array(list(val_dict.values()), dtype=float)\n",
    "\n",
    "    # 3. Convert linear indices to multi-dimensional coordinates\n",
    "    # This uses the dimension sizes in the order of dim_ids.\n",
    "    coords = np.vstack(np.unravel_index(obs_idx, dim_sizes)).T\n",
    "    # coords shape: (n_obs, n_dims)\n",
    "\n",
    "    # 4. Build code lists for each dimension\n",
    "    # For each dimension, we have a mapping: code -> position\n",
    "    # We invert this to get an array: position -> code\n",
    "    code_lists = {}\n",
    "    for dim_name in dim_ids:\n",
    "        index_map = dims[dim_name][\"category\"][\"index\"]  # dict: code -> position\n",
    "        # sort by position (the value in the dict)\n",
    "        codes_sorted = [code for code, pos in sorted(index_map.items(), key=lambda kv: kv[1])]\n",
    "        code_lists[dim_name] = np.array(codes_sorted)\n",
    "\n",
    "    # 5. Map numeric positions in coords to actual codes\n",
    "    data = {}\n",
    "    for dim_i, dim_name in enumerate(dim_ids):\n",
    "        # coords[:, dim_i] gives the position index for this dimension\n",
    "        data[dim_name] = code_lists[dim_name][coords[:, dim_i]]\n",
    "\n",
    "    # 6. Add the numeric values\n",
    "    data[\"value\"] = obs_values\n",
    "\n",
    "    # 7. Build the final tidy DataFrame\n",
    "    df_tidy = pd.DataFrame(data)\n",
    "    return df_tidy\n",
    "\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "\n",
    "# You already have: json_data = response.json()\n",
    "df_tidy = eurostat_json_to_tidy(json_data)\n",
    "\n",
    "print(df_tidy.head())\n",
    "print(\"Shape:\", df_tidy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbd6fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: C:\\TUB\\RDEP\\crime_task\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create directory if not exists\n",
    "task_dir = r\"C:\\TUB\\RDEP\\crime_task\"\n",
    "os.makedirs(task_dir, exist_ok=True)\n",
    "\n",
    "# Split\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "# Save\n",
    "train_df.to_csv(os.path.join(task_dir, \"train.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(task_dir, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"Saved to:\", task_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b97c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.yaml saved to: C:\\TUB\\RDEP\\crime_task\\metadata.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "task_dir = r\"C:\\TUB\\RDEP\\crime_task\"\n",
    "metadata_path = os.path.join(task_dir, \"metadata.yaml\")\n",
    "\n",
    "metadata = {\n",
    "    \"task_type\": \"classification\",\n",
    "    \"label\": \"label\",\n",
    "    \"id_column\": None,\n",
    "    \"description\": \"Predict HIGH or LOW crime rate from Eurostat crim_off_cat dataset.\"\n",
    "}\n",
    "\n",
    "# Write YAML file\n",
    "with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(metadata, f, sort_keys=False)\n",
    "\n",
    "print(\"metadata.yaml saved to:\", metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Build new task ===\n",
      "[STEP] New task created: crime_task_novalue\n",
      "Train shape: (13707, 4)\n",
      "Test shape : (5875, 3)\n",
      "[STEP] metadata.yaml created.\n",
      "\n",
      "=== Baseline models ===\n",
      "\n",
      "[RF] Accuracy: 0.6100170192073912\n",
      "\n",
      "[FAIRNESS] Per-group results:\n",
      "            TPR       FPR       FNR  PositiveRate    N  TP  FP  TN  FN\n",
      "group                                                                 \n",
      "AL     0.431818  0.303030  0.568182      0.354545  110  19  20  46  25\n",
      "AT     0.600000  0.327586  0.400000      0.466102  118  36  19  39  24\n",
      "BA     0.852941  0.171429  0.147059      0.507246   69  29   6  29   5\n",
      "BE     0.712329  0.738095  0.287671      0.721739  115  52  31  11  21\n",
      "BG     0.510204  0.328571  0.489796      0.403361  119  25  23  47  24\n",
      "CH     0.694915  0.444444  0.305085      0.575221  113  41  24  30  18\n",
      "CY     0.657143  0.148649  0.342857      0.311927  109  23  11  63  12\n",
      "CZ     0.452830  0.358209  0.547170      0.400000  120  24  24  43  29\n",
      "DE     0.500000  0.697674  0.500000      0.577982  109  33  30  13  33\n",
      "DK     0.573770  0.350000  0.426230      0.485149  101  35  14  26  26\n",
      "EE     0.666667  0.236842  0.333333      0.358491  106  20  18  58  10\n",
      "EL     0.483871  0.377778  0.516129      0.439252  107  30  17  28  32\n",
      "ES     0.597403  0.588235  0.402597      0.593750  128  46  30  21  31\n",
      "FI     0.652778  0.565217  0.347222      0.618644  118  47  26  20  25\n",
      "FR     0.674699  0.742857  0.325301      0.694915  118  56  26   9  27\n",
      "HR     0.403846  0.355932  0.596154      0.378378  111  21  21  38  31\n",
      "HU     0.509434  0.446809  0.490566      0.480000  100  27  21  26  26\n",
      "IE     0.520000  0.400000  0.480000      0.463158   95  26  18  27  24\n",
      "IS     0.878049  0.175000  0.121951      0.530864   81  36   7  33   5\n",
      "IT     0.652174  0.666667  0.347826      0.657407  108  45  26  13  24\n",
      "LI     0.750000  0.125000  0.250000      0.352273   88  24   7  49   8\n",
      "LT     0.562500  0.376812  0.437500      0.452991  117  27  26  43  21\n",
      "LU     0.859649  0.172414  0.140351      0.513043  115  49  10  48   8\n",
      "LV     0.630435  0.491525  0.369565      0.552381  105  29  29  30  17\n",
      "ME     0.550000  0.149425  0.450000      0.224299  107  11  13  74   9\n",
      "MK     0.280000  0.121212  0.720000      0.189655   58   7   4  29  18\n",
      "MT     0.657143  0.094595  0.342857      0.275229  109  23   7  67  12\n",
      "NL     0.565217  0.636364  0.434783      0.592920  113  39  28  16  30\n",
      "NO     0.666667  0.500000  0.333333      0.582418   91  30  23  23  15\n",
      "PL     0.536232  0.545455  0.463768      0.540323  124  37  30  25  32\n",
      "PT     0.698113  0.521739  0.301887      0.616162   99  37  24  22  16\n",
      "RO     0.320755  0.600000  0.679245      0.462963  108  17  33  22  36\n",
      "RS     0.528302  0.482759  0.471698      0.504505  111  28  28  30  25\n",
      "SE     0.756410  0.750000  0.243590      0.754386  114  59  27   9  19\n",
      "SI     0.615385  0.273973  0.384615      0.392857  112  24  20  53  15\n",
      "SK     0.340909  0.222222  0.659091      0.267241  116  15  16  56  29\n",
      "TR     0.625000  0.800000  0.375000      0.676471   34  15   8   2   9\n",
      "UKC-L  0.733333  0.625000  0.266667      0.695652   46  22  10   6   8\n",
      "UKM    0.702128  0.684211  0.297872      0.696970   66  33  13   6  14\n",
      "UKN    0.714286  0.411765  0.285714      0.615385   52  25   7  10  10\n",
      "XK     0.656250  0.365854  0.343750      0.493151   73  21  15  26  11\n",
      "\n",
      "[FAIRNESS] Gaps:\n",
      "                     value\n",
      "PositiveRate_gap  0.564731\n",
      "TPR_gap           0.598049\n",
      "FPR_gap           0.705405\n",
      "FNR_gap           0.598049\n",
      "\n",
      "[LOG] Accuracy: 0.7186968149769025\n",
      "\n",
      "[FAIRNESS] Per-group results:\n",
      "            TPR       FPR       FNR  PositiveRate    N  TP  FP  TN  FN\n",
      "group                                                                 \n",
      "AL     0.500000  0.106061  0.500000      0.263636  110  22   7  59  22\n",
      "AT     0.716667  0.206897  0.283333      0.466102  118  43  12  46  17\n",
      "BA     0.647059  0.200000  0.352941      0.420290   69  22   7  28  12\n",
      "BE     0.835616  0.523810  0.164384      0.721739  115  61  22  20  12\n",
      "BG     0.551020  0.114286  0.448980      0.294118  119  27   8  62  22\n",
      "CH     0.762712  0.370370  0.237288      0.575221  113  45  20  34  14\n",
      "CY     0.485714  0.040541  0.514286      0.183486  109  17   3  71  18\n",
      "CZ     0.566038  0.179104  0.433962      0.350000  120  30  12  55  23\n",
      "DE     0.696970  0.558140  0.303030      0.642202  109  46  24  19  20\n",
      "DK     0.737705  0.200000  0.262295      0.524752  101  45   8  32  16\n",
      "EE     0.666667  0.105263  0.333333      0.264151  106  20   8  68  10\n",
      "EL     0.564516  0.266667  0.435484      0.439252  107  35  12  33  27\n",
      "ES     0.766234  0.529412  0.233766      0.671875  128  59  27  24  18\n",
      "FI     0.750000  0.478261  0.250000      0.644068  118  54  22  24  18\n",
      "FR     0.819277  0.657143  0.180723      0.771186  118  68  23  12  15\n",
      "HR     0.423077  0.118644  0.576923      0.261261  111  22   7  52  30\n",
      "HU     0.566038  0.255319  0.433962      0.420000  100  30  12  35  23\n",
      "IE     0.580000  0.088889  0.420000      0.347368   95  29   4  41  21\n",
      "IS     0.756098  0.050000  0.243902      0.407407   81  31   2  38  10\n",
      "IT     0.826087  0.564103  0.173913      0.731481  108  57  22  17  12\n",
      "LI     0.718750  0.035714  0.281250      0.284091   88  23   2  54   9\n",
      "LT     0.520833  0.188406  0.479167      0.324786  117  25  13  56  23\n",
      "LU     0.859649  0.137931  0.140351      0.495652  115  49   8  50   8\n",
      "LV     0.586957  0.237288  0.413043      0.390476  105  27  14  45  19\n",
      "ME     0.600000  0.080460  0.400000      0.177570  107  12   7  80   8\n",
      "MK     0.440000  0.060606  0.560000      0.224138   58  11   2  31  14\n",
      "MT     0.685714  0.040541  0.314286      0.247706  109  24   3  71  11\n",
      "NL     0.768116  0.545455  0.231884      0.681416  113  53  24  20  16\n",
      "NO     0.800000  0.239130  0.200000      0.516484   91  36  11  35   9\n",
      "PL     0.811594  0.618182  0.188406      0.725806  124  56  34  21  13\n",
      "PT     0.811321  0.456522  0.188679      0.646465   99  43  21  25  10\n",
      "RO     0.433962  0.381818  0.566038      0.407407  108  23  21  34  30\n",
      "RS     0.490566  0.241379  0.509434      0.360360  111  26  14  44  27\n",
      "SE     0.910256  0.750000  0.089744      0.859649  114  71  27   9   7\n",
      "SI     0.692308  0.150685  0.307692      0.339286  112  27  11  62  12\n",
      "SK     0.454545  0.111111  0.545455      0.241379  116  20   8  64  24\n",
      "TR     0.708333  0.400000  0.291667      0.617647   34  17   4   6   7\n",
      "UKC-L  0.866667  0.812500  0.133333      0.847826   46  26  13   3   4\n",
      "UKM    0.893617  0.684211  0.106383      0.833333   66  42  13   6   5\n",
      "UKN    0.828571  0.294118  0.171429      0.653846   52  29   5  12   6\n",
      "XK     0.656250  0.170732  0.343750      0.383562   73  21   7  34  11\n",
      "\n",
      "[FAIRNESS] Gaps:\n",
      "                     value\n",
      "PositiveRate_gap  0.682079\n",
      "TPR_gap           0.487179\n",
      "FPR_gap           0.776786\n",
      "FNR_gap           0.487179\n",
      "\n",
      "=== AIDE search ===\n",
      "\n",
      "[AIDE] Start search...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wenyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aide\\utils\\tree_export.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  layout = (layout - layout.min(axis=0)) / (layout.max(axis=0) - layout.min(axis=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AIDE] Best accuracy: 0.7145\n",
      "\n",
      "[AIDE] Best code:\n",
      " import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "from sklearn.metrics import accuracy_score\n",
      "import lightgbm as lgb\n",
      "\n",
      "# Load data\n",
      "train = pd.read_csv(\"./input/train.csv\")\n",
      "test = pd.read_csv(\"./input/test.csv\")\n",
      "\n",
      "# Features and label\n",
      "features = [\"geo\", \"iccs\", \"time\"]\n",
      "X = train[features].copy()\n",
      "y = train[\"label\"].copy()\n",
      "X_test = test[features].copy()\n",
      "\n",
      "# Label encode categorical features\n",
      "for col in [\"geo\", \"iccs\"]:\n",
      "    le = LabelEncoder()\n",
      "    X[col] = le.fit_transform(X[col])\n",
      "    X_test[col] = le.transform(X_test[col])\n",
      "\n",
      "# Prepare cross-validation\n",
      "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
      "accs = []\n",
      "\n",
      "for train_idx, valid_idx in skf.split(X, y):\n",
      "    X_tr, X_val = X.iloc[train_idx], X.iloc[valid_idx]\n",
      "    y_tr, y_val = y.iloc[train_idx], y.iloc[valid_idx]\n",
      "\n",
      "    model = lgb.LGBMClassifier(random_state=42)\n",
      "    model.fit(X_tr, y_tr)\n",
      "    preds = model.predict(X_val)\n",
      "    acc = accuracy_score(y_val, preds)\n",
      "    accs.append(acc)\n",
      "\n",
      "# Print cross-validation accuracy\n",
      "mean_acc = np.mean(accs)\n",
      "print(f\"5-fold CV Accuracy: {mean_acc:.4f}\")\n",
      "\n",
      "# Retrain on full data and predict test set\n",
      "final_model = lgb.LGBMClassifier(random_state=42)\n",
      "final_model.fit(X, y)\n",
      "test_preds = final_model.predict(X_test)\n",
      "\n",
      "# Save submission\n",
      "submission = pd.DataFrame({\"label\": test_preds})\n",
      "submission.to_csv(\"./working/submission.csv\", index=False)\n",
      "\n",
      "[ERROR] Could not load AIDE model. Adjust API manually.\n",
      "\n",
      "All steps completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError:\n",
    "    yaml = None\n",
    "\n",
    "try:\n",
    "    import aide\n",
    "except ImportError:\n",
    "    aide = None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Path settings\n",
    "# =========================\n",
    "\n",
    "BASE_ROOT = r\"C:\\TUB\\RDEP\"\n",
    "TASK_WITH_VALUE = os.path.join(BASE_ROOT, \"crime_task\")\n",
    "TASK_NOVALUE = os.path.join(BASE_ROOT, \"crime_task_novalue\")\n",
    "os.makedirs(TASK_NOVALUE, exist_ok=True)\n",
    "\n",
    "WORK_DIR = os.path.join(TASK_NOVALUE, \"working\")\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Build new task (without \"value\")\n",
    "# =========================\n",
    "\n",
    "def build_task_without_value():\n",
    "    \"\"\"\n",
    "    Create a new task where we remove the column 'value'.\n",
    "    This task will only use geo, iccs, and time.\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(TASK_WITH_VALUE, \"train.csv\")\n",
    "    test_path = os.path.join(TASK_WITH_VALUE, \"test.csv\")\n",
    "\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "\n",
    "    # Only keep these columns\n",
    "    train_novalue = train[[\"geo\", \"iccs\", \"time\", \"label\"]].copy()\n",
    "    test_novalue = test[[\"geo\", \"iccs\", \"time\"]].copy()\n",
    "\n",
    "    train_novalue.to_csv(os.path.join(TASK_NOVALUE, \"train.csv\"), index=False)\n",
    "    test_novalue.to_csv(os.path.join(TASK_NOVALUE, \"test.csv\"), index=False)\n",
    "\n",
    "    print(\"[STEP] New task created: crime_task_novalue\")\n",
    "    print(\"Train shape:\", train_novalue.shape)\n",
    "    print(\"Test shape :\", test_novalue.shape)\n",
    "\n",
    "    # Write metadata if YAML is available\n",
    "    if yaml is not None:\n",
    "        metadata = {\n",
    "            \"task_type\": \"classification\",\n",
    "            \"label\": \"label\",\n",
    "            \"id_column\": None,\n",
    "            \"description\": \"Predict crime level using geo, iccs, and time.\",\n",
    "        }\n",
    "        with open(os.path.join(TASK_NOVALUE, \"metadata.yaml\"), \"w\") as f:\n",
    "            yaml.dump(metadata, f)\n",
    "        print(\"[STEP] metadata.yaml created.\")\n",
    "    else:\n",
    "        print(\"[WARN] PyYAML not installed. metadata.yaml not created.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Fairness helper functions\n",
    "# =========================\n",
    "\n",
    "def compute_rates(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics:\n",
    "    TPR, FPR, FNR, PositiveRate and confusion counts.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    eps = 1e-12\n",
    "\n",
    "    return {\n",
    "        \"TPR\": tp / (tp + fn + eps),\n",
    "        \"FPR\": fp / (fp + tn + eps),\n",
    "        \"FNR\": fn / (fn + tp + eps),\n",
    "        \"PositiveRate\": (tp + fp) / (tp + fp + tn + fn + eps),\n",
    "        \"N\": tn + fp + fn + tp,\n",
    "        \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_fairness_by_group(model, X, y, groups, group_name=\"geo\", save_prefix=\"model\"):\n",
    "    \"\"\"\n",
    "    Calculate fairness metrics for each group.\n",
    "    For example, groups = geo.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    result_list = []\n",
    "    for g in sorted(groups.unique()):\n",
    "        mask = (groups == g)\n",
    "        rates = compute_rates(y[mask], y_pred[mask])\n",
    "        rates[\"group\"] = g\n",
    "        result_list.append(rates)\n",
    "\n",
    "    df_group = pd.DataFrame(result_list).set_index(\"group\")\n",
    "\n",
    "    # Calculate fairness gaps\n",
    "    gaps = {}\n",
    "    for metric in [\"PositiveRate\", \"TPR\", \"FPR\", \"FNR\"]:\n",
    "        gaps[metric + \"_gap\"] = df_group[metric].max() - df_group[metric].min()\n",
    "\n",
    "    df_gaps = pd.DataFrame.from_dict(gaps, orient=\"index\", columns=[\"value\"])\n",
    "\n",
    "    print(\"\\n[FAIRNESS] Per-group results:\")\n",
    "    print(df_group)\n",
    "\n",
    "    print(\"\\n[FAIRNESS] Gaps:\")\n",
    "    print(df_gaps)\n",
    "\n",
    "    df_group.to_csv(os.path.join(WORK_DIR, f\"{save_prefix}_fairness_by_{group_name}.csv\"))\n",
    "    df_gaps.to_csv(os.path.join(WORK_DIR, f\"{save_prefix}_fairness_gaps_{group_name}.csv\"))\n",
    "\n",
    "    return df_group, df_gaps\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Baseline Models\n",
    "# =========================\n",
    "\n",
    "def run_baseline_models():\n",
    "    \"\"\"\n",
    "    Train baseline models (RandomForest and LogisticRegression)\n",
    "    and test their fairness.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(os.path.join(TASK_NOVALUE, \"train.csv\"))\n",
    "    X = train[[\"geo\", \"iccs\", \"time\"]]\n",
    "    y = train[\"label\"]\n",
    "    groups = train[\"geo\"]\n",
    "\n",
    "    # Split for validation\n",
    "    X_train, X_val, y_train, y_val, groups_train, groups_val = train_test_split(\n",
    "        X, y, groups, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Preprocessing: encode geo, iccs + scale time\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"geo\", \"iccs\"]),\n",
    "            (\"num\", StandardScaler(), [\"time\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    rf = Pipeline([\n",
    "        (\"preproc\", preproc),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(\"\\n[RF] Accuracy:\", accuracy_score(y_val, rf.predict(X_val)))\n",
    "\n",
    "    evaluate_fairness_by_group(\n",
    "        rf, X_val, y_val, groups_val, save_prefix=\"rf\"\n",
    "    )\n",
    "\n",
    "    dump(rf, os.path.join(WORK_DIR, \"rf_model.joblib\"))\n",
    "\n",
    "    # --- Logistic Regression ---\n",
    "    log = Pipeline([\n",
    "        (\"preproc\", preproc),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    log.fit(X_train, y_train)\n",
    "    print(\"\\n[LOG] Accuracy:\", accuracy_score(y_val, log.predict(X_val)))\n",
    "\n",
    "    evaluate_fairness_by_group(\n",
    "        log, X_val, y_val, groups_val, save_prefix=\"log\"\n",
    "    )\n",
    "\n",
    "    dump(log, os.path.join(WORK_DIR, \"log_model.joblib\"))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. AIDE experiment\n",
    "# =========================\n",
    "\n",
    "def run_aide_experiment():\n",
    "    \"\"\"\n",
    "    Run AIDE automatic model search.\n",
    "    You may need to adjust the API for your own AIDE version.\n",
    "    \"\"\"\n",
    "    if aide is None:\n",
    "        print(\"\\n[WARN] AIDE is not installed. Skip this step.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n[AIDE] Start search...\")\n",
    "\n",
    "    exp = aide.Experiment(\n",
    "        data_dir=TASK_NOVALUE,\n",
    "        goal=\"Predict crime level.\",\n",
    "        eval=\"Classification accuracy\"\n",
    "    )\n",
    "\n",
    "    best_solution = exp.run(steps=5)\n",
    "\n",
    "    print(\"\\n[AIDE] Best accuracy:\", best_solution.valid_metric)\n",
    "    print(\"\\n[AIDE] Best code:\\n\", best_solution.code)\n",
    "\n",
    "    # You may need to change this line to match your own AIDE version.\n",
    "    try:\n",
    "        aide_model = best_solution.model\n",
    "    except:\n",
    "        print(\"[ERROR] Could not load AIDE model. Adjust API manually.\")\n",
    "        return\n",
    "\n",
    "    # Train AIDE model on train split\n",
    "    train = pd.read_csv(os.path.join(TASK_NOVALUE, \"train.csv\"))\n",
    "    X = train[[\"geo\", \"iccs\", \"time\"]]\n",
    "    y = train[\"label\"]\n",
    "    groups = train[\"geo\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val, groups_train, groups_val = train_test_split(\n",
    "        X, y, groups, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    aide_model.fit(X_train, y_train)\n",
    "    print(\"\\n[AIDE] Accuracy:\", accuracy_score(y_val, aide_model.predict(X_val)))\n",
    "\n",
    "    evaluate_fairness_by_group(\n",
    "        aide_model, X_val, y_val, groups_val, save_prefix=\"aide\"\n",
    "    )\n",
    "\n",
    "    dump(aide_model, os.path.join(WORK_DIR, \"aide_model.joblib\"))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Build new task ===\")\n",
    "    build_task_without_value()\n",
    "\n",
    "    print(\"\\n=== Baseline models ===\")\n",
    "    run_baseline_models()\n",
    "\n",
    "    print(\"\\n=== AIDE search ===\")\n",
    "    run_aide_experiment()\n",
    "\n",
    "    print(\"\\nAll steps completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc74e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: ['geo', 'iccs', 'time', 'value']\n",
      "Reweighing weights range: 0.6431338295772507 to 2.2461064639351735\n",
      "\n",
      "=== Baseline logistic regression ===\n",
      "Baseline accuracy: 0.7786\n",
      "Baseline per-group metrics:\n",
      "        N  TP  FP  TN  FN       TPR       FPR       FNR  PositiveRate\n",
      "group                                                                \n",
      "AL     70  13   2  40  15  0.464286  0.047619  0.535714      0.214286\n",
      "AT     84  40   3  35   6  0.869565  0.078947  0.130435      0.511905\n",
      "BA     41  16   5  14   6  0.727273  0.263158  0.272727      0.512195\n",
      "BE     80  41  10  20   9  0.820000  0.333333  0.180000      0.637500\n",
      "BG     82  21   6  44  11  0.656250  0.120000  0.343750      0.329268\n",
      "CH     71  27   2  31  11  0.710526  0.060606  0.289474      0.408451\n",
      "CY     74  10   1  47  16  0.384615  0.020833  0.615385      0.148649\n",
      "CZ     77  15   6  43  13  0.535714  0.122449  0.464286      0.272727\n",
      "DE     69  35   1  23  10  0.777778  0.041667  0.222222      0.521739\n",
      "DK     67  31   3  23  10  0.756098  0.115385  0.243902      0.507463\n",
      "EE     70  16   1  48   5  0.761905  0.020408  0.238095      0.242857\n",
      "EL     70  22   7  24  17  0.564103  0.225806  0.435897      0.414286\n",
      "ES     90  43   9  27  11  0.796296  0.250000  0.203704      0.577778\n",
      "FI     77  37   8  21  11  0.770833  0.275862  0.229167      0.584416\n",
      "FR     74  43   2  20   9  0.826923  0.090909  0.173077      0.608108\n",
      "HR     70  18   6  29  17  0.514286  0.171429  0.485714      0.342857\n",
      "HU     69  23   5  26  15  0.605263  0.161290  0.394737      0.405797\n",
      "IE     65  25   2  29   9  0.735294  0.064516  0.264706      0.415385\n",
      "IS     53  17   1  26   9  0.653846  0.037037  0.346154      0.339623\n",
      "IT     71  42  10  10   9  0.823529  0.500000  0.176471      0.732394\n",
      "LI     66  16   1  42   7  0.695652  0.023256  0.304348      0.257576\n",
      "LT     85  21   7  43  14  0.600000  0.140000  0.400000      0.329412\n",
      "LU     81  34   4  38   5  0.871795  0.095238  0.128205      0.469136\n",
      "LV     69  20   8  27  14  0.588235  0.228571  0.411765      0.405797\n",
      "ME     69   7   4  51   7  0.500000  0.072727  0.500000      0.159420\n",
      "MK     41   9   1  22   9  0.500000  0.043478  0.500000      0.243902\n",
      "MT     70  11   1  51   7  0.611111  0.019231  0.388889      0.171429\n",
      "NL     59  27  10  16   6  0.818182  0.384615  0.181818      0.627119\n",
      "NO     58  21   3  25   9  0.700000  0.107143  0.300000      0.413793\n",
      "PL     86  33  17  29   7  0.825000  0.369565  0.175000      0.581395\n",
      "PT     75  35   7  28   5  0.875000  0.200000  0.125000      0.560000\n",
      "RO     81  24  10  31  16  0.600000  0.243902  0.400000      0.419753\n",
      "RS     82  25   8  32  17  0.595238  0.200000  0.404762      0.402439\n",
      "SE     81  44  11  15  11  0.800000  0.423077  0.200000      0.679012\n",
      "SI     70  14   7  41   8  0.636364  0.145833  0.363636      0.300000\n",
      "SK     74  15   3  40  16  0.483871  0.069767  0.516129      0.243243\n",
      "TR     20  12   2   4   2  0.857143  0.333333  0.142857      0.700000\n",
      "UKC-L  28  17   4   5   2  0.894737  0.444444  0.105263      0.750000\n",
      "UKM    44  26   9   6   3  0.896552  0.600000  0.103448      0.795455\n",
      "UKN    32  17   2   9   4  0.809524  0.181818  0.190476      0.593750\n",
      "XK     47  15   5  22   5  0.750000  0.185185  0.250000      0.425532\n",
      "\n",
      "Baseline disparities:\n",
      "                     value\n",
      "FNR_gap           0.511936\n",
      "FPR_gap           0.580769\n",
      "PositiveRate_gap  0.646806\n",
      "TPR_gap           0.511936\n",
      "\n",
      "=== Fairness-aware logistic regression (reweighing) ===\n",
      "Fairness-aware accuracy: 0.7863\n",
      "Reweighing per-group metrics:\n",
      "        N  TP  FP  TN  FN       TPR       FPR       FNR  PositiveRate\n",
      "group                                                                \n",
      "AL     70  16   8  34  12  0.571429  0.190476  0.428571      0.342857\n",
      "AT     84  41   3  35   5  0.891304  0.078947  0.108696      0.523810\n",
      "BA     41  16   6  13   6  0.727273  0.315789  0.272727      0.536585\n",
      "BE     80  39   3  27  11  0.780000  0.100000  0.220000      0.525000\n",
      "BG     82  24   7  43   8  0.750000  0.140000  0.250000      0.378049\n",
      "CH     71  27   2  31  11  0.710526  0.060606  0.289474      0.408451\n",
      "CY     74  20   2  46   6  0.769231  0.041667  0.230769      0.297297\n",
      "CZ     77  15   7  42  13  0.535714  0.142857  0.464286      0.285714\n",
      "DE     69  31   0  24  14  0.688889  0.000000  0.311111      0.449275\n",
      "DK     67  29   1  25  12  0.707317  0.038462  0.292683      0.447761\n",
      "EE     70  18   8  41   3  0.857143  0.163265  0.142857      0.371429\n",
      "EL     70  21   4  27  18  0.538462  0.129032  0.461538      0.357143\n",
      "ES     90  38   5  31  16  0.703704  0.138889  0.296296      0.477778\n",
      "FI     77  34   4  25  14  0.708333  0.137931  0.291667      0.493506\n",
      "FR     74  36   0  22  16  0.692308  0.000000  0.307692      0.486486\n",
      "HR     70  19   8  27  16  0.542857  0.228571  0.457143      0.385714\n",
      "HU     69  23   5  26  15  0.605263  0.161290  0.394737      0.405797\n",
      "IE     65  25   2  29   9  0.735294  0.064516  0.264706      0.415385\n",
      "IS     53  18   2  25   8  0.692308  0.074074  0.307692      0.377358\n",
      "IT     71  41   4  16  10  0.803922  0.200000  0.196078      0.633803\n",
      "LI     66  18   4  39   5  0.782609  0.093023  0.217391      0.333333\n",
      "LT     85  23   9  41  12  0.657143  0.180000  0.342857      0.376471\n",
      "LU     81  32   3  39   7  0.820513  0.071429  0.179487      0.432099\n",
      "LV     69  21  10  25  13  0.617647  0.285714  0.382353      0.449275\n",
      "ME     69   9   8  47   5  0.642857  0.145455  0.357143      0.246377\n",
      "MK     41  13   5  18   5  0.722222  0.217391  0.277778      0.439024\n",
      "MT     70  14   3  49   4  0.777778  0.057692  0.222222      0.242857\n",
      "NL     59  21   1  25  12  0.636364  0.038462  0.363636      0.372881\n",
      "NO     58  21   3  25   9  0.700000  0.107143  0.300000      0.413793\n",
      "PL     86  31  10  36   9  0.775000  0.217391  0.225000      0.476744\n",
      "PT     75  31   3  32   9  0.775000  0.085714  0.225000      0.453333\n",
      "RO     81  23   8  33  17  0.575000  0.195122  0.425000      0.382716\n",
      "RS     82  25  10  30  17  0.595238  0.250000  0.404762      0.426829\n",
      "SE     81  40   0  26  15  0.727273  0.000000  0.272727      0.493827\n",
      "SI     70  14  11  37   8  0.636364  0.229167  0.363636      0.357143\n",
      "SK     74  21   6  37  10  0.677419  0.139535  0.322581      0.364865\n",
      "TR     20  12   1   5   2  0.857143  0.166667  0.142857      0.650000\n",
      "UKC-L  28  13   1   8   6  0.684211  0.111111  0.315789      0.500000\n",
      "UKM    44  26   5  10   3  0.896552  0.333333  0.103448      0.704545\n",
      "UKN    32  17   1  10   4  0.809524  0.090909  0.190476      0.562500\n",
      "XK     47  17   5  22   3  0.850000  0.185185  0.150000      0.468085\n",
      "\n",
      "Reweighing disparities:\n",
      "                     value\n",
      "FNR_gap           0.360837\n",
      "FPR_gap           0.333333\n",
      "PositiveRate_gap  0.461688\n",
      "TPR_gap           0.360837\n",
      "\n",
      "=== Post-processing thresholds (group-wise) ===\n",
      "Example thresholds (first 10 groups): [('AL', 0.62), ('AT', 0.58), ('BA', 0.7), ('BE', 0.58), ('BG', 0.78), ('CH', 0.62), ('CY', 0.45999999999999996), ('CZ', 0.7), ('DE', 0.45999999999999996), ('DK', 0.54)]\n",
      "Post-processed accuracy: 0.7947\n",
      "Post-processing per-group metrics:\n",
      "        N  TP  FP  TN  FN       TPR       FPR       FNR  PositiveRate\n",
      "group                                                                \n",
      "AL     70  13   2  40  15  0.464286  0.047619  0.535714      0.214286\n",
      "AT     84  36   0  38  10  0.782609  0.000000  0.217391      0.428571\n",
      "BA     41  13   2  17   9  0.590909  0.105263  0.409091      0.365854\n",
      "BE     80  37   1  29  13  0.740000  0.033333  0.260000      0.475000\n",
      "BG     82  17   1  49  15  0.531250  0.020000  0.468750      0.219512\n",
      "CH     71  24   0  33  14  0.631579  0.000000  0.368421      0.338028\n",
      "CY     74  22   2  46   4  0.846154  0.041667  0.153846      0.324324\n",
      "CZ     77  13   1  48  15  0.464286  0.020408  0.535714      0.181818\n",
      "DE     69  32   0  24  13  0.711111  0.000000  0.288889      0.463768\n",
      "DK     67  29   0  26  12  0.707317  0.000000  0.292683      0.432836\n",
      "EE     70  16   1  48   5  0.761905  0.020408  0.238095      0.242857\n",
      "EL     70  18   0  31  21  0.461538  0.000000  0.538462      0.257143\n",
      "ES     90  37   0  36  17  0.685185  0.000000  0.314815      0.411111\n",
      "FI     77  29   1  28  19  0.604167  0.034483  0.395833      0.389610\n",
      "FR     74  40   0  22  12  0.769231  0.000000  0.230769      0.540541\n",
      "HR     70  12   0  35  23  0.342857  0.000000  0.657143      0.171429\n",
      "HU     69  14   0  31  24  0.368421  0.000000  0.631579      0.202899\n",
      "IE     65  24   1  30  10  0.705882  0.032258  0.294118      0.384615\n",
      "IS     53  14   0  27  12  0.538462  0.000000  0.461538      0.264151\n",
      "IT     71  32   0  20  19  0.627451  0.000000  0.372549      0.450704\n",
      "LI     66  18   2  41   5  0.782609  0.046512  0.217391      0.303030\n",
      "LT     85  16   2  48  19  0.457143  0.040000  0.542857      0.211765\n",
      "LU     81  27   0  42  12  0.692308  0.000000  0.307692      0.333333\n",
      "LV     69  16   2  33  18  0.470588  0.057143  0.529412      0.260870\n",
      "ME     69   7   4  51   7  0.500000  0.072727  0.500000      0.159420\n",
      "MK     41   9   1  22   9  0.500000  0.043478  0.500000      0.243902\n",
      "MT     70  11   0  52   7  0.611111  0.000000  0.388889      0.157143\n",
      "NL     59  19   0  26  14  0.575758  0.000000  0.424242      0.322034\n",
      "NO     58  18   0  28  12  0.600000  0.000000  0.400000      0.310345\n",
      "PL     86  22   0  46  18  0.550000  0.000000  0.450000      0.255814\n",
      "PT     75  31   3  32   9  0.775000  0.085714  0.225000      0.453333\n",
      "RO     81  14   1  40  26  0.350000  0.024390  0.650000      0.185185\n",
      "RS     82  19   0  40  23  0.452381  0.000000  0.547619      0.231707\n",
      "SE     81  40   0  26  15  0.727273  0.000000  0.272727      0.493827\n",
      "SI     70  13   1  47   9  0.590909  0.020833  0.409091      0.200000\n",
      "SK     74   9   0  43  22  0.290323  0.000000  0.709677      0.121622\n",
      "TR     20  11   0   6   3  0.785714  0.000000  0.214286      0.550000\n",
      "UKC-L  28  13   0   9   6  0.684211  0.000000  0.315789      0.464286\n",
      "UKM    44  22   0  15   7  0.758621  0.000000  0.241379      0.500000\n",
      "UKN    32  17   1  10   4  0.809524  0.090909  0.190476      0.562500\n",
      "XK     47  14   1  26   6  0.700000  0.037037  0.300000      0.319149\n",
      "\n",
      "Post-processing disparities:\n",
      "                     value\n",
      "FNR_gap           0.555831\n",
      "FPR_gap           0.105263\n",
      "PositiveRate_gap  0.440878\n",
      "TPR_gap           0.555831\n",
      "\n",
      "=== Train final model on full train (with reweighing) ===\n",
      "Saved final submission to: C:\\TUB\\RDEP\\crime_task\\working\\submission_fair_logreg.csv\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 0. Prompt for AIDE-ML (to drive code generation)\n",
    "# =====================================================\n",
    "\n",
    "PROMPT_FOR_AIDE = \"\"\"\n",
    "You are an AI assistant that automatically builds, trains and evaluates a fairness-aware\n",
    "machine learning model for a crime prediction task.\n",
    "\n",
    "Goal:\n",
    "- Improve group fairness across countries (feature `geo`) while keeping classification accuracy high.\n",
    "- Implement an end-to-end workflow that can be run as a single Python script.\n",
    "\n",
    "Data description:\n",
    "- Input files: `train.csv` and `test.csv` in a given BASE_DIR.\n",
    "- Target column in train: `label` (binary).\n",
    "- Candidate feature columns: \"geo\", \"iccs\", \"time\", \"value\" (some of them may be missing).\n",
    "- Sensitive attribute: \"geo\" (country / region).\n",
    "- Test.csv has the same feature columns as train, but without `label`.\n",
    "\n",
    "Your generated code must do the following:\n",
    "\n",
    "1. Load data and prepare features:\n",
    "   - Set a BASE_DIR (e.g. \"C:\\\\\\\\TUB\\\\\\\\RDEP\\\\\\\\crime_task\").\n",
    "   - Load `train.csv` and `test.csv` with pandas.\n",
    "   - From [\"geo\", \"iccs\", \"time\", \"value\"], select only those columns that exist in train.\n",
    "   - Define:\n",
    "       X      = train[features]\n",
    "       y      = train[\"label\"]\n",
    "       X_test = test[features]\n",
    "   - Extract the sensitive attribute:\n",
    "       groups = X[\"geo\"].values\n",
    "\n",
    "2. Create a train/validation split:\n",
    "   - Use sklearn.model_selection.train_test_split\n",
    "   - 80% train, 20% validation\n",
    "   - stratify by y\n",
    "   - Keep track of group labels for both sets (g_train, g_val).\n",
    "\n",
    "3. Implement group fairness metrics:\n",
    "   - Write a function `compute_group_fairness(y_true, y_pred, group_values)` that:\n",
    "       * For each group g:\n",
    "           - Computes TP, FP, TN, FN.\n",
    "           - Computes:\n",
    "               TPR = TP / (TP + FN)\n",
    "               FPR = FP / (FP + TN)\n",
    "               FNR = FN / (TP + FN)\n",
    "               PositiveRate = (TP + FP) / N_group\n",
    "       * Returns:\n",
    "           - A per-group metrics DataFrame.\n",
    "           - A DataFrame (or dict) with disparities:\n",
    "               TPR_gap          = max(TPR) - min(TPR)\n",
    "               FPR_gap          = max(FPR) - min(FPR)\n",
    "               FNR_gap          = max(FNR) - min(FNR)\n",
    "               PositiveRate_gap = max(PositiveRate) - min(PositiveRate).\n",
    "\n",
    "4. Implement Reweighing (Kamiran & Calders, 2012):\n",
    "   - Write a function `compute_reweighing_weights(df, group_col, label_col)` that:\n",
    "       * Estimates empirical probabilities:\n",
    "           P(a)   for each group a,\n",
    "           P(y)   for each label y,\n",
    "           P(a,y) joint probability of group a and label y.\n",
    "       * Computes sample weights:\n",
    "           w(a,y) = P(a) * P(y) / P(a,y).\n",
    "       * Returns a numpy array of sample weights, aligned with the rows of df.\n",
    "   - Construct a DataFrame from the training split:\n",
    "       train_for_weights = X_train.copy()\n",
    "       train_for_weights[\"label\"] = y_train\n",
    "   - Call `compute_reweighing_weights` to get `w_train`.\n",
    "\n",
    "5. Define preprocessing and baseline classifier:\n",
    "   - Use a ColumnTransformer that:\n",
    "       * One-hot encodes categorical columns (dtype == object), e.g. \"geo\", \"iccs\".\n",
    "       * Standard-scales numerical columns, e.g. \"time\", \"value\".\n",
    "   - Wrap this in a Pipeline with:\n",
    "       (\"preproc\", ColumnTransformer),\n",
    "       (\"clf\", LogisticRegression(max_iter=1000, random_state=42)).\n",
    "\n",
    "6. Baseline logistic regression (no fairness):\n",
    "   - Fit the pipeline on (X_train, y_train) without sample weights.\n",
    "   - Predict on X_val.\n",
    "   - Compute:\n",
    "       * Overall accuracy (sklearn.metrics.accuracy_score).\n",
    "       * Per-group fairness metrics and disparities using `compute_group_fairness`.\n",
    "   - Print:\n",
    "       \"=== Baseline logistic regression ===\"\n",
    "       - accuracy\n",
    "       - per-group metrics\n",
    "       - disparities (TPR_gap, FPR_gap, etc.).\n",
    "\n",
    "7. Fairness-aware training with Reweighing:\n",
    "   - Refit the same pipeline on (X_train, y_train), but now pass:\n",
    "       sample_weight = w_train\n",
    "     to the classifier in the pipeline via the correct parameter name\n",
    "       (e.g. clf__sample_weight in sklearn Pipeline).\n",
    "   - Predict on X_val.\n",
    "   - Compute accuracy and fairness metrics again.\n",
    "   - Print:\n",
    "       \"=== Fairness-aware logistic regression (reweighing) ===\"\n",
    "       - accuracy\n",
    "       - per-group metrics\n",
    "       - disparities.\n",
    "   - Compare fairness gaps with the baseline to show improvement.\n",
    "\n",
    "8. Post-processing fairness improvement (group-wise thresholds, Hardt-style):\n",
    "   - Implement a function:\n",
    "       search_group_thresholds(model, X, y_true, groups, n_grid=21)\n",
    "     that:\n",
    "       * Uses model.predict_proba(X)[:, 1] to get positive-class scores.\n",
    "       * For each group g:\n",
    "           - Loops over thresholds t in np.linspace(0.1, 0.9, n_grid).\n",
    "           - For each t, creates binary predictions y_hat = (proba >= t).\n",
    "           - Computes accuracy and FPR on that group.\n",
    "           - Defines a score = accuracy - FPR (simple trade-off).\n",
    "           - Keeps the threshold t with the best score.\n",
    "       * Returns a dictionary: {group_value: best_threshold}.\n",
    "   - Implement a function:\n",
    "       predict_with_thresholds(model, X, groups, thresholds, proba_pos_index=1)\n",
    "     that:\n",
    "       * Uses predict_proba to get scores.\n",
    "       * Applies group-specific thresholds from the dict.\n",
    "   - Apply these thresholds on the validation set:\n",
    "       * Compute new accuracy and fairness metrics.\n",
    "       * Print results as:\n",
    "           \"=== Post-processing thresholds ===\"\n",
    "           - thresholds per group\n",
    "           - accuracy\n",
    "           - disparities.\n",
    "\n",
    "9. Final fairness-aware model and submission:\n",
    "   - Recompute reweighing weights on the full training data (X, y).\n",
    "   - Fit the pipeline on the full (X, y) with these weights.\n",
    "   - Predict labels for X_test.\n",
    "   - Save a CSV file:\n",
    "       working/submission_fair_logreg.csv\n",
    "     with a single column \"label\".\n",
    "\n",
    "10. Requirements:\n",
    "   - The script must be fully runnable as a single file, with all imports included.\n",
    "   - It must print baseline vs. reweighing vs. post-processing metrics so that fairness improvements are visible.\n",
    "   - It must save the final CSV file for submission.\n",
    "\n",
    "Now, generate a complete Python script that implements all the steps above.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1. Imports and paths\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. Paths and data loading\n",
    "# =====================================================\n",
    "\n",
    "BASE_DIR = r\"C:\\TUB\\RDEP\\crime_task\"   \n",
    "train_path = os.path.join(BASE_DIR, \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"test.csv\")\n",
    "working_dir = os.path.join(BASE_DIR, \"working\")\n",
    "\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "# possible feature columns (with or without \"value\")\n",
    "possible_features = [\"geo\", \"iccs\", \"time\", \"value\"]\n",
    "features = [c for c in possible_features if c in train.columns]\n",
    "\n",
    "print(\"Using features:\", features)\n",
    "\n",
    "X = train[features].copy()\n",
    "y = train[\"label\"].copy()\n",
    "X_test = test[features].copy()\n",
    "\n",
    "# sensitive attribute for group fairness\n",
    "sensitive_col = \"geo\"\n",
    "groups = X[sensitive_col].values\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. Train / validation split\n",
    "# =====================================================\n",
    "\n",
    "X_train, X_val, y_train, y_val, g_train, g_val = train_test_split(\n",
    "    X, y, groups,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 4. Fairness helper functions\n",
    "# =====================================================\n",
    "\n",
    "def compute_group_fairness(y_true, y_pred, group_values):\n",
    "    \"\"\"\n",
    "    Compute per-group TPR, FPR, FNR and positive prediction rate.\n",
    "    Also compute gaps (max - min) across groups.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame({\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"group\": group_values\n",
    "    })\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for g, df_g in data.groupby(\"group\"):\n",
    "        N = len(df_g)\n",
    "        tp = ((df_g[\"y_true\"] == 1) & (df_g[\"y_pred\"] == 1)).sum()\n",
    "        fp = ((df_g[\"y_true\"] == 0) & (df_g[\"y_pred\"] == 1)).sum()\n",
    "        tn = ((df_g[\"y_true\"] == 0) & (df_g[\"y_pred\"] == 0)).sum()\n",
    "        fn = ((df_g[\"y_true\"] == 1) & (df_g[\"y_pred\"] == 0)).sum()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        positive_rate = (tp + fp) / N if N > 0 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"group\": g,\n",
    "            \"N\": N,\n",
    "            \"TP\": tp,\n",
    "            \"FP\": fp,\n",
    "            \"TN\": tn,\n",
    "            \"FN\": fn,\n",
    "            \"TPR\": tpr,\n",
    "            \"FPR\": fpr,\n",
    "            \"FNR\": fnr,\n",
    "            \"PositiveRate\": positive_rate\n",
    "        })\n",
    "\n",
    "    df_metrics = pd.DataFrame(rows).set_index(\"group\")\n",
    "\n",
    "    disparities = pd.DataFrame({\n",
    "        \"value\": {\n",
    "            \"TPR_gap\": df_metrics[\"TPR\"].max() - df_metrics[\"TPR\"].min(),\n",
    "            \"FPR_gap\": df_metrics[\"FPR\"].max() - df_metrics[\"FPR\"].min(),\n",
    "            \"FNR_gap\": df_metrics[\"FNR\"].max() - df_metrics[\"FNR\"].min(),\n",
    "            \"PositiveRate_gap\": df_metrics[\"PositiveRate\"].max() - df_metrics[\"PositiveRate\"].min(),\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return df_metrics, disparities\n",
    "\n",
    "\n",
    "def compute_reweighing_weights(df, group_col, label_col):\n",
    "    \"\"\"\n",
    "    Reweighing as in Kamiran & Calders (2012).\n",
    "\n",
    "    w(a,y) = P(y) * P(a) / P(a,y)\n",
    "\n",
    "    This reduces sample bias between (group, label) combinations.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    a = df[group_col]\n",
    "    y = df[label_col]\n",
    "\n",
    "    n = len(df)\n",
    "\n",
    "    # Marginal probabilities\n",
    "    p_y = y.value_counts() / n         # P(y)\n",
    "    p_a = a.value_counts() / n         # P(a)\n",
    "\n",
    "    # Joint probability P(a,y)\n",
    "    p_a_y = df.groupby([group_col, label_col]).size() / n  # P(a,y)\n",
    "\n",
    "    weight_table = {}\n",
    "    for (a_val, y_val), p_ay in p_a_y.items():\n",
    "        w = (p_y[y_val] * p_a[a_val]) / p_ay\n",
    "        weight_table[(a_val, y_val)] = w\n",
    "\n",
    "    # Map back to each sample\n",
    "    sample_weights = []\n",
    "    for a_val, y_val in zip(a, y):\n",
    "        sample_weights.append(weight_table[(a_val, y_val)])\n",
    "\n",
    "    return np.array(sample_weights)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 5. Compute reweighing weights on training split\n",
    "# =====================================================\n",
    "\n",
    "train_for_weights = X_train.copy()\n",
    "train_for_weights[\"label\"] = y_train.values\n",
    "\n",
    "w_train = compute_reweighing_weights(\n",
    "    train_for_weights,\n",
    "    group_col=sensitive_col,\n",
    "    label_col=\"label\"\n",
    ")\n",
    "\n",
    "print(\"Reweighing weights range:\", w_train.min(), \"to\", w_train.max())\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 6. Preprocessing + logistic regression pipeline\n",
    "# =====================================================\n",
    "\n",
    "cat_cols = []\n",
    "num_cols = []\n",
    "for c in features:\n",
    "    if train[c].dtype == \"object\":\n",
    "        cat_cols.append(c)\n",
    "    else:\n",
    "        num_cols.append(c)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "pipe_log = Pipeline(\n",
    "    steps=[\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\"clf\", log_reg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 7. Baseline logistic regression (no fairness)\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== Baseline logistic regression ===\")\n",
    "pipe_log.fit(X_train, y_train)\n",
    "y_val_pred_base = pipe_log.predict(X_val)\n",
    "acc_base = accuracy_score(y_val, y_val_pred_base)\n",
    "print(f\"Baseline accuracy: {acc_base:.4f}\")\n",
    "\n",
    "fair_base, disp_base = compute_group_fairness(y_val, y_val_pred_base, g_val)\n",
    "print(\"Baseline per-group metrics:\")\n",
    "print(fair_base)\n",
    "print(\"\\nBaseline disparities:\")\n",
    "print(disp_base)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 8. Fairness-aware training (reweighing)\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== Fairness-aware logistic regression (reweighing) ===\")\n",
    "pipe_log.fit(X_train, y_train, clf__sample_weight=w_train)\n",
    "y_val_pred_fair = pipe_log.predict(X_val)\n",
    "acc_fair = accuracy_score(y_val, y_val_pred_fair)\n",
    "print(f\"Fairness-aware accuracy: {acc_fair:.4f}\")\n",
    "\n",
    "fair_fair, disp_fair = compute_group_fairness(y_val, y_val_pred_fair, g_val)\n",
    "print(\"Reweighing per-group metrics:\")\n",
    "print(fair_fair)\n",
    "print(\"\\nReweighing disparities:\")\n",
    "print(disp_fair)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 9. Post-processing: group-wise thresholds (Hardt-style)\n",
    "# =====================================================\n",
    "\n",
    "def predict_with_thresholds(model, X, groups, thresholds, proba_pos_index=1):\n",
    "    \"\"\"\n",
    "    Apply different thresholds per group on predicted probabilities.\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(X)[:, proba_pos_index]\n",
    "    preds = []\n",
    "    for p, g in zip(proba, groups):\n",
    "        t = thresholds.get(g, 0.5)\n",
    "        preds.append(1 if p >= t else 0)\n",
    "    return np.array(preds)\n",
    "\n",
    "\n",
    "def search_group_thresholds(model, X, y_true, groups, n_grid=21):\n",
    "    \"\"\"\n",
    "    For each group, search a threshold that gives good accuracy\n",
    "    and lower FPR (simple trade-off).\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    df = pd.DataFrame({\n",
    "        \"proba\": proba,\n",
    "        \"y_true\": y_true,\n",
    "        \"group\": groups\n",
    "    })\n",
    "\n",
    "    thresholds = {}\n",
    "    for g, df_g in df.groupby(\"group\"):\n",
    "        best_t = 0.5\n",
    "        best_score = -1.0\n",
    "\n",
    "        for t in np.linspace(0.1, 0.9, n_grid):\n",
    "            y_hat = (df_g[\"proba\"].values >= t).astype(int)\n",
    "            y_true_g = df_g[\"y_true\"].values\n",
    "\n",
    "            tp = ((y_true_g == 1) & (y_hat == 1)).sum()\n",
    "            fp = ((y_true_g == 0) & (y_hat == 1)).sum()\n",
    "            tn = ((y_true_g == 0) & (y_hat == 0)).sum()\n",
    "            fn = ((y_true_g == 1) & (y_hat == 0)).sum()\n",
    "\n",
    "            acc = (tp + tn) / len(df_g) if len(df_g) > 0 else 0.0\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "            # simple trade-off: high accuracy and low FPR\n",
    "            score = acc - fpr\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_t = t\n",
    "\n",
    "        thresholds[g] = best_t\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "print(\"\\n=== Post-processing thresholds (group-wise) ===\")\n",
    "thresholds = search_group_thresholds(pipe_log, X_val, y_val, g_val, n_grid=21)\n",
    "print(\"Example thresholds (first 10 groups):\", list(thresholds.items())[:10])\n",
    "\n",
    "y_val_pred_post = predict_with_thresholds(pipe_log, X_val, g_val, thresholds)\n",
    "acc_post = accuracy_score(y_val, y_val_pred_post)\n",
    "print(f\"Post-processed accuracy: {acc_post:.4f}\")\n",
    "\n",
    "fair_post, disp_post = compute_group_fairness(y_val, y_val_pred_post, g_val)\n",
    "print(\"Post-processing per-group metrics:\")\n",
    "print(fair_post)\n",
    "print(\"\\nPost-processing disparities:\")\n",
    "print(disp_post)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 10. Train final fairness-aware model on full train\n",
    "#     and save submission for test set\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== Train final model on full train (with reweighing) ===\")\n",
    "\n",
    "train_full = X.copy()\n",
    "train_full[\"label\"] = y.values\n",
    "w_full = compute_reweighing_weights(\n",
    "    train_full,\n",
    "    group_col=sensitive_col,\n",
    "    label_col=\"label\"\n",
    ")\n",
    "\n",
    "pipe_log.fit(X, y, clf__sample_weight=w_full)\n",
    "\n",
    "test_pred = pipe_log.predict(X_test)\n",
    "submission = pd.DataFrame({\"label\": test_pred})\n",
    "\n",
    "out_path = os.path.join(working_dir, \"submission_fair_logreg.csv\")\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(\"Saved final submission to:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
